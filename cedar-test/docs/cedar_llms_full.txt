Source: https://docs.cedarcopilot.com/llms-full.txt
Fetched: 2025-08-12T02:32:40.879564+00:00
ETag: None
Last-Modified: None
# Agent Backend Connection
Source: https://docs.cedarcopilot.com/agent-backend-connection/agent-backend-connection

Overview of connecting AI agents to Cedar-OS

Before we get to the juicy interactions like speaking to your AI for it to do work inside your applications, we have to connect to the agent!

Cedar-OS has working configurations with every major backend. We support:

* **Mastra** - Full-featured agent framework with memory, tools, and knowledge base
* **AI SDK** - Vercel's unified AI SDK supporting multiple providers
* **OpenAI** - Direct OpenAI API integration
* **Anthropic** - Direct Anthropic API integration
* **Custom Backend** - Build your own integration

## How to choose your provider

* **You want a full-featured agent: Mastra** - When you need memory across sessions (conversation history), tool calls, knowledge base, and more complex agent capabilities.
* **Simplicity: AI SDK** - One interface, multiple providers. Perfect for getting started quickly.
* **You only have one API Key: Anthropic/OpenAI** - Direct integration with a single provider.
* **You already have a custom backend (e.g Langchain): Custom Backend** - glad I could help :^)

## Initial Configuration

Choose your provider and configure it with the CedarCopilot component. By default, this will make your chat and all other functions to the backend work

<CodeGroup>
  ```tsx AI SDK
  import { CedarCopilot } from 'cedar-os';

  // TypeScript Type
  type AISDKConfig = {
  	provider: 'ai-sdk';
  	providers: {
  		openai?: { apiKey: string };
  		anthropic?: { apiKey: string };
  		google?: { apiKey: string };
  		mistral?: { apiKey: string };
  		groq?: { apiKey: string };
  		xai?: { apiKey: string };
  	};
  };

  function App() {
  	return (
  		// You don't need to put every model,
  		// but if you try to use a model without a key it will fail
  		<CedarCopilot
  			llmProvider={{
  				provider: 'ai-sdk',
  				providers: {
  					openai: {
  						apiKey: process.env.NEXT_PUBLIC_OPENAI_API_KEY,
  					},
  					anthropic: {
  						apiKey: process.env.NEXT_PUBLIC_ANTHROPIC_API_KEY,
  					},
  					google: {
  						apiKey: process.env.NEXT_PUBLIC_GOOGLE_API_KEY,
  					},
  					groq: {
  						apiKey: process.env.GROQ_API_KEY,
  					},
  					xai: {
  						apiKey: process.env.XAI_API_KEY,
  					},
  				},
  			}}>
  			<YourApp />
  		</CedarCopilot>
  	);
  }
  ```

  ```tsx OpenAI
  import { CedarCopilot } from 'cedar-os';

  // TypeScript Type
  type OpenAIConfig = {
  	provider: 'openai';
  	apiKey: string;
  };

  function App() {
  	return (
  		<CedarCopilot
  			llmProvider={{
  				provider: 'openai',
  				apiKey: process.env.NEXT_PUBLIC_OPENAI_API_KEY,
  			}}>
  			<YourApp />
  		</CedarCopilot>
  	);
  }
  ```

  ```tsx Anthropic
  import { CedarCopilot } from 'cedar-os';

  // TypeScript Type
  type AnthropicConfig = {
  	provider: 'anthropic';
  	apiKey: string;
  };

  function App() {
  	return (
  		<CedarCopilot
  			llmProvider={{
  				provider: 'anthropic',
  				apiKey: process.env.NEXT_PUBLIC_ANTHROPIC_API_KEY,
  			}}>
  			<YourApp />
  		</CedarCopilot>
  	);
  }
  ```

  ```tsx Mastra
  import { CedarCopilot } from 'cedar-os';

  // TypeScript Type
  type MastraConfig = {
  	provider: 'mastra';
  	baseURL: string;
  	apiKey?: string; // Optional: only if your backend requires auth
  	chatPath?: string; // Optional: base chat path (defaults to '/chat')
  	voiceRoute?: string; // Optional: voice endpoint route (auto-configures voice endpoint)
  };

  function App() {
  	return (
  		<CedarCopilot
  			llmProvider={{
  				provider: 'mastra',
  				baseURL: 'http://localhost:3000/api', // Your Mastra backend URL
  				apiKey: process.env.MASTRA_API_KEY, // Optional: only if your backend requires auth
  				chatPath: '/chat', // Optional: base chat path (defaults to '/chat')
  				voiceRoute: '/chat/voice-execute', // Optional: voice endpoint route
  			}}>
  			<YourApp />
  		</CedarCopilot>
  	);
  }
  ```

  ```tsx Custom Backend
  import { CedarCopilot } from 'cedar-os';

  // TypeScript Type
  type CustomConfig = {
  	provider: 'custom';
  	config: Record<string, unknown>; // Flexible config object for any custom backend needs
  };

  function App() {
  	return (
  		<CedarCopilot
  			llmProvider={{
  				provider: 'custom',
  				config: {
  					baseURL: 'https://your-api.com',
  					apiKey: 'your-api-key',
  					// Any additional config your backend needs
  					organizationId: 'org-123',
  					projectId: 'project-456',
  				},
  			}}>
  			<YourApp />
  		</CedarCopilot>
  	);
  }
  ```
</CodeGroup>

### Mastra Configuration Options

When using the Mastra provider, you have additional configuration options:

* **baseURL**: The base URL of your Mastra backend (required)
* **apiKey**: Optional API key if your backend requires authentication
* **chatPath**: Optional base path for chat endpoints (defaults to `/chat`)
* **voiceRoute**: Optional route for voice endpoints (automatically configures voice endpoint)

The `chatPath` parameter is particularly useful if your Mastra backend uses a different base path for chat endpoints. For example:

* With default `chatPath: '/chat'`: Routes become `/chat/execute-function`, `/chat/init`, etc.
* With custom `chatPath: '/api/v1/chat'`: Routes become `/api/v1/chat/execute-function`, `/api/v1/chat/init`, etc.

The `voiceRoute` parameter automatically configures the voice endpoint by combining it with the `baseURL`. For example:

* With `baseURL: 'http://localhost:3000/api'` and `voiceRoute: '/chat/voice-execute'`: Voice endpoint becomes `http://localhost:3000/api/chat/voice-execute`
* If not specified, the voice endpoint remains at the default value and must be configured manually

The high-level `sendMessage()` function (that handles sending the chat), calls by default:

* `${chatPath}` (on message send, non-streaming)
* `${chatPath}/stream` (on message send, streaming).

You can still override this by passing a custom `route` parameter to `sendMessage()`.

### Typed Provider Connection

Cedar-OS provides full TypeScript support. When you configure a provider, all methods are properly typed:

```typescript
// Types can be found in cedar-os/types.ts

// Base params sent to all providers
export interface BaseParams {
	prompt: string; // Compiled frontend context and user message
	systemPrompt?: string;
	temperature?: number;
	maxTokens?: number;
	[key: string]: unknown;
}

// What the frontend expects in return
export interface LLMResponse {
	content: string; // In the chat sendMessage() flow, this is the message added to the chat
	usage?: {
		promptTokens: number;
		completionTokens: number;
		totalTokens: number;
	};
	metadata?: Record<string, unknown>;
	object?: unknown; // The object field contains structured output that is parsed for state execution, generative UI, etc.
}
```

<AccordionGroup>
  <Accordion title="Reserved object shapes in the Cedar chat">
    When returning structured data in the `object` field of `LLMResponse`, objects with a `type` field have special meanings in Cedar-OS:

    **`type: "action"`** - Defers to frontend action execution (state manipulation)

    * Learn more: [Agentic State Access](/state-access/agentic-state-access)

    **`type: "message"`** - Processes and adds content as a message in the chat

    * Automatically renders the object content as a new chat message
    * Note: if you return an object with this type AND a content field, both will be rendered as chat messages in the order the are received

    Other `type` values can be used for custom handling (such as generative UI).
  </Accordion>
</AccordionGroup>

When designing agentic workflows, the best practice for working with Cedar-OS is to create typed functions with hardcoded system prompts in a single file. This approach ensures consistency, type safety, and reusability across your application.

<CodeGroup>
  ```tsx AI SDK
  import { useTypedAgentConnection } from 'cedar-os';
  import { z } from 'zod';

  // Get a typed connection for AI SDK
  const { callLLM, streamLLM, callLLMStructured } =
  	useTypedAgentConnection('ai-sdk');

  interface AISDKParams extends BaseParams {
  	model: string; // Format: "provider/model" e.g., "openai/gpt-4o", "anthropic/claude-3-sonnet"
  }

  // Example with OpenAI model through AI SDK
  const openaiResponse = await callLLM({
  	model: 'openai/gpt-4o',
  	prompt: 'Hello, AI!',
  	systemPrompt: 'You are a helpful assistant.',
  });

  // Example with Anthropic model through AI SDK
  const anthropicResponse = await callLLM({
  	model: 'anthropic/claude-3-sonnet',
  	prompt: 'Hello, AI!',
  	systemPrompt: 'You are a helpful assistant.',
  });

  // Example with xAI Grok model through AI SDK
  const grokResponse = await callLLM({
  	model: 'xai/grok-beta',
  	prompt: 'Hello, AI!',
  	systemPrompt: 'You are a helpful assistant.',
  });

  // Structured response example with Zod schema
  const ProductSchema = z.object({
  	name: z.string(),
  	price: z.number(),
  	category: z.string(),
  	inStock: z.boolean(),
  	description: z.string(),
  });

  type Product = z.infer<typeof ProductSchema>;

  const structuredResponse: Product = await callLLMStructured({
  	model: 'openai/gpt-4o',
  	prompt: 'Generate a product for a tech store',
  	systemPrompt: 'You are a product generator for an electronics store.',
  	schema: ProductSchema,
  });

  // structuredResponse.object is now fully typed as Product
  console.log(structuredResponse.object.name); // string
  console.log(structuredResponse.object.price); // number
  console.log(structuredResponse.object.inStock); // boolean
  ```

  ```tsx OpenAI
  import { useTypedAgentConnection } from 'cedar-os';
  import { z } from 'zod';

  // Get a typed connection for OpenAI
  const { callLLM, streamLLM, callLLMStructured } =
  	useTypedAgentConnection('openai');

  interface OpenAIParams extends BaseParams {
  	model: string;
  }

  // Example usage
  const response = await callLLM({
  	model: 'gpt-4o',
  	prompt: 'Generate a product description for a new product',
  	systemPrompt:
  		'You are a product description generator. You are given a product name and you need to generate a product description for it.',
  });

  // Streaming example
  const stream = await streamLLM({
  	model: 'gpt-4o',
  	prompt: 'Tell me a story',
  	systemPrompt: 'You are a creative storyteller.',
  });

  // Structured response example
  const UserAnalysisSchema = z.object({
  	sentiment: z.enum(['positive', 'negative', 'neutral']),
  	confidence: z.number().min(0).max(1),
  	keyTopics: z.array(z.string()),
  	actionItems: z.array(
  		z.object({
  			task: z.string(),
  			priority: z.enum(['low', 'medium', 'high']),
  			dueDate: z.string().optional(),
  		})
  	),
  });

  type UserAnalysis = z.infer<typeof UserAnalysisSchema>;

  const analysisResponse: UserAnalysis = await callLLMStructured({
  	model: 'gpt-4o',
  	prompt:
  		'Analyze this user feedback: "The app is great but loading times are slow"',
  	systemPrompt:
  		'You are a user feedback analyzer. Extract sentiment, topics, and action items.',
  	schema: UserAnalysisSchema,
  });

  // analysisResponse.object is fully typed as UserAnalysis
  console.log(analysisResponse.object.sentiment); // 'positive' | 'negative' | 'neutral'
  console.log(analysisResponse.object.confidence); // number
  console.log(analysisResponse.object.actionItems[0].priority); // 'low' | 'medium' | 'high'
  ```

  ```tsx Anthropic
  import { useTypedAgentConnection } from 'cedar-os';
  import { z } from 'zod';

  // Get a typed connection for Anthropic
  const { callLLM, streamLLM, callLLMStructured } =
  	useTypedAgentConnection('anthropic');

  interface AnthropicParams extends BaseParams {
  	model: string;
  }

  // Example usage
  const response = await callLLM({
  	model: 'claude-3-sonnet-20240229',
  	prompt: 'Hello, AI!',
  	systemPrompt: 'You are a helpful assistant.',
  });

  // System prompts example
  const analysisResponse = await callLLM({
  	model: 'claude-3-opus-20240229',
  	prompt: 'Analyze this data...',
  	systemPrompt:
  		'You are a data analyst with expertise in statistical analysis.',
  });

  // Structured response example with complex schema
  const CodeReviewSchema = z.object({
  	overallScore: z.number().min(1).max(10),
  	issues: z.array(
  		z.object({
  			type: z.enum([
  				'bug',
  				'performance',
  				'security',
  				'style',
  				'maintainability',
  			]),
  			severity: z.enum(['low', 'medium', 'high', 'critical']),
  			line: z.number().optional(),
  			description: z.string(),
  			suggestion: z.string(),
  		})
  	),
  	strengths: z.array(z.string()),
  	recommendations: z.array(
  		z.object({
  			category: z.string(),
  			action: z.string(),
  			impact: z.enum(['low', 'medium', 'high']),
  		})
  	),
  });

  type CodeReview = z.infer<typeof CodeReviewSchema>;

  const codeReviewResponse: CodeReview = await callLLMStructured({
  	model: 'claude-3-opus-20240229',
  	prompt: 'Review this React component code: [code here]',
  	systemPrompt:
  		'You are a senior software engineer conducting a thorough code review.',
  	schema: CodeReviewSchema,
  });

  // codeReviewResponse.object is fully typed as CodeReview
  console.log(codeReviewResponse.object.overallScore); // number (1-10)
  console.log(codeReviewResponse.object.issues[0].severity); // 'low' | 'medium' | 'high' | 'critical'
  console.log(codeReviewResponse.object.recommendations[0].impact); // 'low' | 'medium' | 'high'
  ```

  ```tsx Mastra
  import { useTypedAgentConnection } from 'cedar-os';
  import { z } from 'zod';

  // Get a typed connection for Mastra
  const { callLLM, streamLLM, callLLMStructured } =
  	useTypedAgentConnection('mastra');

  interface MastraParams extends BaseParams {
  	route: string;
  	resourceId?: string; // User ID in Cedar
  	threadId?: string;
  }

  // Example usage
  const response = await callLLM({
  	route: '/chat/completions', // Full route path
  	prompt: 'Hello, AI!',
  });

  // Response is fully typed as LLMResponse
  console.log(response.content); // string
  console.log(response.usage?.totalTokens); // number | undefined
  console.log(response.object); // unknown (for structured responses)

  // Example with additional Mastra features
  // Note: When using sendMessage(), the default route is automatically constructed
  // as `${chatPath}` (e.g., '/chat')
  const advancedResponse = await callLLM({
  	route: '/chat', // This would be the default for sendMessage()
  	prompt: 'Remember my name is John',
  	// Additional Mastra-specific parameters can be added
  	sessionId: 'user-123',
  	tools: ['web_search', 'calculator'],
  	knowledgeBaseId: 'kb-456',
  });

  // Structured response example with Mastra
  const TaskPlanSchema = z.object({
  	title: z.string(),
  	steps: z.array(
  		z.object({
  			id: z.number(),
  			description: z.string(),
  			estimatedTime: z.string(),
  			dependencies: z.array(z.number()).optional(),
  			toolsRequired: z.array(z.string()).optional(),
  		})
  	),
  	totalEstimatedTime: z.string(),
  	complexity: z.enum(['simple', 'moderate', 'complex']),
  	requiredTools: z.array(z.string()),
  });

  type TaskPlan = z.infer<typeof TaskPlanSchema>;

  const taskPlanResponse: TaskPlan = await callLLMStructured({
  	route: '/chat/structured',
  	prompt: 'Create a plan to build a todo app with user authentication',
  	schema: TaskPlanSchema,
  	// Mastra-specific parameters
  	sessionId: 'user-123',
  	tools: ['web_search', 'code_generator'],
  	knowledgeBaseId: 'development-kb',
  });

  // taskPlanResponse.object is fully typed as TaskPlan
  console.log(taskPlanResponse.object.title); // string
  console.log(taskPlanResponse.object.steps[0].estimatedTime); // string
  console.log(taskPlanResponse.object.complexity); // 'simple' | 'moderate' | 'complex'
  console.log(taskPlanResponse.object.requiredTools); // string[]
  ```

  ```tsx Custom Backend
  import { useTypedAgentConnection } from 'cedar-os';
  import { z } from 'zod';

  // Get a typed connection for Custom Backend
  const { callLLM, streamLLM, callLLMStructured } =
  	useTypedAgentConnection('custom');

  interface CustomParams extends BaseParams {
  	userId?: string;
  	threadId?: string;
  	[key: string]: unknown;
  }

  // Example usage with custom parameters
  const response = await callLLM({
  	prompt: 'Hello, AI!',
  	systemPrompt: 'You are a helpful assistant.',
  	// Custom backend specific parameters
  	organizationId: 'org-123',
  	projectId: 'project-456',
  	customParameter: 'custom-value',
  });

  // Streaming example with custom parameters
  const stream = await streamLLM({
  	prompt: 'Generate a report',
  	systemPrompt: 'You are a report generator.',
  	organizationId: 'org-123',
  	projectId: 'project-456',
  	streamOptions: {
  		bufferSize: 1024,
  		timeout: 30000,
  	},
  });

  // Custom authentication example
  const authenticatedResponse = await callLLM({
  	prompt: 'Secure request',
  	systemPrompt: 'You are a secure assistant.',
  	headers: {
  		'X-Custom-Auth': 'bearer-token',
  		'X-Request-ID': 'req-789',
  	},
  	organizationId: 'org-123',
  });

  // Structured response example with custom backend
  const ReportSchema = z.object({
  	summary: z.string(),
  	metrics: z.object({
  		totalUsers: z.number(),
  		activeUsers: z.number(),
  		conversionRate: z.number().min(0).max(1),
  		revenue: z.number(),
  	}),
  	insights: z.array(
  		z.object({
  			category: z.string(),
  			finding: z.string(),
  			impact: z.enum(['positive', 'negative', 'neutral']),
  			confidence: z.number().min(0).max(1),
  		})
  	),
  	recommendations: z.array(z.string()),
  	generatedAt: z.string(),
  });

  type Report = z.infer<typeof ReportSchema>;

  const reportResponse: Report = await callLLMStructured({
  	prompt: 'Generate a business analytics report based on the provided data',
  	systemPrompt: 'You are a business analyst creating comprehensive reports.',
  	schema: ReportSchema,
  	// Custom backend parameters
  	organizationId: 'org-123',
  	projectId: 'project-456',
  	dataSource: 'analytics-db',
  	reportType: 'monthly',
  	headers: {
  		'X-Custom-Auth': 'bearer-token',
  		'X-Report-Format': 'structured',
  	},
  });

  // reportResponse.object is fully typed as Report
  console.log(reportResponse.object.summary); // string
  console.log(reportResponse.object.metrics.conversionRate); // number (0-1)
  console.log(reportResponse.object.insights[0].impact); // 'positive' | 'negative' | 'neutral'
  console.log(reportResponse.object.recommendations); // string[]
  ```
</CodeGroup>

For more advanced usage, see our guides on:

* [Chat Input](/agent-input-context/agent-input-context) - Building rich chat interfaces
* [Messages](/chat/chat-overview) - Handling AI responses
* [State Access](/state-access/agentic-state-access) - Managing application state

## Detailed Provider Guides

For more advanced configuration and usage patterns:

* [Extending Mastra](/agent-backend-connection/mastra) - Full agent framework with backend extension guides
* [Custom Backend](/agent-backend-connection/custom) - Build your own integration through API


# Custom Backend Implementation
Source: https://docs.cedarcopilot.com/agent-backend-connection/custom

Learn how to create a custom backend provider for Cedar-OS by implementing the required provider interface functions.

# Custom Backend Implementation

Cedar-OS provides a flexible agent connection system that allows you to integrate with any LLM provider or custom backend. This guide explains how to implement a custom provider by creating the required functions and registering them with the system.

## Agent Connection Architecture

The Cedar-OS agent connection system is built around a **provider pattern** that abstracts different LLM services behind a common interface. Each provider implements a set of standardized functions that handle:

* **Non-streaming LLM calls** (`callLLM`)
* **Structured output calls** (`callLLMStructured`)
* **Streaming responses** (`streamLLM`)
* **Response parsing** (`handleResponse`, `handleStreamResponse`)

The system automatically handles:

* Request/response logging
* Error handling and retries
* Stream management and cancellation
* Type safety and validation

## Provider Interface

Every custom provider must implement the `ProviderImplementation` interface with these 5 required functions:

```typescript
interface ProviderImplementation<TParams, TConfig> {
	callLLM: (params: TParams, config: TConfig) => Promise<LLMResponse>;
	callLLMStructured: (
		params: TParams & StructuredParams,
		config: TConfig
	) => Promise<LLMResponse>;
	streamLLM: (
		params: TParams,
		config: TConfig,
		handler: StreamHandler
	) => StreamResponse;
	handleResponse: (response: Response) => Promise<LLMResponse>;
	handleStreamResponse: (chunk: string) => StreamEvent;
}
```

## Required Function Implementations

### 1. `callLLM` - Basic LLM Calls

**Purpose:** Make non-streaming calls to your LLM service.

**Input Parameters:**

```typescript
interface CustomParams extends BaseParams {
	prompt: string; // The user's input prompt
	systemPrompt?: string; // Optional system prompt
	temperature?: number; // Sampling temperature (0-1)
	maxTokens?: number; // Maximum tokens to generate
	[key: string]: unknown; // Any additional custom parameters
}

// Your custom config type
type CustomConfig = { provider: 'custom'; config: Record<string, unknown> };
```

**Expected Output:**

```typescript
interface LLMResponse {
	content: string; // The generated text response
	usage?: {
		// Optional token usage info
		promptTokens: number;
		completionTokens: number;
		totalTokens: number;
	};
	metadata?: Record<string, unknown>; // Optional metadata (model, id, etc.)
	object?: unknown; // For structured output
}
```

**Example Implementation:**

```typescript
callLLM: async (params, config) => {
	const { prompt, systemPrompt, temperature, maxTokens, ...rest } = params;

	// Build your API request
	const response = await fetch('https://your-api.com/chat/completions', {
		method: 'POST',
		headers: {
			'Content-Type': 'application/json',
			Authorization: `Bearer ${config.config.apiKey}`,
		},
		body: JSON.stringify({
			messages: [
				...(systemPrompt ? [{ role: 'system', content: systemPrompt }] : []),
				{ role: 'user', content: prompt },
			],
			temperature,
			max_tokens: maxTokens,
			...rest,
		}),
	});

	return this.handleResponse(response);
};
```

### 2. `callLLMStructured` - Structured Output Calls

**Purpose:** Make calls that return structured data (JSON) based on a provided schema.

**Input Parameters:**

```typescript
interface StructuredParams {
	schema?: unknown; // JSON Schema or Zod schema
	schemaName?: string; // Name for the schema
	schemaDescription?: string; // Description of expected output
}

// Combined with your custom params
type StructuredCustomParams = CustomParams & StructuredParams;
```

**Expected Output:** Same as `callLLM`, but with the `object` field populated with parsed structured data.

**Example Implementation:**

```typescript
callLLMStructured: async (params, config) => {
	const {
		prompt,
		systemPrompt,
		schema,
		schemaName,
		schemaDescription,
		...rest
	} = params;

	const body = {
		messages: [
			...(systemPrompt ? [{ role: 'system', content: systemPrompt }] : []),
			{ role: 'user', content: prompt },
		],
		...rest,
	};

	// Add schema for structured output (format depends on your API)
	if (schema) {
		body.response_format = {
			type: 'json_schema',
			json_schema: {
				name: schemaName || 'response',
				description: schemaDescription,
				schema: schema,
			},
		};
	}

	const response = await fetch('https://your-api.com/chat/completions', {
		method: 'POST',
		headers: {
			'Content-Type': 'application/json',
			Authorization: `Bearer ${config.config.apiKey}`,
		},
		body: JSON.stringify(body),
	});

	const result = await this.handleResponse(response);

	// Parse structured output if schema was provided
	if (schema && result.content) {
		try {
			result.object = JSON.parse(result.content);
		} catch {
			// Leave object undefined if parsing fails
		}
	}

	return result;
};
```

### 3. `streamLLM` - Streaming Responses

**Purpose:** Handle real-time streaming responses from your LLM service.

**Input Parameters:**

* Same `params` as `callLLM`
* `handler`: A callback function to process stream events

**Stream Handler Types:**

```typescript
type StreamEvent =
	| { type: 'chunk'; content: string } // New content chunk
	| { type: 'done' } // Stream completed
	| { type: 'error'; error: Error } // Error occurred
	| { type: 'metadata'; data: unknown }; // Optional metadata

type StreamHandler = (event: StreamEvent) => void | Promise<void>;
```

**Expected Output:**

```typescript
interface StreamResponse {
	abort: () => void; // Function to cancel the stream
	completion: Promise<void>; // Promise that resolves when stream completes
}
```

**Example Implementation:**

```typescript
streamLLM: (params, config, handler) => {
	const abortController = new AbortController();

	const completion = (async () => {
		try {
			const { prompt, systemPrompt, temperature, maxTokens, ...rest } = params;

			const response = await fetch('https://your-api.com/chat/completions', {
				method: 'POST',
				headers: {
					'Content-Type': 'application/json',
					Authorization: `Bearer ${config.config.apiKey}`,
				},
				body: JSON.stringify({
					messages: [
						...(systemPrompt
							? [{ role: 'system', content: systemPrompt }]
							: []),
						{ role: 'user', content: prompt },
					],
					temperature,
					max_tokens: maxTokens,
					stream: true, // Enable streaming
					...rest,
				}),
				signal: abortController.signal,
			});

			if (!response.ok) {
				throw new Error(`HTTP error! status: ${response.status}`);
			}

			// Handle Server-Sent Events stream
			await this.handleEventStream(response, {
				onMessage: (chunk) => {
					// Parse your API's streaming format
					try {
						const data = JSON.parse(chunk);
						const content = data.choices?.[0]?.delta?.content || '';
						if (content) {
							handler({ type: 'chunk', content });
						}
					} catch {
						// Skip parsing errors
					}
				},
				onDone: () => {
					handler({ type: 'done' });
				},
			});
		} catch (error) {
			if (error instanceof Error && error.name !== 'AbortError') {
				handler({ type: 'error', error });
			}
		}
	})();

	return {
		abort: () => abortController.abort(),
		completion,
	};
};
```

### 4. `handleResponse` - Parse API Responses

**Purpose:** Convert your API's response format to the standard `LLMResponse` format.

**Input:** Standard `Response` object from fetch
**Output:** `LLMResponse` object

**Example Implementation:**

```typescript
handleResponse: async (response) => {
	if (!response.ok) {
		throw new Error(`HTTP error! status: ${response.status}`);
	}

	const data = await response.json();

	return {
		content: data.choices?.[0]?.message?.content || data.text || '',
		usage: data.usage
			? {
					promptTokens: data.usage.prompt_tokens || 0,
					completionTokens: data.usage.completion_tokens || 0,
					totalTokens: data.usage.total_tokens || 0,
			  }
			: undefined,
		metadata: {
			model: data.model,
			id: data.id,
			// Add any other relevant metadata
		},
	};
};
```

### 5. `handleStreamResponse` - Parse Stream Chunks

**Purpose:** Convert individual stream chunks to `StreamEvent` objects.

**Input:** Raw string chunk from the stream
**Output:** `StreamEvent` object

**Example Implementation:**

```typescript
handleStreamResponse: (chunk) => {
	try {
		const data = JSON.parse(chunk);
		const content = data.choices?.[0]?.delta?.content || '';
		return { type: 'chunk', content };
	} catch (error) {
		return { type: 'error', error: error as Error };
	}
};
```

## Complete Custom Provider Example

Here's a complete example of a custom provider implementation:

```typescript
import type {
	CustomParams,
	ProviderImplementation,
	InferProviderConfig,
	StructuredParams,
	LLMResponse,
	StreamHandler,
	StreamResponse,
	StreamEvent,
} from '@cedar-os/core';

type CustomConfig = InferProviderConfig<'custom'>;

export const myCustomProvider: ProviderImplementation<
	CustomParams,
	CustomConfig
> = {
	callLLM: async (params, config) => {
		const { prompt, systemPrompt, temperature, maxTokens, ...rest } = params;

		const response = await fetch(`${config.config.baseURL}/chat/completions`, {
			method: 'POST',
			headers: {
				'Content-Type': 'application/json',
				Authorization: `Bearer ${config.config.apiKey}`,
			},
			body: JSON.stringify({
				messages: [
					...(systemPrompt ? [{ role: 'system', content: systemPrompt }] : []),
					{ role: 'user', content: prompt },
				],
				temperature,
				max_tokens: maxTokens,
				...rest,
			}),
		});

		return myCustomProvider.handleResponse(response);
	},

	callLLMStructured: async (params, config) => {
		// Implementation similar to callLLM but with schema handling
		// ... (see example above)
	},

	streamLLM: (params, config, handler) => {
		// Implementation for streaming
		// ... (see example above)
	},

	handleResponse: async (response) => {
		if (!response.ok) {
			throw new Error(`HTTP error! status: ${response.status}`);
		}

		const data = await response.json();
		return {
			content: data.response || data.text || '',
			usage: data.usage,
			metadata: { model: data.model, id: data.id },
		};
	},

	handleStreamResponse: (chunk) => {
		try {
			const data = JSON.parse(chunk);
			const content = data.delta?.content || '';
			return { type: 'chunk', content };
		} catch (error) {
			return { type: 'error', error: error as Error };
		}
	},
};
```

## Registering Your Custom Provider

After implementing your provider, you need to register it with the Cedar-OS system:

### 1. Add to Provider Registry

Update the provider registry in `packages/cedar-os/src/store/agentConnection/providers/index.ts`:

```typescript
import { myCustomProvider } from './my-custom-provider';

export const providerRegistry = {
	openai: openAIProvider,
	anthropic: openAIProvider,
	mastra: mastraProvider,
	'ai-sdk': aiSDKProvider,
	custom: myCustomProvider, // Replace the default with your implementation
} as const;
```

### 2. Configure the Provider

Set up your custom provider configuration:

```typescript
import { useCedarStore } from '@cedar-os/core';

const store = useCedarStore();

// Configure your custom provider
store.setProviderConfig({
	provider: 'custom',
	config: {
		apiKey: 'your-api-key',
		baseURL: 'https://your-api.com',
		model: 'your-model-name',
		// Any other configuration your provider needs
	},
});

// Connect to the provider
await store.connect();
```

### 3. Use the Provider

Once configured, you can use your custom provider like any other:

```typescript
// Make a basic call
const response = await store.callLLM({
	prompt: 'Hello, world!',
	temperature: 0.7,
	maxTokens: 100,
});

// Make a streaming call
store.streamLLM(
	{
		prompt: 'Tell me a story',
		temperature: 0.8,
	},
	(event) => {
		if (event.type === 'chunk') {
			console.log('New content:', event.content);
		} else if (event.type === 'done') {
			console.log('Stream completed');
		}
	}
);
```

## Helper Utilities

Cedar-OS provides several utility functions to help with common tasks:

### Event Stream Handling

For processing Server-Sent Events streams:

```typescript
import { handleEventStream } from '@cedar-os/core';

await handleEventStream(response, {
	onMessage: (chunk) => {
		handler({ type: 'chunk', content: chunk });
	},
	onDone: () => {
		handler({ type: 'done' });
	},
});
```

### Type Safety

Use TypeScript interfaces for better type safety:

```typescript
interface MyProviderParams extends CustomParams {
	model: string;
	customSetting?: boolean;
}

interface MyProviderConfig {
	provider: 'custom';
	config: {
		apiKey: string;
		baseURL: string;
		defaultModel: string;
	};
}
```

## Best Practices

1. **Error Handling**: Always handle network errors, API errors, and parsing errors gracefully
2. **Abort Signals**: Support cancellation in streaming operations using `AbortController`
3. **Type Safety**: Use TypeScript interfaces for better development experience
4. **Logging**: The system automatically logs requests/responses, but you can add custom logging
5. **Configuration**: Make your provider configurable through the config object
6. **Testing**: Test all functions thoroughly, especially streaming and error scenarios

## Troubleshooting

**Provider not found**: Make sure you've registered your provider in the provider registry.

**Type errors**: Ensure your parameter and config types extend the required base interfaces.

**Streaming issues**: Check that your API supports Server-Sent Events and that you're parsing the format correctly.

**Authentication errors**: Verify your API key and authentication method match your provider's requirements.


# Extending Mastra
Source: https://docs.cedarcopilot.com/agent-backend-connection/mastra

Connecting Cedar-OS with Mastra

Mastra is a full-featured typescript framework to build agents. It provides memory, tool calls, knowledge base, and more. It's our personal recommended choice when building complex agents. Our features are designed to integrate most seamlessly with a backend server configured in Mastra.

<Info>
  {' '}

  **Note:** Cedar-OS is not currently compatible with Mastra's client SDK. To use
  Cedar-OS with Mastra, run a Mastra server and point Cedar-OS at it.{' '}
</Info>

**Want to skip the docs?** Clone the [cedar-mastra-starter](https://github.com/CedarCopilot/cedar-mastra-starter) starter repository and to interactively explore the possible interactions between Mastra and Cedar.

## Initial Configuration

<Steps>
  <Step title="Spin up a Mastra backend">
    Follow the official guide: [Install using the `create-mastra` CLI](https://mastra.ai/en/docs/getting-started/installation#install-using-the-create-mastra-cli).
  </Step>

  <Step title="Understand Cedar-Backend Communication">
    Understand how Cedar speaks to your backend, including default API routes,
    return types, etc. - [Agent Backend
    Connection](https://docs.cedarcopilot.com/agent-backend-connection/agent-backend-connection)
  </Step>

  <Step title="Point Cedar-OS at your backend">
    ```tsx
    import { CedarCopilot } from 'cedar-os';
    function App() {
    	return (
    		<CedarCopilot
    			llmProvider={{
    					provider: 'mastra',
    					baseURL: 'http://localhost:4111', // default dev port for Mastra
    					apiKey: process.env.NEXT_PUBLIC_MASTRA_API_KEY, // optional — only for backend auth
    				}}>
    			<YourApp />
    		</CedarCopilot>
    	); 
    }
    ```
  </Step>

  <Step title="Backend Configuration">
    [Register API routes](https://mastra.ai/en/examples/deployment/custom-api-route) in your Mastra server so Cedar's chat components have something to talk to:

    ```ts mastra/src/index.ts
    import { registerApiRoute } from '@mastra/core/server';

    // POST /chat
    // The chat's non-streaming default endpoint
    registerApiRoute('/chat', {
    	method: 'POST',
    	// …validate input w/ zod
    	handler: async (c) => {
    		/* your agent.generate() logic */
    	},
    });

    // POST /chat/stream (SSE)
    // The chat's streaming default endpoint
    registerApiRoute('/chat/stream', {
    	method: 'POST',
    	handler: async (c) => {
    		/* stream agent output in SSE format */
    	},
    });
    ```
  </Step>

  <Step title="Add Cedar Chat">
    Drop a Cedar chat component into your frontend – see [Chat Overview](https://docs.cedarcopilot.com/chat/chat-overview).
    Your backend and frontend are now linked! You're ready to start bringing the power of your Mastra agentic workflows to your UI.
  </Step>
</Steps>

## Features to explore

Now that you have Cedar-OS connected to Mastra, explore these powerful features:

* **[State Access & Manipulation](https://docs.cedarcopilot.com/state-access/agentic-state-access#useregisterstate-hook)** - Use the `useRegisterState` hook for communicating frontend state and letting agents manipulate your frontend state
* **[Mentions & Context](https://docs.cedarcopilot.com/agent-input-context/mentions#usementionprovider)** - Send @ mentions to your backend using the `useMentionProvider`

## New to Mastra? Here are a few primitives to understand

<Tabs>
  <Tab title="Agent">
    ```ts
    // Agents encapsulate instructions, model, memory and tools
    export const roadmap = new Agent({
      name: 'Roadmap',
      model: openai('gpt-4o-mini'),
      tools: { upvoteTool },
      instructions: `You are …`,
    });
    // Docs: https://mastra.ai/en/docs/agents/overview
    ```
  </Tab>

  <Tab title="Tool">
    ```ts
    // Tools expose type-safe side effects
    export const upvoteTool = createTool({
      id: 'upvote-feature',
      inputSchema: z.object({ id: z.string() }),
      execute: async ({ context }) => { /* … */ },
    });
    // Docs: https://mastra.ai/en/docs/tools-mcp/overview
    ```
  </Tab>

  <Tab title="Workflow">
    ```ts
    // Orchestrate multi-step logic
    const roadmapWorkflow = createWorkflow({ id: 'roadmap-analysis' })
      .then(step1)
      .then(step2);
    roadmapWorkflow.commit();
    // Docs: https://mastra.ai/en/docs/workflows/overview
    ```
  </Tab>

  <Tab title="index.ts">
    ```ts
    // Register all your Mastra primitives here, and configure agent memory, message storage, etc.
    export const mastra = new Mastra({
      agents: { roadmap },
      server: { apiRoutes },
      workflows: { roadmapWorkflow }
      storage: new LibSQLStore({ url: ':memory:' }),
    });
    ```
  </Tab>
</Tabs>

## Deployment

The recommended deployment setup is:

* **Frontend**: Deploy to [Vercel](https://vercel.com) for optimal performance and seamless integration
* **Backend**: Use [Mastra Cloud](https://mastra.ai/cloud) for hosting your Mastra server

## Next Steps

* Clone & run the [cedar-mastra-starter](https://github.com/CedarCopilot/cedar-mastra-starter) to see everything in action.
* Explore the official Mastra docs for further customisation.


# Agent Input Context
Source: https://docs.cedarcopilot.com/agent-input-context/agent-input-context

Provide additional context to your AI agents

Agent Input Context allows you to provide additional information to your AI agents beyond just the user's message.

This includes application state, user data, conversation history, and any other contextual information that helps the agent provide better responses.

## How It Works

When a user sends a message, Cedar automatically gathers context from various sources:

1. **Registered State**: Application state you've made available to the agent via `useCedarState`
2. **Mentions**: Specific data the user has referenced with @ mentions
3. **Additional Context**: Context entries you've manually added
4. **Chat Input Content**: The current editor content with mentions

Cedar provides several methods to stringify and access this context for your AI agents.

# Adding Context

## Mentions System

Cedar's mention system allows users to reference specific data using @ symbols. You can create mention providers that let users easily reference state or custom data:

```tsx
import { useStateBasedMentionProvider } from 'cedar-os';
import { FileText } from 'lucide-react';

function DocumentChat() {
	const [documents] = useCedarState('documents', [
		{ id: 'doc1', title: 'Project Proposal', type: 'pdf' },
		{ id: 'doc2', title: 'Meeting Notes', type: 'doc' },
	]);

	// Create a mention provider for documents
	useStateBasedMentionProvider({
		stateKey: 'documents',
		trigger: '@',
		labelField: 'title',
		searchFields: ['title', 'type'],
		description: 'Documents',
		icon: <FileText size={16} />,
		color: '#8b5cf6',
		order: 5, // Control display order when mentioned
	});

	return <ChatInput />;
}
```

For detailed information about mentions, including custom providers, multiple triggers, and advanced rendering, see the [Mentions documentation](/agent-input-context/mentions).

## State Subscription

Automatically sync your specific cedarStates to the agent input context using `useSubscribeInputContext`:

```tsx
import { useSubscribeInputContext } from 'cedar-os';
import { useState } from 'react';

function AutoSyncChat() {
	const [appState, setAppState] = useState({
		currentTab: 'dashboard',
		selectedItems: ['item1', 'item2'],
	});

	// Automatically sync state to context
	useSubscribeInputContext(
		appState,
		(state) => ({
			'app-state': [
				{
					id: 'current-state',
					currentTab: state.currentTab,
					selectedItems: state.selectedItems,
				},
			],
		}),
		{
			icon: <Settings size={16} />,
			color: '#6b7280',
			order: 10, // Control display order of context badges
		}
	);

	return <ChatInput />;
}
```

For comprehensive examples and best practices for state subscription, see the [Subscribing State documentation](/agent-input-context/subscribing-state).

# Advanced

This is for advanced understanding. Move on to [Agentic State Access](/state-access/agentic-state-access) or [Spells](/spells/spells) for better functionality.

## Default Context Tokenization

Cedar's `sendMessage` function automatically handles context based on your provider configuration:

### For Structured Providers (Mastra, Custom)

```tsx
// Inside sendMessage for Mastra/Custom providers
const editorContent = state.stringifyEditor(); // "Update @task-123 status"
const structuredContext = sanitizeJson(state.additionalContext); // Raw object

// Sent as separate fields:
const llmParams = {
	prompt: editorContent, // Clean user text
	additionalContext: structuredContext, // Structured data object
	route: '/chat',
	userId: 'user-123',
};
```

### For Text-Based Providers (OpenAI, Anthropic, AI-SDK)

```tsx
// Inside sendMessage for direct LLM providers
const editorContent = state.stringifyEditor(); // "Update @task-123 status"
const fullContext = state.stringifyInputContext(); // Combined string

// Sent as unified prompt:
const llmParams = {
	prompt: fullContext, // Contains: "User Text: ... \n\nAdditional Context: {...}"
	model: 'gpt-4o-mini',
};
```

## Backend Integration Examples

### Mastra Backend Example

```typescript
// Your Mastra agent receives structured data
export const chatAgent = {
	name: 'Chat Agent',
	instructions: 'You are a helpful assistant',
	model: 'gpt-4o-mini',

	async execute(input: { prompt: string; additionalContext?: any }) {
		const { prompt, additionalContext } = input;

		// Process user intent from clean prompt
		console.log('User wants:', prompt);

		// Access structured context programmatically
		if (additionalContext?.tasks) {
			const mentionedTasks = additionalContext.tasks
				.filter((entry) => entry.source === 'mention')
				.map((entry) => entry.data);

			console.log('Referenced tasks:', mentionedTasks);
		}

		// Your custom logic here...
	},
};
```

### Custom Provider Backend Example

```typescript
// Your custom endpoint receives both fields
app.post('/api/chat', async (req, res) => {
	const { prompt, additionalContext, userId } = req.body;

	// Clean user message for intent processing
	const userIntent = await processIntent(prompt);

	// Structured context for data operations
	const contextualData = {
		mentions: additionalContext?.tasks || [],
		userState: additionalContext?.['user-state'] || [],
		documents: additionalContext?.documents || [],
	};

	// Combine for LLM call
	const systemPrompt = `
    User Intent: ${userIntent}
    Available Context: ${JSON.stringify(contextualData)}
    
    Respond helpfully using the provided context.
  `;

	const response = await openai.chat.completions.create({
		model: 'gpt-4o-mini',
		messages: [
			{ role: 'system', content: systemPrompt },
			{ role: 'user', content: prompt },
		],
	});

	res.json({ content: response.choices[0].message.content });
});
```

## Accessing Additional Context

You can access the context data in several ways:

### Raw Context Access

```tsx
import { useCedarStore } from 'cedar-os';

function ContextInspector() {
	const { additionalContext } = useCedarStore();

	// Access raw context structure
	const tasks = additionalContext.tasks || [];
	const mentionedDocuments = additionalContext.documents || [];

	return (
		<div>
			<h3>Current Context:</h3>
			{Object.entries(additionalContext).map(([key, entries]) => (
				<div key={key}>
					<h4>{key}</h4>
					{entries.map((entry) => (
						<div key={entry.id}>
							{entry.metadata?.label} ({entry.source})
						</div>
					))}
				</div>
			))}
		</div>
	);
}
```

### Stringified Context Access

```tsx
import { useCedarStore } from 'cedar-os';

function ContextStringifier() {
	const { stringifyEditor, stringifyAdditionalContext, stringifyInputContext } =
		useCedarStore();

	const inspectContext = () => {
		// Get just the user's text input
		const userText = stringifyEditor();
		console.log('User text:', userText);

		// Get additional context as JSON
		const contextData = stringifyAdditionalContext();
		console.log('Context data:', contextData);

		// Get combined input and context (what gets sent to AI)
		const fullContext = stringifyInputContext();
		console.log('Full context:', fullContext);
	};

	return <button onClick={inspectContext}>Inspect Context</button>;
}
```

## Adding Specific Context

### Manual Context Entries

Add context entries programmatically for specific use cases:

```tsx
import { useCedarStore } from 'cedar-os';

function ErrorReportingChat() {
	const { addContextEntry, removeContextEntry } = useCedarStore();

	const addErrorContext = (error: Error) => {
		addContextEntry('errors', {
			id: `error-${Date.now()}`,
			source: 'manual',
			data: {
				message: error.message,
				stack: error.stack,
				timestamp: new Date().toISOString(),
				url: window.location.href,
			},
			metadata: {
				label: `Error: ${error.message}`,
				color: '#ef4444',
			},
		});
	};

	const addUserContext = (user: any) => {
		addContextEntry('user-info', {
			id: 'current-user',
			source: 'manual',
			data: {
				id: user.id,
				role: user.role,
				permissions: user.permissions,
			},
			metadata: {
				label: `User: ${user.name}`,
				color: '#3b82f6',
			},
		});
	};

	return (
		<div>
			<button onClick={() => addErrorContext(new Error('Sample error'))}>
				Add Error Context
			</button>
			<ChatInput />
		</div>
	);
}
```

### Conditional Context

Add context based on application state or user actions:

```tsx
import { useCedarStore } from 'cedar-os';
import { useEffect } from 'react';

function ConditionalContextChat() {
	const { addContextEntry, clearContextBySource } = useCedarStore();
	const currentRoute = useRouter().pathname;
	const selectedItems = useSelection();

	useEffect(() => {
		// Clear previous route context
		clearContextBySource('manual');

		// Add route-specific context
		if (currentRoute === '/dashboard') {
			addContextEntry('navigation', {
				id: 'dashboard-context',
				source: 'manual',
				data: {
					page: 'dashboard',
					widgets: getDashboardWidgets(),
					metrics: getCurrentMetrics(),
				},
				metadata: {
					label: 'Dashboard Context',
				},
			});
		}

		// Add selection context if items are selected
		if (selectedItems.length > 0) {
			addContextEntry('selection', {
				id: 'current-selection',
				source: 'manual',
				data: {
					items: selectedItems,
					count: selectedItems.length,
					types: [...new Set(selectedItems.map((item) => item.type))],
				},
				metadata: {
					label: `${selectedItems.length} items selected`,
				},
			});
		}
	}, [currentRoute, selectedItems]);

	return <ChatInput />;
}
```

## Custom Context Processing

Create your own message sending logic with custom context filtering:

```tsx
import { useCedarStore } from 'cedar-os';

function CustomContextProcessor() {
	const store = useCedarStore();

	const sendMessageWithFilteredContext = async (
		contextFilter?: (key: string, entries: ContextEntry[]) => ContextEntry[]
	) => {
		// Get the user's input
		const editorContent = store.stringifyEditor();

		// Get raw additional context
		const rawContext = store.additionalContext;

		// Apply custom filtering if provided
		let filteredContext = rawContext;
		if (contextFilter) {
			filteredContext = {};
			Object.entries(rawContext).forEach(([key, entries]) => {
				const filtered = contextFilter(key, entries);
				if (filtered.length > 0) {
					filteredContext[key] = filtered;
				}
			});
		}

		// Create custom context string
		const contextString = JSON.stringify(filteredContext, null, 2);
		const customPrompt = `User Text: ${editorContent}\n\nFiltered Context: ${contextString}`;

		// Send to AI with custom prompt
		const response = await store.callLLM({
			prompt: customPrompt,
			systemPrompt:
				'You are a helpful assistant with access to filtered context.',
		});

		// Handle response
		store.handleLLMResult(response);

		// Add user message to chat (persists by default if message storage is configured)
		store.addMessage({
			role: 'user',
			type: 'text',
			content: editorContent,
		});

		// Clear mentions after sending
		store.clearMentions();
	};

	// Example: Only include high-priority tasks
	const sendWithHighPriorityTasksOnly = () => {
		sendMessageWithFilteredContext((key, entries) => {
			if (key === 'tasks') {
				return entries.filter((entry) => entry.data.priority === 'high');
			}
			return entries; // Keep other context as-is
		});
	};

	// Example: Exclude sensitive data
	const sendWithoutSensitiveData = () => {
		sendMessageWithFilteredContext((key, entries) => {
			if (key === 'user-profile') {
				return entries.map((entry) => ({
					...entry,
					data: {
						...entry.data,
						email: undefined,
						phone: undefined,
						ssn: undefined,
					},
				}));
			}
			return entries;
		});
	};

	// Example: Only include recent items
	const sendWithRecentContextOnly = () => {
		const oneHourAgo = Date.now() - 60 * 60 * 1000;

		sendMessageWithFilteredContext((key, entries) => {
			return entries.filter((entry) => {
				const entryTime = entry.data.timestamp || entry.data.createdAt;
				return entryTime && new Date(entryTime).getTime() > oneHourAgo;
			});
		});
	};

	return (
		<div>
			<ChatInput />
			<div className='flex gap-2 mt-2'>
				<button onClick={sendWithHighPriorityTasksOnly}>
					Send with High Priority Tasks Only
				</button>
				<button onClick={sendWithoutSensitiveData}>
					Send without Sensitive Data
				</button>
				<button onClick={sendWithRecentContextOnly}>
					Send with Recent Context Only
				</button>
			</div>
		</div>
	);
}
```

## Next Steps

* Learn about [mentions functionality](/agent-input-context/mentions) for detailed @ mention patterns
* Explore [subscribing to state](/agent-input-context/subscribing-state) for automatic state synchronization


# Mentions
Source: https://docs.cedarcopilot.com/agent-input-context/mentions

Enable @ mentions for contextual references in chat

Cedar's mention system allows users to reference specific data, people, or objects in their messages using @ symbols. This provides a natural way to give agents precise context about what the user is referring to.

### State-Based Mentions

Automatically allows the user to @ mention any registered state items. The state must first be a cedarState. See the [Agentic State](/state-access/agentic-state-access) documentation.

```tsx
import { useState } from 'react';
import { useRegisterState, useStateBasedMentionProvider } from 'cedar-os';

function TodoApp() {
	const [todos, setTodos] = useState([
		{ id: 1, text: 'Buy groceries', category: 'shopping' },
		{ id: 2, text: 'Call dentist', category: 'health' },
	]);

	// Register the state
	useRegisterState({
		key: 'todos',
		value: todos,
		setValue: setTodos,
		description: 'Todo items',
	});

	// Enable mentions for todos
	useStateBasedMentionProvider({
		stateKey: 'todos',
		trigger: '@',
		labelField: 'text',
		searchFields: ['text', 'category'],
		description: 'Todo items',
		icon: '📝',
		color: '#3b82f6',
		order: 10, // Control display order of mentioned todos
	});

	return <ChatInput />;
}
```

### useMentionProvider

UseMentionProvider allows you to add anything as a mention, which formats and adds even non-state or other items as part of the input context.

```tsx
import { useMentionProvider } from 'cedar-os';

function UserMentions() {
	const users = [
		{
			id: 1,
			name: 'Alice Johnson',
			email: 'alice@company.com',
			role: 'Designer',
		},
		{ id: 2, name: 'Bob Smith', email: 'bob@company.com', role: 'Developer' },
	];

	useMentionProvider({
		id: 'users',
		trigger: '@',
		label: 'Users',
		description: 'Team members',
		icon: '👤',
		getItems: (query) => {
			const filtered = query
				? users.filter(
						(user) =>
							user.name.toLowerCase().includes(query.toLowerCase()) ||
							user.email.toLowerCase().includes(query.toLowerCase()) ||
							user.role.toLowerCase().includes(query.toLowerCase())
				  )
				: users;

			return filtered.map((user) => ({
				id: user.id.toString(),
				label: `${user.name} (${user.role})`,
				data: user,
				metadata: {
					icon: '👤',
					color: '#10b981',
				},
			}));
		},
		toContextEntry: (item) => ({
			id: item.id,
			source: 'mention',
			data: item.data,
			metadata: {
				label: item.label,
				icon: '👤',
				color: '#10b981',
				order: 5, // Users appear before other mentions
			},
		}),
	});

	return <ChatInput />;
}
```

## Custom Mention Providers

Create custom mention sources:

```tsx
import { useMentionProvider } from 'cedar-os';

function CustomMentions() {
	// Static mention provider for commands
	useMentionProvider({
		id: 'commands',
		trigger: '/',
		label: 'Commands',
		description: 'Available commands',
		icon: '⚡',
		color: '#f59e0b',
		getItems: (query) => {
			const commands = [
				{ id: 'help', name: 'help', description: 'Show available commands' },
				{ id: 'reset', name: 'reset', description: 'Reset the conversation' },
				{ id: 'export', name: 'export', description: 'Export chat history' },
			];

			const filtered = query
				? commands.filter(
						(cmd) =>
							cmd.name.toLowerCase().includes(query.toLowerCase()) ||
							cmd.description.toLowerCase().includes(query.toLowerCase())
				  )
				: commands;

			return filtered.map((cmd) => ({
				id: cmd.id,
				label: cmd.name,
				data: cmd,
				metadata: {
					icon: '⚡',
					color: '#f59e0b',
				},
			}));
		},
		toContextEntry: (item) => ({
			id: item.id,
			source: 'mention',
			data: item.data,
			metadata: {
				label: item.label,
				icon: '⚡',
				color: '#f59e0b',
			},
		}),
	});

	// Dynamic mention provider for files
	useMentionProvider({
		id: 'files',
		trigger: '#',
		label: 'Files',
		description: 'Project files',
		icon: '📄',
		color: '#8b5cf6',
		getItems: async (query) => {
			// Simulate async file search
			const files = await searchFiles(query);
			return files.map((file) => ({
				id: file.id,
				label: file.name,
				data: file,
				metadata: {
					icon: '📄',
					color: '#8b5cf6',
				},
			}));
		},
		toContextEntry: (item) => ({
			id: item.id,
			source: 'mention',
			data: item.data,
			metadata: {
				label: item.label,
				icon: '📄',
				color: '#8b5cf6',
			},
		}),
	});

	return <ChatInput />;
}

// Mock function for demonstration
async function searchFiles(query: string) {
	// Simulate API call
	return [
		{ id: '1', name: 'README.md', path: '/README.md', type: 'markdown' },
		{ id: '2', name: 'package.json', path: '/package.json', type: 'json' },
	].filter(
		(file) => !query || file.name.toLowerCase().includes(query.toLowerCase())
	);
}
```

## Custom Rendering

Customize how mentions appear in different contexts:

```tsx
import { useMentionProvider } from 'cedar-os';

function CustomRendering() {
	useMentionProvider({
		id: 'users',
		trigger: '@',
		label: 'Users',
		icon: '👤',
		getItems: (query) => {
			// ... get items logic
		},
		toContextEntry: (item) => ({
			id: item.id,
			source: 'mention',
			data: item.data,
			metadata: { label: item.label, icon: '👤' },
		}),

		// Custom rendering in the mention menu
		renderMenuItem: (item) => (
			<div className='flex items-center gap-2 p-2'>
				<span className='text-lg'>👤</span>
				<div>
					<div className='font-medium'>{item.label}</div>
					<div className='text-sm text-gray-500'>{item.data.role}</div>
				</div>
			</div>
		),

		// Custom rendering in the editor
		renderEditorItem: (item, attrs) => (
			<span className='bg-blue-100 text-blue-800 px-1 rounded'>
				👤 {item.label}
			</span>
		),

		// Custom rendering in context badges
		renderContextBadge: (entry) => (
			<div className='bg-gray-100 px-2 py-1 rounded text-sm'>
				👤 {entry.metadata?.label}
			</div>
		),
	});

	return <ChatInput />;
}
```

## Multiple Mention Types

Support different mention triggers:

```tsx
import { useMentionProvider } from 'cedar-os';

function MultiMentionChat() {
	const users = [
		{ id: '1', name: 'Alice', role: 'Designer' },
		{ id: '2', name: 'Bob', role: 'Developer' },
	];

	const channels = [
		{ id: '1', name: 'general', topic: 'General discussion' },
		{ id: '2', name: 'dev', topic: 'Development updates' },
	];

	const commands = [
		{ id: '1', name: 'help', description: 'Show help' },
		{ id: '2', name: 'reset', description: 'Reset chat' },
	];

	// @ for users
	useMentionProvider({
		id: 'users',
		trigger: '@',
		label: 'Users',
		icon: '👤',
		getItems: (query) => {
			const filtered = query
				? users.filter((u) =>
						u.name.toLowerCase().includes(query.toLowerCase())
				  )
				: users;
			return filtered.map((user) => ({
				id: user.id,
				label: user.name,
				data: user,
				metadata: { icon: '👤' },
			}));
		},
		toContextEntry: (item) => ({
			id: item.id,
			source: 'mention',
			data: item.data,
			metadata: { label: item.label, icon: '👤' },
		}),
	});

	// # for channels/topics
	useMentionProvider({
		id: 'channels',
		trigger: '#',
		label: 'Channels',
		icon: '📢',
		getItems: (query) => {
			const filtered = query
				? channels.filter((c) =>
						c.name.toLowerCase().includes(query.toLowerCase())
				  )
				: channels;
			return filtered.map((channel) => ({
				id: channel.id,
				label: channel.name,
				data: channel,
				metadata: { icon: '📢' },
			}));
		},
		toContextEntry: (item) => ({
			id: item.id,
			source: 'mention',
			data: item.data,
			metadata: { label: item.label, icon: '📢' },
		}),
	});

	// / for commands
	useMentionProvider({
		id: 'commands',
		trigger: '/',
		label: 'Commands',
		icon: '⚡',
		getItems: (query) => {
			const filtered = query
				? commands.filter((c) =>
						c.name.toLowerCase().includes(query.toLowerCase())
				  )
				: commands;
			return filtered.map((command) => ({
				id: command.id,
				label: command.name,
				data: command,
				metadata: { icon: '⚡' },
			}));
		},
		toContextEntry: (item) => ({
			id: item.id,
			source: 'mention',
			data: item.data,
			metadata: { label: item.label, icon: '⚡' },
		}),
	});

	return <ChatInput />;
}
```

## Contextual Mentions

Show different mentions based on context:

```tsx
import { useEffect } from 'react';
import { useMentionProvider } from 'cedar-os';

function ContextualMentions() {
	const currentPage = useCurrentPage();

	// Conditional mention providers
	useEffect(() => {
		if (currentPage === 'projects') {
			const projects = getProjects();
			useMentionProvider({
				id: 'projects',
				trigger: '@',
				label: 'Projects',
				icon: '📁',
				getItems: (query) => {
					const filtered = query
						? projects.filter((p) =>
								p.name.toLowerCase().includes(query.toLowerCase())
						  )
						: projects;
					return filtered.map((project) => ({
						id: project.id,
						label: project.name,
						data: project,
						metadata: { icon: '📁' },
					}));
				},
				toContextEntry: (item) => ({
					id: item.id,
					source: 'mention',
					data: item.data,
					metadata: { label: item.label, icon: '📁' },
				}),
			});
		} else if (currentPage === 'tasks') {
			const tasks = getTasks();
			useMentionProvider({
				id: 'tasks',
				trigger: '@',
				label: 'Tasks',
				icon: '✅',
				getItems: (query) => {
					const filtered = query
						? tasks.filter((t) =>
								t.title.toLowerCase().includes(query.toLowerCase())
						  )
						: tasks;
					return filtered.map((task) => ({
						id: task.id,
						label: task.title,
						data: task,
						metadata: { icon: '✅' },
					}));
				},
				toContextEntry: (item) => ({
					id: item.id,
					source: 'mention',
					data: item.data,
					metadata: { label: item.label, icon: '✅' },
				}),
			});
		}
	}, [currentPage]);

	return <ChatInput />;
}

// Mock functions for demonstration
function useCurrentPage() {
	return 'projects'; // or 'tasks'
}

function getProjects() {
	return [
		{ id: '1', name: 'Website Redesign' },
		{ id: '2', name: 'Mobile App' },
	];
}

function getTasks() {
	return [
		{ id: '1', title: 'Review PR #123' },
		{ id: '2', title: 'Update documentation' },
	];
}
```

## Ordering Mentions

When you have multiple mention providers or context subscriptions, you can control their display order using the `order` property. This ensures the most important context appears first in the UI.

### Order Property

The `order` property (optional) determines the display order of context badges:

* **Lower numbers appear first**: `order: 1` appears before `order: 10`
* **Default behavior**: Items without an order appear last
* **Applies to both**: Mention providers and `useSubscribeInputContext`

### Example: Prioritized Mentions

```tsx
import {
	useStateBasedMentionProvider,
	useSubscribeInputContext,
} from 'cedar-os';
import { User, FileText, Tag, Archive } from 'lucide-react';

function PrioritizedMentions() {
	// Current user context - highest priority (order: 1)
	useSubscribeInputContext(currentUser, (user) => ({ currentUser: user }), {
		icon: <User />,
		color: '#8B5CF6',
		order: 1, // Always visible first
	});

	// Active documents - high priority (order: 5)
	useStateBasedMentionProvider({
		stateKey: 'activeDocuments',
		trigger: '@',
		labelField: 'title',
		description: 'Active Documents',
		icon: <FileText />,
		color: '#3B82F6',
		order: 5, // After user, before tags
	});

	// Tags - medium priority (order: 10)
	useStateBasedMentionProvider({
		stateKey: 'tags',
		trigger: '#',
		labelField: 'name',
		description: 'Tags',
		icon: <Tag />,
		color: '#10B981',
		order: 10, // After documents
	});

	// Archived items - low priority (order: 100)
	useStateBasedMentionProvider({
		stateKey: 'archivedItems',
		trigger: '@',
		labelField: 'title',
		description: 'Archived',
		icon: <Archive />,
		color: '#6B7280',
		order: 100, // Appears last
	});

	return <ChatInput />;
}
```

### Custom Provider with Order

When using `useMentionProvider`, include order in the metadata:

```tsx
useMentionProvider({
	id: 'priority-users',
	trigger: '@',
	label: 'Team Leads',
	getItems: (query) => {
		// ... filtering logic
		return items.map((item) => ({
			id: item.id,
			label: item.name,
			data: item,
			metadata: {
				icon: '👤',
				color: '#EF4444',
			},
		}));
	},
	toContextEntry: (item) => ({
		id: item.id,
		source: 'mention',
		data: item.data,
		metadata: {
			label: item.label,
			icon: item.metadata.icon,
			color: item.metadata.color,
			order: 2, // Team leads appear near the top
		},
	}),
});
```

### Best Practices for Ordering

1. **Reserve low numbers (1-9)** for critical context like user info
2. **Use 10-49** for active/selected items
3. **Use 50-99** for general mentions
4. **Use 100+** for archived or low-priority items
5. **Leave gaps** between orders for future additions

## Mention Data in Context

Access mentioned items in your agent context:

```tsx
// When user types: "Update the status of @task-123 to completed"
// The agent receives:
{
  message: "Update the status of @task-123 to completed",
  mentions: [
    {
      id: "task-123",
      type: "tasks",
      data: {
        id: "task-123",
        title: "Implement user authentication",
        status: "in-progress",
        assignee: "Alice"
      },
      position: { start: 23, end: 32 }
    }
  ]
}
```

## Advanced Mention Features

### Filtering and Validation

You can implement custom filtering and validation logic within your mention providers:

```tsx
import { useMentionProvider } from 'cedar-os';

function ValidatedMentions() {
	const currentUser = getCurrentUser();

	useMentionProvider({
		id: 'team-users',
		trigger: '@',
		label: 'Team Members',
		icon: '👤',
		getItems: (query) => {
			const allUsers = getAllUsers();

			// Filter by permissions - only show users in same team
			const allowedUsers = allUsers.filter(
				(user) => user.teamId === currentUser.teamId && user.active
			);

			// Filter by search query
			const filtered = query
				? allowedUsers.filter((user) =>
						user.name.toLowerCase().includes(query.toLowerCase())
				  )
				: allowedUsers;

			return filtered.map((user) => ({
				id: user.id,
				label: user.name,
				data: user,
				metadata: {
					icon: '👤',
					color: user.active ? '#10b981' : '#6b7280',
				},
			}));
		},
		toContextEntry: (item) => ({
			id: item.id,
			source: 'mention',
			data: item.data,
			metadata: {
				label: item.label,
				icon: '👤',
				// Validate data is still current
				valid: item.data.active,
			},
		}),
	});

	return <ChatInput />;
}

// Mock functions
function getCurrentUser() {
	return { id: '1', teamId: 'team-a' };
}

function getAllUsers() {
	return [
		{ id: '1', name: 'Alice', teamId: 'team-a', active: true },
		{ id: '2', name: 'Bob', teamId: 'team-a', active: false },
		{ id: '3', name: 'Charlie', teamId: 'team-b', active: true },
	];
}
```

### Rich Context Information

Provide detailed context data through the mention system:

```tsx
import { useMentionProvider } from 'cedar-os';

function RichContextMentions() {
	useMentionProvider({
		id: 'project-tasks',
		trigger: '#',
		label: 'Tasks',
		icon: '✅',
		getItems: async (query) => {
			const tasks = await fetchTasks(query);
			return tasks.map((task) => ({
				id: task.id,
				label: `${task.title} (${task.status})`,
				data: {
					...task,
					// Include rich context data
					assignee: task.assignee,
					dueDate: task.dueDate,
					priority: task.priority,
					comments: task.comments,
				},
				metadata: {
					icon: getTaskIcon(task.status),
					color: getTaskColor(task.priority),
				},
			}));
		},
		toContextEntry: (item) => ({
			id: item.id,
			source: 'mention',
			data: item.data,
			metadata: {
				label: item.label,
				icon: item.metadata?.icon,
				color: item.metadata?.color,
				// Additional metadata for agent context
				type: 'task',
				status: item.data.status,
				priority: item.data.priority,
			},
		}),

		// Rich preview in mention menu
		renderMenuItem: (item) => (
			<div
				className='p-2 border-l-2'
				style={{ borderColor: item.metadata?.color }}>
				<div className='flex items-center justify-between'>
					<span className='font-medium'>{item.data.title}</span>
					<span className='text-xs bg-gray-100 px-2 py-1 rounded'>
						{item.data.status}
					</span>
				</div>
				<div className='text-sm text-gray-500 mt-1'>
					Due: {item.data.dueDate} • Assigned to: {item.data.assignee?.name}
				</div>
			</div>
		),
	});

	return <ChatInput />;
}

// Helper functions
function getTaskIcon(status: string) {
	const icons = {
		todo: '⏳',
		'in-progress': '🔄',
		done: '✅',
		blocked: '🚫',
	};
	return icons[status] || '📋';
}

function getTaskColor(priority: string) {
	const colors = {
		high: '#ef4444',
		medium: '#f59e0b',
		low: '#10b981',
	};
	return colors[priority] || '#6b7280';
}

async function fetchTasks(query: string) {
	// Mock async task fetching
	return [
		{
			id: '1',
			title: 'Fix login bug',
			status: 'in-progress',
			priority: 'high',
			dueDate: '2024-01-15',
			assignee: { name: 'Alice' },
		},
	];
}
```

## Next Steps

* Explore [state access patterns](/state-access/agentic-state-access)
* Learn about [spells and shortcuts](/spells/spells)


# Subscribing State to Input
Source: https://docs.cedarcopilot.com/agent-input-context/subscribing-state

Automatically add state to the agent input context

Cedar's `useSubscribeInputContext` function allows you to automatically make any part of your application state available to AI agents as context. This enables agents to understand your app's current state and provide more relevant, contextual responses.

## useSubscribeInputContext Overview

The `useSubscribeInputContext` function subscribes to local state changes and automatically updates the agent's input context whenever the state changes. This means your AI agent always has access to the most up-to-date information from your application.

### Function Signature

```typescript
function useSubscribeInputContext<T>(
	localState: T,
	mapFn: (state: T) => Record<string, any>,
	options?: {
		icon?: ReactNode;
		color?: string;
		labelField?: string | ((item: unknown) => string);
		order?: number;
	}
): void;
```

### Parameters

* **`localState: T`** - The local state to subscribe to (any type including single values or arrays)
* **`mapFn: (state: T) => Record<string, any>`** - Function that maps your state to context entries
* **`options`** (optional) - Configuration for visual representation and label extraction:
  * `icon?: ReactNode` - Icon to display for this context
  * `color?: string` - Hex color for visual styling
  * `labelField?: string | ((item: unknown) => string)` - How to extract labels from your data
  * `order?: number` - Display order for context badges (lower numbers appear first)

## Basic Usage Example

Here's a simple example with a todo list:

```tsx
import React, { useState } from 'react';
import { useSubscribeInputContext } from 'cedar-os';
import { CheckCircle } from 'lucide-react';

function TodoApp() {
	const [todos, setTodos] = useState([
		{ id: 1, text: 'Buy groceries', completed: false },
		{ id: 2, text: 'Walk the dog', completed: true },
	]);

	// Subscribe todos to input context
	useSubscribeInputContext(
		todos,
		(todoList) => ({
			todos: todoList, // Key 'todos' will be available to the agent
		}),
		{
			icon: <CheckCircle />,
			color: '#10B981', // Green color
			labelField: 'text', // Use the 'text' field as label for each todo
		}
	);

	return (
		<div>
			<TodoList todos={todos} onToggle={setTodos} />
			<ChatInput /> {/* Agent can now see todos in context */}
		</div>
	);
}
```

## Complex State Example

Here's a more advanced example from the Product Roadmap demo:

```tsx
import { useSubscribeInputContext } from 'cedar-os';
import { Box } from 'lucide-react';
import { Node } from 'reactflow';

interface FeatureNodeData {
	title: string;
	status: 'planned' | 'in-progress' | 'completed';
	priority: 'low' | 'medium' | 'high';
	description?: string;
}

function SelectedNodesPanel() {
	const [selected, setSelected] = useState<Node<FeatureNodeData>[]>([]);

	// Subscribe selected nodes to input context
	useSubscribeInputContext(
		selected,
		(nodes: Node<FeatureNodeData>[]) => ({
			selectedNodes: nodes.map((node) => ({
				id: node.id,
				title: node.data.title,
				status: node.data.status,
				priority: node.data.priority,
				description: node.data.description,
			})),
		}),
		{
			icon: <Box />,
			color: '#8B5CF6', // Purple color for selected nodes
			labelField: 'title', // Use title field for labels
		}
	);

	// Update selection when user selects nodes
	useOnSelectionChange({
		onChange: ({ nodes }) => setSelected(nodes),
	});

	return (
		<div>
			<h4>Selected Nodes</h4>
			{selected.map((node) => (
				<div key={node.id}>{node.data.title}</div>
			))}
		</div>
	);
}
```

## Using the labelField Option

The `labelField` option allows you to specify how labels should be extracted from your data. This is especially useful when your data has custom field names or when you need custom label logic.

### labelField as a String

Specify which field to use as the label:

```tsx
const [users, setUsers] = useState([
	{ userId: 'u1', fullName: 'John Doe', email: 'john@example.com' },
	{ userId: 'u2', fullName: 'Jane Smith', email: 'jane@example.com' },
]);

useSubscribeInputContext(users, (userList) => ({ activeUsers: userList }), {
	labelField: 'fullName', // Will use fullName as the label
	icon: <User />,
	color: '#3B82F6',
});
```

### labelField as a Function

Use a function for custom label generation:

```tsx
const [products, setProducts] = useState([
	{ sku: 'P001', name: 'Laptop', price: 999, inStock: true },
	{ sku: 'P002', name: 'Mouse', price: 29, inStock: false },
]);

useSubscribeInputContext(
	products,
	(productList) => ({ inventory: productList }),
	{
		labelField: (product) =>
			`${product.name} (${product.inStock ? 'In Stock' : 'Out of Stock'})`,
		icon: <Package />,
		color: '#10B981',
	}
);
```

### Single Values Support

`useSubscribeInputContext` now supports single values (not just arrays):

```tsx
const [count, setCount] = useState(42);
const [isEnabled, setIsEnabled] = useState(true);
const [userName, setUserName] = useState('Alice');

// Subscribe a number
useSubscribeInputContext(count, (value) => ({ itemCount: value }), {
	icon: <Hash />,
	color: '#F59E0B',
});

// Subscribe a boolean
useSubscribeInputContext(isEnabled, (value) => ({ featureEnabled: value }), {
	icon: <ToggleLeft />,
	color: '#10B981',
});

// Subscribe a string with custom label
useSubscribeInputContext(userName, (value) => ({ currentUser: value }), {
	labelField: (name) => `User: ${name}`,
	icon: <User />,
	color: '#8B5CF6',
});
```

## Controlling Display Order

The `order` property allows you to control the display order of context badges in the UI. This is useful when you have multiple context subscriptions and want to prioritize their visibility.

### How Order Works

* **Lower numbers appear first**: `order: 1` will appear before `order: 10`
* **Default behavior**: Items without an order are treated as having the maximum order value
* **Stable sorting**: Items with the same order maintain their original relative position

### Basic Order Example

```tsx
function PrioritizedContext() {
	const [criticalAlerts, setCriticalAlerts] = useState([]);
	const [normalTasks, setNormalTasks] = useState([]);
	const [archivedItems, setArchivedItems] = useState([]);

	// Critical alerts appear first (order: 1)
	useSubscribeInputContext(
		criticalAlerts,
		(alerts) => ({ criticalAlerts: alerts }),
		{
			icon: <AlertCircle />,
			color: '#EF4444',
			order: 1, // Highest priority - appears first
		}
	);

	// Normal tasks appear second (order: 10)
	useSubscribeInputContext(normalTasks, (tasks) => ({ activeTasks: tasks }), {
		icon: <CheckCircle />,
		color: '#3B82F6',
		order: 10, // Medium priority
	});

	// Archived items appear last (order: 100)
	useSubscribeInputContext(
		archivedItems,
		(items) => ({ archivedItems: items }),
		{
			icon: <Archive />,
			color: '#6B7280',
			order: 100, // Low priority - appears last
		}
	);

	return <ChatInput />;
}
```

### Complex Order Example with Mention Providers

When combining `useSubscribeInputContext` with mention providers, you can create a well-organized context display:

```tsx
import { useStateBasedMentionProvider } from 'cedar-os';

function OrganizedWorkspace() {
	const [selectedNodes, setSelectedNodes] = useState([]);
	const [user, setUser] = useState(null);

	// User context appears first (order: 1)
	useSubscribeInputContext(user, (userData) => ({ currentUser: userData }), {
		icon: <User />,
		color: '#8B5CF6',
		labelField: 'name',
		order: 1, // User info always visible first
	});

	// Selected items appear second (order: 5)
	useSubscribeInputContext(
		selectedNodes,
		(nodes) => ({ selectedNodes: nodes }),
		{
			icon: <Box />,
			color: '#F59E0B',
			labelField: 'title',
			order: 5, // Selection context after user
		}
	);

	// Mention provider for all nodes (order: 10)
	useStateBasedMentionProvider({
		stateKey: 'nodes',
		trigger: '@',
		labelField: 'title',
		description: 'All nodes',
		icon: <Box />,
		color: '#3B82F6',
		order: 10, // Available nodes after selections
	});

	// Mention provider for connections (order: 20)
	useStateBasedMentionProvider({
		stateKey: 'edges',
		trigger: '@',
		labelField: (edge) => `${edge.source} → ${edge.target}`,
		description: 'Connections',
		icon: <ArrowRight />,
		color: '#10B981',
		order: 20, // Connections appear last
	});

	return <ChatInput />;
}
```

### Order Best Practices

1. **Use consistent spacing**: Leave gaps between order values (1, 10, 20) to allow for future insertions
2. **Group related contexts**: Give similar contexts adjacent order values
3. **Prioritize by importance**: Most relevant context should have lower order values
4. **Document your ordering**: Comment why certain items have specific orders

```tsx
// Order schema for our app:
// 1-9: User and session data (critical context)
// 10-19: Current selections and active state
// 20-49: General application state
// 50-99: Historical or computed data
// 100+: Low priority or debug information

useSubscribeInputContext(sessionData, mapper, { order: 1 }); // Critical
useSubscribeInputContext(selectedItems, mapper, { order: 10 }); // Active
useSubscribeInputContext(appState, mapper, { order: 20 }); // General
useSubscribeInputContext(history, mapper, { order: 50 }); // Historical
useSubscribeInputContext(debugInfo, mapper, { order: 100 }); // Debug
```

### Default Label Extraction

When no `labelField` is specified, the function looks for labels in this order:

1. `title` field
2. `label` field
3. `name` field
4. `id` field
5. String representation of the value

## How It Works Automatically

When you use `useSubscribeInputContext`, here's what happens automatically:

1. **State Monitoring**: The function uses React's `useEffect` to monitor changes to your `localState`
2. **Context Mapping**: When state changes, your `mapFn` transforms the state into context entries
3. **Store Update**: The mapped context is automatically added to Cedar's internal store
4. **Agent Access**: When the user sends a message, the agent receives this context along with the message

### Context Structure

The context entries follow this structure:

```typescript
interface ContextEntry {
	id: string;
	source: 'mention' | 'subscription' | 'manual';
	data: any;
	metadata?: {
		label?: string;
		icon?: ReactNode;
		color?: string;
		[key: string]: any;
	};
}
```

When using `useSubscribeInputContext`, entries are automatically marked with `source: 'subscription'`.

## Multiple State Subscriptions

You can subscribe multiple pieces of state:

```tsx
function MyApp() {
	const [user, setUser] = useState(null);
	const [preferences, setPreferences] = useState({});
	const [currentPage, setCurrentPage] = useState('/dashboard');

	// Subscribe user data
	useSubscribeInputContext(
		user,
		(userData) => ({
			currentUser: userData
				? {
						id: userData.id,
						name: userData.name,
						role: userData.role,
				  }
				: null,
		}),
		{
			icon: <User />,
			color: '#3B82F6',
		}
	);

	// Subscribe preferences
	useSubscribeInputContext(
		preferences,
		(prefs) => ({
			userPreferences: prefs,
		}),
		{
			icon: <Settings />,
			color: '#6B7280',
		}
	);

	// Subscribe navigation state
	useSubscribeInputContext(
		currentPage,
		(page) => ({
			currentPage: {
				path: page,
				timestamp: new Date().toISOString(),
			},
		}),
		{
			icon: <Navigation />,
			color: '#F59E0B',
		}
	);

	return <YourAppContent />;
}
```

## Best Practices

### 1. Transform Sensitive Data

Don't expose sensitive information to the agent:

```tsx
useSubscribeInputContext(userProfile, (profile) => ({
	user: {
		name: profile.name,
		tier: profile.subscriptionTier,
		preferences: profile.preferences,
		// Don't include: email, password, tokens, etc.
	},
}));
```

### 2. Use Meaningful Keys

Choose descriptive keys for your context:

```tsx
useSubscribeInputContext(shoppingCart, (cart) => ({
	shoppingCart: cart.items, // Clear and descriptive
	cartTotal: cart.total,
	cartItemCount: cart.items.length,
}));
```

### 3. Optimize Large Data Sets

For large data sets, consider filtering or summarizing:

```tsx
useSubscribeInputContext(allTransactions, (transactions) => ({
	recentTransactions: transactions
		.slice(0, 10) // Only last 10 transactions
		.map((t) => ({
			id: t.id,
			amount: t.amount,
			date: t.date,
			// Exclude detailed metadata
		})),
	transactionSummary: {
		total: transactions.length,
		totalAmount: transactions.reduce((sum, t) => sum + t.amount, 0),
	},
}));
```

## Visual Customization

The `options` parameter allows you to customize how the context appears in the UI:

```tsx
import { Star, AlertCircle, CheckCircle } from 'lucide-react';

// Different colors and icons for different priorities
useSubscribeInputContext(
	highPriorityTasks,
	(tasks) => ({ highPriorityTasks: tasks }),
	{
		icon: <AlertCircle />,
		color: '#EF4444', // Red for high priority
	}
);

useSubscribeInputContext(
	completedTasks,
	(tasks) => ({ completedTasks: tasks }),
	{
		icon: <CheckCircle />,
		color: '#10B981', // Green for completed
	}
);

useSubscribeInputContext(starredItems, (items) => ({ starredItems: items }), {
	icon: <Star />,
	color: '#F59E0B', // Yellow for starred
});
```

## Integration with Chat Input

The subscribed context automatically becomes available to your AI agent when using Cedar's chat components:

```tsx
import { ChatInput } from 'cedar-os-components';

function MyChat() {
	// Your useSubscribeInputContext calls here...

	return (
		<div>
			{/* Context is automatically included when user sends messages */}
			<ChatInput placeholder='Ask me about your todos, selected nodes, or anything else...' />
		</div>
	);
}
```

The agent will receive the context in a structured format and can reference it when generating responses, making the conversation more contextual and relevant to your application's current state.


# Blog
Source: https://docs.cedarcopilot.com/blog/index



We can yap about AI interfaces and UX for hours.

Therefore, we have some thoughts.

<CardGroup cols={2}>
  <Card title="How we became obsessed with AI interfaces" icon="pen" href="/blog/on-advancing-ai-interfaces">
    On advancing AI interfaces: why chat is just the beginning, and what comes
    next for truly native AI UX.
  </Card>
</CardGroup>


# Changelog
Source: https://docs.cedarcopilot.com/changelog

See what we're up to :-D

<Update label="August 2025" description="v0.1.0">
  Cedar-OS launches its first public release! 🎉
</Update>


# Chat Components (Advanced)
Source: https://docs.cedarcopilot.com/chat/chat-components

Deep dive into individual chat components for advanced customization

Cedar provides you with fully working chat out of the box, so this documentation is only needed if you want to customize or build your own chat interface. We break down every component so you can understand how they work together and modify them as needed.

## Chat Input Components

The chat input system consists of several key components that handle user input, mentions, and context management.

<img src="https://vrhlhwfhghqbpdpfnpdq.supabase.co/storage/v1/object/public/logos/caption.gif" alt="Bottom Center Chat Demo" />

### ContextBadgeRow

The `ContextBadgeRow` component displays selected context items (like mentions, state, or custom context) as badges above the input field. Users can click these badges to remove them from the context.

```tsx
import { ContextBadgeRow } from '@/chatInput/ContextBadgeRow';

<ContextBadgeRow editor={editor} />;
```

**Key Features:**

* Displays context entries as removable badges
* Shows icons and colors from mention providers
* Handles removal of mentions from both context and editor
* Supports custom rendering through mention providers
* Automatically renders badges for all registered mention types

**Props:**

* `editor` - The Tiptap editor instance for mention removal

### Editor Section (Tiptap Integration)

Cedar uses [Tiptap](https://tiptap.dev/) as the rich text editor for chat input. This provides powerful text editing capabilities with support for mentions, formatting, and extensibility.

```tsx
import { useCedarEditor } from 'cedar-os';
import { EditorContent } from '@tiptap/react';

const { editor, isEditorEmpty, handleSubmit } = useCedarEditor({
	onSubmit,
	onFocus: handleFocus,
	onBlur: handleBlur,
});

<EditorContent
	editor={editor}
	className='prose prose-sm max-w-none focus:outline-none'
/>;
```

**Key Features:**

* Rich text editing with markdown support
* Mention system integration (@, #, / triggers)
* Keyboard shortcuts and navigation
* Multi-line input support
* Extensible through Tiptap extensions

**Editor Extensions Included:**

* Document, Paragraph, Text (core)
* HardBreak for line breaks
* Mention extension for @mentions
* History for undo/redo
* Placeholder support

### MentionList

The `MentionList` component renders the dropdown menu that appears when users type mention triggers (like @ or #). It displays available mention options with custom rendering support.

```tsx
import MentionList from '@/components/chatInput/MentionList';

<MentionList items={mentionItems} command={selectMention} />;
```

**Key Features:**

* Keyboard navigation (arrow keys, enter)
* Custom rendering through mention providers
* Color theming based on mention metadata
* Hover and selection states
* Provider-specific item rendering

**Props:**

* `items` - Array of `MentionItem` objects to display
* `command` - Function called when an item is selected

For more details on setting up mentions, see the [Mentions documentation](/agent-input-context/mentions).

## Message Components

Message components handle the display and rendering of chat messages in different formats and layouts.

### ChatRenderer

The `ChatRenderer` is the core component responsible for rendering individual messages based on their type. It supports both built-in message types and custom message renderers.

```tsx
import { ChatRenderer } from '@/chatMessages/ChatRenderer';

<ChatRenderer message={message} />;
```

**Built-in Message Types:**

* `text` - Standard text messages with markdown support
* `dialogue_options` - Interactive option buttons
* `multiple_choice` - Choice buttons with keyboard shortcuts
* `todolist` - Interactive todo list items
* `ticker` - Horizontal scrolling content cards
* `slider` - Slider input for numeric values

**Key Features:**

* Automatic message type detection
* Custom renderer registration support
* Markdown rendering with code block support
* Consistent styling across message types
* Dark/light mode support

**Custom Renderers:**

```tsx
// Register a custom message renderer
store.registerMessageRenderer({
	type: 'my-custom-type',
	renderer: ({ message }) => <div>{message.content}</div>,
});
```

### ChatBubbles

The `ChatBubbles` component manages the scrollable message container with animations and auto-scrolling behavior.

```tsx
import { ChatBubbles } from '@/chatMessages/ChatBubbles';

<ChatBubbles maxHeight='400px' className='custom-chat-container' />;
```

**Key Features:**

* Auto-scroll to latest messages
* Smooth animations for new messages
* Typing indicator support
* Consecutive message grouping
* Custom height and styling options
* Optimized scrolling performance

**Props:**

* `maxHeight` - Maximum height (e.g., "300px", "60vh")
* `className` - Additional CSS classes

### CaptionMessages

The `CaptionMessages` component is specialized for caption-style chat interfaces, showing only the latest message with enhanced typography and interactions.

```tsx
import CaptionMessages from '@/chatMessages/CaptionMessages';

<CaptionMessages showThinking={true} />;
```

**Key Features:**

* Shows only the latest relevant message
* Enhanced typography with typewriter effects
* Shimmer text for thinking states
* Interactive elements (buttons, sliders)
* Keyboard shortcut integration
* Optimized for overlay/caption use cases

**Props:**

* `showThinking` - Whether to show user messages and thinking states

**Message Type Handling:**

* `text` - Typewriter animation with "Cedar:" prefix
* `dialogue_options` - 3D button grid with hover effects
* `ticker` - Horizontal scrolling with "Next Step" button
* `multiple_choice` - Inline choice buttons with shortcuts
* `slider` - Interactive 3D slider component

## Container Components

Container components provide the structural layout and positioning for different chat interface styles.

### FloatingContainer

The `FloatingContainer` provides a floating, resizable chat window that can be positioned at different screen locations.

```tsx
import { FloatingContainer } from '@/structural/FloatingContainer';

<FloatingContainer
	isActive={isOpen}
	position='bottom-right'
	dimensions={{
		width: 400,
		height: 600,
		minWidth: 300,
		minHeight: 400,
	}}
	resizable={true}
	onResize={(width, height) => console.log('Resized:', width, height)}>
	{/* Chat content */}
</FloatingContainer>;
```

**Key Features:**

* Multiple positioning options (bottom-left, bottom-right, bottom-center)
* Resizable with drag handles (corner positions only)
* Smooth animations with spring physics
* Responsive design with mobile adaptations
* Constraint-based resizing with min/max limits

**Props:**

* `isActive` - Whether the container is visible
* `position` - Screen position ('bottom-left' | 'bottom-right' | 'bottom-center')
* `dimensions` - Size configuration object
* `resizable` - Enable/disable resize functionality
* `onResize` - Callback for size changes
* `className` - Additional CSS classes

**Dimensions Object:**

```tsx
interface FloatingDimensions {
	width?: number | string;
	height?: number | string;
	minWidth?: number | string;
	minHeight?: number | string;
	maxWidth?: number | string;
	maxHeight?: number | string;
}
```

### SidePanelContainer

The `SidePanelContainer` creates a side panel that pushes the main content to make room for the chat interface.

```tsx
import { SidePanelContainer } from '@/structural/SidePanelContainer';

<SidePanelContainer
	isActive={isOpen}
	side='right'
	dimensions={{
		width: 600,
		minWidth: 300,
		maxWidth: 800,
	}}
	resizable={true}
	topOffset={64} // Account for navbar
	onResize={(width) => console.log('Panel width:', width)}
	panelContent={<ChatInterface />}>
	{/* Main page content */}
	<div>Your app content here</div>
</SidePanelContainer>;
```

**Key Features:**

* Left or right side positioning
* Content area automatically adjusts padding
* Resizable with drag handle
* Mobile-responsive (full screen on mobile)
* Top offset support for fixed headers
* Smooth slide animations

**Props:**

* `isActive` - Whether the panel is open
* `side` - Panel position ('left' | 'right')
* `panelContent` - React node to render in the panel
* `dimensions` - Width configuration object
* `resizable` - Enable/disable resize functionality
* `topOffset` - Top spacing in pixels (for fixed headers)
* `onResize` - Callback for width changes
* `className` - Additional CSS classes for main content
* `panelClassName` - Additional CSS classes for panel

**Dimensions Object:**

```tsx
interface SidePanelDimensions {
	width?: number;
	minWidth?: number;
	maxWidth?: number;
}
```

## Component Integration

These components work together to create the complete chat experience:

1. **Input Flow**: `ContextBadgeRow` → Tiptap Editor → `MentionList`
2. **Message Flow**: `ChatRenderer` → `ChatBubbles` or `CaptionMessages`
3. **Layout Flow**: `FloatingContainer` or `SidePanelContainer` wraps the entire chat

Each component is designed to be used independently or as part of the complete system, giving you flexibility in how you build your chat interface.

## Next Steps

* Explore [Custom Message Rendering](/chat/custom-message-rendering) to create your own message types
* Learn about [Mentions](/agent-input-context/mentions) to add contextual references
* Check out [Streaming](/chat/streaming) for real-time message updates


# Chat Overview
Source: https://docs.cedarcopilot.com/chat/chat-overview

Get started with Cedar chat components and understand how they work

With one component, you can download a fully working chat. You also get all of the individual components of it, so you can customise styling, rendering, and functionality.

## Quick Start

To get started, choose one of the 3 types of chat. See [cedarcopilot.com/examples/cedar-playground](https://cedarcopilot.com/examples/cedar-playground) to see them in action.

### Chat Types

**Caption Chat**
A caption-style chat interface that appears at the bottom center of the screen, perfect for overlay-style interactions. Great for AI assistants that provide contextual help without taking up dedicated screen space.

We realised that in a conversation, you don't need to see an entire history of the chat all the time, just the latest message (of course, we give the user the option to see past texts). This gives a much more central and embedded experience for agentic interactions

<img src="https://vrhlhwfhghqbpdpfnpdq.supabase.co/storage/v1/object/public/logos/caption.gif" alt="Bottom Center Chat Demo" />

**Floating Chat**
A floating chat window that can be positioned on the left or right side of the screen with expand/collapse functionality. Perfect for assistance that doesn't interfere with the main application.

<img src="https://vrhlhwfhghqbpdpfnpdq.supabase.co/storage/v1/object/public/logos/floating.gif" alt="Floating Chat Demo" />

**Side Panel Chat**\
A side panel chat that pushes your main content to make room for the chat interface. This is if you want the chat to not overlap with any elements, and always have its own dedicated spot.

<img src="https://vrhlhwfhghqbpdpfnpdq.supabase.co/storage/v1/object/public/logos/sidepanel.gif" alt="Side Panel Chat Demo" />

<CodeGroup>
  ```tsx Floating Chat
  import { FloatingCedarChat } from '@/chatComponents/FloatingCedarChat';

  function App() {
  	return (
  		<div>
  			{/* Your main content */}
  			<FloatingCedarChat
  				side='right'
  				title='Assistant'
  				collapsedLabel='How can I help you today?'
  				dimensions={{
  					width: 400,
  					height: 600,
  					minWidth: 350,
  					minHeight: 400,
  				}}
  				resizable={true}
  			/>
  		</div>
  	);
  }

  // Props interface
  interface FloatingCedarChatProps {
  	side?: 'left' | 'right';
  	title?: string;
  	collapsedLabel?: string;
  	companyLogo?: React.ReactNode;
  	dimensions?: {
  		width?: number;
  		height?: number;
  		minWidth?: number;
  		minHeight?: number;
  		maxWidth?: number;
  		maxHeight?: number;
  	};
  	resizable?: boolean;
  }
  ```

  ```tsx Side Panel Chat
  import { SidePanelCedarChat } from '@/chatComponents/SidePanelCedarChat';

  function App() {
  	return (
  		<SidePanelCedarChat
  			side='right'
  			title='Chat Assistant'
  			dimensions={{
  				width: 600,
  				minWidth: 300,
  				maxWidth: 800,
  			}}
  			resizable={true}
  			topOffset={64} // Offset for navbar height (optional)
  			className='' // Additional styling (optional)
  		>
  			{/* Your main page content goes here */}
  			<div className='p-8'>
  				<h1>Your App Content</h1>
  				<p>This content will be pushed to the side when chat opens.</p>
  			</div>
  		</SidePanelCedarChat>
  	);
  }

  // Props interface
  interface SidePanelCedarChatProps {
  	children?: React.ReactNode; // Page content to wrap
  	side?: 'left' | 'right';
  	title?: string;
  	collapsedLabel?: string;
  	showCollapsedButton?: boolean; // Control whether to show the collapsed button
  	companyLogo?: React.ReactNode;
  	dimensions?: {
  		width?: number;
  		minWidth?: number;
  		maxWidth?: number;
  	};
  	resizable?: boolean;
  	className?: string; // Additional CSS classes for positioning
  	topOffset?: number; // Top offset in pixels (e.g., for navbar height)
  }
  ```

  ```tsx Bottom Center Chat
  import { CedarCaptionChat } from '@/chatComponents/CedarCaptionChat';

  function App() {
  	return (
  		<div>
  			{/* Your main content */}
  			<CedarCaptionChat
  				dimensions={{
  					width: 600,
  					maxWidth: 800,
  				}}
  				showThinking={true}
  			/>
  		</div>
  	);
  }

  // Props interface
  interface CedarCaptionChatProps {
  	dimensions?: {
  		width?: number;
  		maxWidth?: number;
  	};
  	className?: string;
  	showThinking?: boolean; // Whether to show "Thinking..." state and user messages
  }
  ```
</CodeGroup>

## Understanding & Customising Chat

Chat works through 2 main modules working together: **AgentInputContext** and **MessagesSlice**.

### AgentInputContext

[**AgentInputContext**](/agent-input-context/agent-input-context) manages everything that we pass into the agent. This includes not only the user message, but also:

* **State** that the agent is subscribed to ([learn more about state access](/chat/subscribing-state))
* **User @mentions** of specific items in your application ([learn more about mentions](/chat/mentions))
* **Conversation history** and contextual information
* **Custom context** you provide programmatically, such as user role

This ensures your AI agent has full context about your application's current state and what the user is specifically referring to.

### MessagesSlice

**MessagesSlice** takes care of the saving and rendering of messages. It handles:

* **Message storage** and conversation history
* **Message rendering** with support for different message types
* **Streaming responses** from AI providers
* **Custom message components** for rich interactions

### Additional Customization

* **[Streaming](/chat/streaming)** - Enable real-time response streaming
* **[Configuring the Cedar Editor](/chat/configuring-cedar-editor)** - Customize the chat input experience
* **[Custom Message Rendering](/chat/custom-message-rendering)** - Create custom message types and interactions
* **[Understanding Chat Components](/chat/chat-components)** - Deep dive into individual chat components for advanced customization

## Next Steps

1. **Try it out**: Visit the [Cedar Playground](https://cedarcopilot.com/examples/cedar-playground) to see all chat types in action
2. **Choose your style**: Pick the chat type that best fits your application
3. **Add context**: Set up state access and mentions to make your agent context-aware
4. **Customize**: Tailor the appearance and behavior to match your application's needs


# Configuring the Cedar Editor
Source: https://docs.cedarcopilot.com/chat/configuring-cedar-editor

Customize the chat input editor behavior and features

The Cedar editor is the rich text input component that powers chat interactions. It supports mentions, keyboard shortcuts, auto-completion, and extensive customization options.

## Basic Configuration

Configure the editor through the `useCedarEditor` hook or ChatInput props:

```tsx
import { ChatInput } from 'cedar-os-components/chatInput';

function CustomChat() {
	return <ChatInput className='custom-chat-input' />;
}
```

## Advanced Editor Configuration

Use the `useCedarEditor` hook for fine-grained control:

```tsx
import { useCedarEditor } from 'cedar-os';
import { EditorContent } from '@tiptap/react';

function AdvancedChat() {
	const { editor, isEditorEmpty, handleSubmit } = useCedarEditor({
		placeholder: 'Ask me anything...',
		onFocus: () => console.log('Editor focused'),
		onBlur: () => console.log('Editor blurred'),
	});

	return (
		<div>
			<EditorContent editor={editor} />
			<button onClick={() => editor?.commands.focus()}>Focus Editor</button>
			<button onClick={handleSubmit} disabled={isEditorEmpty}>
				Submit
			</button>
		</div>
	);
}
```

## Keyboard Shortcuts

Cedar editor comes with built-in keyboard shortcuts:

| Shortcut        | Action            |
| --------------- | ----------------- |
| `Enter`         | Submit message    |
| `Shift + Enter` | New line          |
| `@`             | Trigger mentions  |
| `Tab`           | Focus editor      |
| `M`             | Toggle microphone |

### Custom Shortcuts

The editor uses TipTap under the hood, so you can extend it with custom keyboard shortcuts by configuring the editor extensions directly.

## Styling and Themes

Customize the editor appearance:

```tsx
<ChatInput className='custom-chat-input' />
```

You can customize the appearance using CSS classes. The component uses Tailwind CSS classes internally, so you can override them:

```css
.custom-chat-input {
	background: #f8f9fa;
	border-radius: 12px;
	padding: 16px;
	min-height: 120px;
}

.custom-chat-input .ProseMirror {
	font-family: 'Inter', sans-serif;
	font-size: 14px;
	line-height: 1.5;
}
```

## Editor Events

Handle various editor events:

```tsx
<ChatInput
	handleFocus={() => console.log('Editor focused')}
	handleBlur={() => console.log('Editor blurred')}
	className='my-chat-input'
/>
```

## Content Validation

Content validation can be implemented using the `useCedarEditor` hook:

```tsx
import { useCedarEditor } from 'cedar-os';
import { EditorContent } from '@tiptap/react';

function ValidatedChat() {
	const { editor, isEditorEmpty, handleSubmit } = useCedarEditor({
		placeholder: 'Type your message...',
		onFocus: () => console.log('Editor focused'),
		onBlur: () => console.log('Editor blurred'),
	});

	const handleCustomSubmit = () => {
		if (!editor) return;

		const text = editor.getText();

		if (text.length > 1000) {
			console.error('Message too long');
			return;
		}
		if (text.includes('@everyone')) {
			console.error('Mass mentions are not allowed');
			return;
		}

		// Process valid message
		console.log('Valid message:', text);
		handleSubmit();
	};

	return (
		<div>
			<EditorContent editor={editor} />
			<button onClick={handleCustomSubmit} disabled={isEditorEmpty}>
				Submit
			</button>
		</div>
	);
}

<ChatInput />;
```

## Next Steps

* Learn about [custom message rendering](/chat/custom-message-rendering)
* Explore [agent input context](/agent-input-context/agent-input-context)


# Custom Message Rendering
Source: https://docs.cedarcopilot.com/chat/custom-message-rendering

Create custom message components and renderers

Cedar allows you to completely customize how messages are displayed in your chat interface. You can create custom message components, override default renderers, and add interactive elements to messages.

## Custom Message Components

Create custom components for specific message types:

```tsx
import { useMessageRenderer } from 'cedar-os';

// Custom component for code execution results
function CodeResultMessage({ message }) {
	return (
		<div className='bg-gray-900 text-green-400 p-4 rounded-lg font-mono'>
			<div className='text-xs text-gray-500 mb-2'>Execution Result</div>
			<pre>{message.content}</pre>
			<button
				className='mt-2 text-blue-400 hover:text-blue-300'
				onClick={() => navigator.clipboard.writeText(message.content)}>
				Copy Result
			</button>
		</div>
	);
}

// Custom component for interactive surveys
function SurveyMessage({ message, onResponse }) {
	const [selectedOption, setSelectedOption] = useState(null);

	return (
		<div className='border rounded-lg p-4'>
			<h3 className='font-semibold mb-3'>{message.content.question}</h3>
			<div className='space-y-2'>
				{message.content.options.map((option, index) => (
					<button
						key={index}
						className={`block w-full text-left p-2 rounded ${
							selectedOption === index ? 'bg-blue-100' : 'hover:bg-gray-50'
						}`}
						onClick={() => {
							setSelectedOption(index);
							onResponse(option);
						}}>
						{option}
					</button>
				))}
			</div>
		</div>
	);
}
```

## Registering Custom Renderers

Register your custom components with the message renderer:

```tsx
import { CedarCopilot } from 'cedar-os';

function App() {
	const customRenderers = {
		'code-result': CodeResultMessage,
		survey: SurveyMessage,
		'file-upload': FileUploadMessage,
		chart: ChartMessage,
	};

	return (
		<CedarCopilot llmProvider={llmProvider} messageRenderers={customRenderers}>
			{/* Your app content */}
		</CedarCopilot>
	);
}
```

## Message Renderer Hook

Use the `useMessageRenderer` hook for advanced customization:

```tsx
import { useMessageRenderer } from 'cedar-os';

function CustomChatBubbles() {
	const { renderMessage, registerRenderer } = useMessageRenderer();

	// Register a new renderer
	useEffect(() => {
		registerRenderer('todo-list', ({ message }) => (
			<TodoListMessage
				items={message.content.items}
				onToggle={(index) => {
					// Handle todo toggle
				}}
			/>
		));
	}, [registerRenderer]);

	return (
		<div className='messages-container'>
			{messages.map((message) => (
				<div key={message.id} className='message-wrapper'>
					{renderMessage(message)}
				</div>
			))}
		</div>
	);
}
```

## Message Types and Structure

Messages in Cedar follow this structure:

```tsx
interface Message {
	id: string;
	type: string; // 'text', 'image', 'file', or custom type
	role: 'user' | 'assistant' | 'system';
	content: any; // Content varies by type
	timestamp: Date;
	metadata?: {
		streaming?: boolean;
		error?: string;
		[key: string]: any;
	};
}
```

## Built-in Message Types

Cedar provides several built-in message renderers:

### Text Messages

```tsx
{
  type: 'text',
  content: 'Hello, how can I help you?'
}
```

### Image Messages

```tsx
{
  type: 'image',
  content: {
    url: 'https://example.com/image.jpg',
    alt: 'Description',
    caption: 'Optional caption'
  }
}
```

### File Messages

```tsx
{
  type: 'file',
  content: {
    filename: 'document.pdf',
    size: 1024000,
    url: 'https://example.com/document.pdf',
    type: 'application/pdf'
  }
}
```

## Conditional Rendering

Render different components based on message properties:

```tsx
function ConditionalRenderer({ message }) {
	const { renderMessage } = useMessageRenderer();

	// Custom logic for rendering based on user role
	if (message.role === 'system') {
		return <SystemNotification message={message} />;
	}

	// Custom rendering for messages with attachments
	if (message.metadata?.hasAttachments) {
		return <MessageWithAttachments message={message} />;
	}

	// Fall back to default renderer
	return renderMessage(message);
}
```

## Interactive Message Actions

Add interactive elements to messages:

```tsx
function InteractiveMessage({ message }) {
	const { addMessage, callLLM } = useCedarStore();

	const handleFollowUp = async (question) => {
		addMessage({
			type: 'text',
			role: 'user',
			content: question,
		});

		await callLLM({
			prompt: question,
			context: message.content, // Use original message as context
		});
	};

	return (
		<div className='message-content'>
			<p>{message.content}</p>

			<div className='message-actions mt-2 space-x-2'>
				<button
					onClick={() => handleFollowUp('Can you explain this further?')}
					className='text-blue-600 hover:text-blue-800 text-sm'>
					Explain Further
				</button>
				<button
					onClick={() => handleFollowUp('Give me an example')}
					className='text-blue-600 hover:text-blue-800 text-sm'>
					Show Example
				</button>
			</div>
		</div>
	);
}
```

## Next Steps

* Learn about [agent input context](/chat/agent-input-context)
* Explore [subscribing to state](/chat/subscribing-state)


# Message Storage Configuration
Source: https://docs.cedarcopilot.com/chat/message-storage-configuration

Configure how Cedar persists chat messages and manages threads

Cedar provides a flexible storage system for managing chat messages with automatic persistence and **automatic thread management**. By default, Cedar uses **no storage** - no persistence unless you configure a storage adapter.

# Quick Start

Cedar works out of the box with no storage (no persistence). To enable persistence, configure a storage adapter.

# Storage Options

Cedar supports three storage adapters:

1. **Local Storage** (default) - Browser localStorage
2. **No Storage** - Disables persistence
3. **Custom Storage** - Your own implementation

Configure storage by passing the `messageStorage` prop to CedarCopilot:

<CodeGroup>
  ```typescript Local Storage (Default)
  import { CedarCopilot } from 'cedar-os';

  function App() {
  	return (
  		<CedarCopilot
  			// No storage prop needed - uses local storage by default.
  			// Optionally pass in user ID
  			userId='user-123'>
  			<YourChatComponent />
  		</CedarCopilot>
  	);
  }
  ```

  ```typescript Local Storage (Custom Key)
  import { CedarCopilot } from 'cedar-os';

  function App() {
  	return (
  		<CedarCopilot
  			messageStorage={{
  				type: 'local',
  				options: {
  					key: 'my-app', // Optional: defaults to 'cedar'
  				},
  			}}
  			userId='user-123'>
  			<YourChatComponent />
  		</CedarCopilot>
  	);
  }
  ```

  ```typescript No Storage
  import { CedarCopilot } from 'cedar-os';

  function App() {
  	return (
  		<CedarCopilot messageStorage={{ type: 'none' }} userId='user-123'>
  			<YourChatComponent />
  		</CedarCopilot>
  	);
  }
  ```

  ```typescript Custom Storage
  import { CedarCopilot } from 'cedar-os';

  const myCustomAdapter = {
  	async listThreads(userId) {
  		return await myDatabase.getThreads(userId);
  	},
  	async loadMessages(userId, threadId) {
  		return await myDatabase.getMessages(userId, threadId);
  	},
  	async persistMessage(userId, threadId, message) {
  		await myDatabase.appendMessage(userId, threadId, message);
  	},
  };

  function App() {
  	return (
  		<CedarCopilot
  			messageStorage={{
  				type: 'custom',
  				adapter: myCustomAdapter,
  			}}
  			userId='user-123'>
  			<YourChatComponent />
  		</CedarCopilot>
  	);
  }
  ```
</CodeGroup>

# Additional Configuration Requirements by Storage Type

### Local Storage & No Storage

**No additional configuration needed** - these options work out of the box.

### Custom Storage

Implement your own storage solution by providing a custom adapter:

<CodeGroup>
  ```typescript Interface
  interface BaseStorageAdapter {
  	// Required methods
  	loadMessages(
  		userId: string | null | undefined,
  		threadId: string
  	): Promise<Message[]>;
  	persistMessage(
  		userId: string | null | undefined,
  		threadId: string,
  		message: Message
  	): Promise<Message>; // returns the saved message

  	// Optional thread methods
  	listThreads?(userId?: string | null): Promise<ThreadMeta[]>;
  	createThread?(
  		userId: string | null | undefined,
  		threadId: string,
  		meta: ThreadMeta
  	): Promise<ThreadMeta>; // returns new meta
  	updateThread?(
  		userId: string | null | undefined,
  		threadId: string,
  		meta: ThreadMeta
  	): Promise<ThreadMeta>; // returns updated meta
  	deleteThread?(
  		userId: string | null | undefined,
  		threadId: string
  	): Promise<ThreadMeta | undefined>; // returns deleted meta (optional)

  	// Optional message methods
  	updateMessage?(
  		userId: string | null | undefined,
  		threadId: string,
  		message: Message
  	): Promise<Message>; // returns updated message
  	deleteMessage?(
  		userId: string | null | undefined,
  		threadId: string,
  		messageId: string
  	): Promise<Message | undefined>; // returns deleted message (optional)
  }
  ```

  All mutator/creator functions return the object they operate on, making it easy to work with the freshly-persisted data (IDs, timestamps, etc.).

  ```typescript Example Implementation
  import { CedarCopilot } from 'cedar-os';

  // Your custom storage implementation
  const myCustomAdapter = {
  	async listThreads(userId) {
  		return await myDatabase.getThreads(userId);
  	},

  	async loadMessages(userId, threadId) {
  		return await myDatabase.getMessages(userId, threadId);
  	},

  	async persistMessage(userId, threadId, message) {
  		await myDatabase.appendMessage(userId, threadId, message);
  	},
  };

  function App() {
  	return (
  		<CedarCopilot
  			messageStorage={{
  				type: 'custom',
  				adapter: myCustomAdapter,
  			}}
  			userId='user-123' // User ID passed in initial configuration
  		>
  			<YourChatComponent />
  		</CedarCopilot>
  	);
  }
  ```
</CodeGroup>

## How Default Message Storage Works

Cedar automatically handles message storage operations in the following way:

**Thread & Message Loading:**

* **When user ID changes**: Automatically loads all threads for that user
* **When storage adapter changes**: Loads threads from the new storage system
* **When the thread ID changes**: Automatically loads in all messages for that thread

<Info>
  Best practice: make your **load** endpoints idempotent. Implementations of
  `listThreads` or `loadMessages`should **NOT** perform any mutations such as
  creating threads or messages. Cedar may call these functions multiple times
  during normal operation; if they create data each time you could end up with
  duplicated threads or messages.
</Info>

**Message Persistence & Thread Updates:**

* **User messages**: Persisted immediately when sent via `sendMessage()` (internally calls `store.persistMessage`).
* **Assistant responses**:
  * **Non-streaming**: Persisted right after the LLM reply finishes.
  * **Streaming**: Buffered during streaming and persisted once when the stream ends.
* **Automatic thread meta update**: Every persisted message refreshes the thread’s `updatedAt` and `lastMessage` fields. The thread title is **always** the very first message in the thread and is never overwritten.

# User and Thread Management

<Info>
  User ID and thread ID are automatically sent to the backend for Mastra and
  Custom backends. See Agent Backend Connection docs for more details.
</Info>

**Setting User ID:**
User ID is treated internally as just another Cedar State. If at runtime you want to override the user ID when using `callLLM`, it is also available on the `SendMessageParams` type

<CodeGroup>
  ```typescript Using Store
  import { useCedarStore } from 'cedar-os';

  function LoginHandler({ userId }: { userId: string }) {
  	const store = useCedarStore();

  	// Set user context - Cedar will load this user's threads
  	store.getState().setCedarState('userId', userId);

  	return <YourChatComponent />;
  }
  ```

  ```typescript Initial Configuration
  import { CedarCopilot } from 'cedar-os';

  function App({ userId }: { userId: string }) {
  	return (
  		<CedarCopilot
  			userId={userId} // Set user ID in initial configuration
  			messageStorage={{
  				type: 'local', // or 'none', 'custom'
  			}}>
  			<YourChatComponent />
  		</CedarCopilot>
  	);
  }
  ```
</CodeGroup>

**Setting Thread ID:**
By default, Cedar will use the thread ID stored in the store. If at runtime you want to override the thread ID when using `callLLM`, it is also available on the `SendMessageParams` type

```typescript
import { useCedarStore } from 'cedar-os';

function ConversationSwitcher() {
	const store = useCedarStore();

	const switchToThread = (threadId: string) => {
		// Cedar will save current thread and load the new one
		store.getState().setCurrentThreadId(threadId);
	};

	return (
		<div>
			<button onClick={() => switchToThread('project-discussion')}>
				Project Discussion
			</button>
			<button onClick={() => switchToThread('support-ticket')}>
				Support Ticket
			</button>
		</div>
	);
}
```

## Error Handling

Cedar's storage system is designed to fail gracefully:

* **Storage failures** are silently caught to prevent UI crashes
* **Failed loads** return empty arrays rather than throwing errors
* **Malformed data** is handled with safe fallbacks

## Next Steps

* Learn about [custom message rendering](/getting-started/chat/custom-message-rendering) for rich content
* Explore [streaming responses](/getting-started/chat/streaming) for real-time interactions
* Set up [agent backend connections](/getting-started/agent-backend-connection/agent-backend-connection) for AI responses


# Streaming
Source: https://docs.cedarcopilot.com/chat/streaming

Enable real-time streaming responses in chat components

Cedar enables streaming responses by default in your chat interface. Responses appear in real-time as they're generated. You can disable streaming by setting the `stream` prop to `false` on any chat component or ChatInput.

<Info>
  **Partial Object Streaming Not Supported**: Cedar currently doesn't support partial object streaming. For structured data to be handled via Cedar's automatic handlers, only complete objects should be streamed or streaming should be turned off.
</Info>

## Quick Start

### With Pre-built Components

Streaming is enabled by default for all pre-built chat components:

```tsx
import { FloatingCedarChat } from 'cedar-os-components';

function App() {
	return (
		<FloatingCedarChat
			// Streaming is enabled by default
			side='right'
			title='Streaming Assistant'
		/>
	);
}
```

To disable streaming, set `stream={false}`:

```tsx
<FloatingCedarChat
	stream={false} // Disable streaming
	side='right'
	title='Non-Streaming Assistant'
/>
```

### With Individual Components

The ChatInput component has streaming enabled by default, but you can override it:

```tsx
import { ChatInput, ChatBubbles } from 'cedar-os';

function CustomChat() {
	return (
		<div className='flex flex-col h-full'>
			<div className='flex-1'>
				<ChatBubbles />
			</div>
			<ChatInput
				stream={false} // Override to disable streaming
				position='embedded'
			/>
		</div>
	);
}
```

## How It Works

When streaming is enabled:

1. **Real-time Updates**: Text streams in character by character as it's generated
2. **Out of the box support for streaming flexible objects**: Handle structured objects and custom events (see [Custom Message Rendering](/chat/custom-message-rendering))
3. **Smooth Animation**: Built-in typewriter effect for natural reading experience
4. **Error Handling**: Graceful fallbacks if streaming fails

## Additional Configuration necessary by provider

* **OpenAI and AI SDK**: Streaming is supported out of the box
* **Mastra and Custom backend**: The backend is required to send a data-only SSE stream of information in either streamed text or entire objects

<AccordionGroup>
  <Accordion title="Uhhh... what is a data-only SSE stream?">
    Server-Sent Events (SSE) are part of a one-way streaming protocol to deliver a sequence of `data:` messages over a single HTTP connection. The stream mixes plain text and structured JSON, sent as newline-delimited chunks prefixed with `data:`.

    Under the hood, the server emits:

    **Text chunks for incremental message rendering**

    ```
    → data: Hello, how can I help you?
    ```

    **JSON objects for structured data or tool outputs**

    ```
    → data: {"type":"action","command":"search","query":"nearest cafe"}
    ```

    Our client handlers will parse each `data:` line as it arrives and handle parsed text or JSON accordingly. This enables real-time, mixed-format updates with minimal overhead.
  </Accordion>
</AccordionGroup>

<AccordionGroup>
  <Accordion title="Sample Backend Streaming Handler Implementation">
    Feel free to drop these handlers into your backend to take care of Cedar-OS-compatible streaming.

    **1. Example Usage:**

    ```typescript
    // Return statement of /handleStream
    return createSSEStream(async (controller) => {
    	// Handle application execution logic with controller variable available

    	// Stream text responses from your agent
    	const streamResult = await agent.stream({
    		prompt,
    		temperature,
    		maxTokens,
    		systemPrompt,
    	});

    	// Stream the text result using the helper function
    	await handleTextStream(streamResult, controller);

    	// Example: Stream a tool call result as structured JSON
    	const toolCallResult = {
    		type: 'tool_call',
    		tool: 'plant_seed',
    		result: {
    			status: 'success',
    			message: 'A new Cedar tree has been planted <3',
    		},
    		timestamp: new Date().toISOString(),
    	};

    	// Stream the JSON event using the helper function
    	streamJSONEvent(controller, toolCallResult);
    });

    /**
     * Handles streaming of text chunks using data-only format.
     * Pass your agent's stream result and the controller to stream text chunks to frontend.
     */
    export async function handleTextStream(
    	streamResult: StreamTextResult<any, any>,
    	streamController: ReadableStreamDefaultController<Uint8Array>
    ): Promise<string> {
    	const encoder = new TextEncoder();
    	const chunks: string[] = [];

    	// Stream raw text chunks through data field
    	for await (const chunk of streamResult.textStream) {
    		chunks.push(chunk);
    		// Escape literal newlines for SSE compliance
    		const escaped = chunk.replace(/\n/g, '\\n');
    		streamController.enqueue(encoder.encode(`data:${escaped}\n\n`));
    	}

    	return chunks.join('');
    }

    /**
     * Emit any JSON object as a data event.
     * Use this to stream structured data like tool results, progress updates, or custom events.
     */
    export function streamJSONEvent<T>(
    	controller: ReadableStreamDefaultController<Uint8Array>,
    	eventData: T
    ) {
    	const encoder = new TextEncoder();
    	controller.enqueue(encoder.encode('data: '));
    	controller.enqueue(encoder.encode(`${JSON.stringify(eventData)}\n\n`));
    }
    ```

    **2. Core SSE Stream Creator:**

    ```typescript
    /**
     * Creates a Server-Sent Events stream response.
     * Pass a callback function that receives a controller for streaming data.
     * Use this as your main endpoint return value for streaming responses.
     */
    export function createSSEStream(
    	cb: (controller: ReadableStreamDefaultController<Uint8Array>) => Promise<void>
    ): Response {
    	const encoder = new TextEncoder();

    	const stream = new ReadableStream<Uint8Array>({
    		async start(controller) {
    			try {
    				// Execute your application logic with streaming controller
    				await cb(controller);

    				// Signal completion with empty data
    				controller.enqueue(encoder.encode('event: done\n'));
    				controller.enqueue(encoder.encode('data:\n\n'));
    			} catch (err) {
    				console.error('Error during SSE stream', err);

    				const message = err instanceof Error ? err.message : 'Internal error';
    				controller.enqueue(encoder.encode('data: '));
    				controller.enqueue(
    					encoder.encode(`${JSON.stringify({ type: 'error', message })}\n\n`)
    				);
    			} finally {
    				controller.close();
    			}
    		},
    	});

    	return new Response(stream, {
    		headers: {
    			'Content-Type': 'text/event-stream',
    			'Cache-Control': 'no-cache',
    			Connection: 'keep-alive',
    		},
    	});
    }
    ```
  </Accordion>
</AccordionGroup>

## Next Steps

* Set up our [cedar-mastra-starter](https://github.com/CedarCopilot/cedar-mastra-starter) for an example of streaming from a separate Mastra server.
* Sending back a custom object type? Render it using [custom message rendering](/chat/custom-message-rendering)
* Learn about [configuring the Cedar editor](/chat/configuring-cedar-editor)


# null
Source: https://docs.cedarcopilot.com/components/markdown-renderer



# MarkdownRenderer Component

The `MarkdownRenderer` component provides consistent markdown rendering across Cedar OS with advanced features like code syntax highlighting, copy functionality, and prefix processing for chat interfaces.

## Features

* **GitHub Flavored Markdown**: Full GFM support including tables, strikethrough, and more
* **Code Highlighting**: Syntax highlighting with copy-to-clipboard functionality
* **Prefix Processing**: Special handling for prefixed content (useful in chat)
* **Cedar OS Integration**: Automatic theming and styling consistency
* **Inline/Block Modes**: Flexible rendering for different contexts
* **Interactive Elements**: Copy buttons, hover effects, and edit placeholders

## Import

```tsx
import { MarkdownRenderer } from 'cedar-os-components/chatMessages/MarkdownRenderer';
```

## Basic Usage

```tsx
<MarkdownRenderer content='# Hello World\n\nThis is **bold** and *italic* text.' />
```

## Props

| Prop            | Type      | Default | Description                                           |
| --------------- | --------- | ------- | ----------------------------------------------------- |
| `content`       | `string`  | -       | The markdown content to render (required)             |
| `processPrefix` | `boolean` | `false` | Whether to process prefix markers for special styling |
| `className`     | `string`  | `''`    | Additional CSS classes                                |
| `inline`        | `boolean` | `false` | Whether to render in inline mode                      |

## Supported Markdown Elements

### Headers

```markdown
# H1 Header

## H2 Header

### H3 Header

#### H4 Header

##### H5 Header

###### H6 Header
```

### Text Formatting

```markdown
**Bold text**
_Italic text_
~~Strikethrough~~
`Inline code`
```

### Links

```markdown
[Link text](https://example.com)
```

* Opens in new tab automatically
* `noopener noreferrer` for security

### Code Blocks

````markdown
```javascript
function hello() {
	console.log('Hello World!');
}
```
````

Features:

* Language detection and display
* Copy to clipboard button
* Edit placeholder button
* Syntax highlighting
* Horizontal scrolling for long lines

### Lists

```markdown
- Unordered list item 1
- Unordered list item 2

1. Ordered list item 1
2. Ordered list item 2
```

### Blockquotes

```markdown
> This is a blockquote
> It can span multiple lines
```

### Tables

```markdown
| Column 1 | Column 2 | Column 3 |
| -------- | -------- | -------- |
| Row 1    | Data     | More     |
| Row 2    | Data     | More     |
```

## Inline Mode

Perfect for rendering markdown within text flows:

```tsx
<MarkdownRenderer content='This is **bold** text inline' inline={true} />
```

In inline mode:

* Paragraphs render as `<span>` instead of `<p>`
* Maintains text flow
* Preserves inline formatting

## Prefix Processing

For chat interfaces where you need special styling for prefixes:

```tsx
<MarkdownRenderer
	content="@@PREFIX@@Assistant: @@ENDPREFIX@@Here's my response with **markdown**!"
	processPrefix={true}
/>
```

The prefix markers (`@@PREFIX@@` and `@@ENDPREFIX@@`) are automatically:

* Styled with the accent color
* Processed recursively in nested elements
* Removed from final output

## Code Block Features

### Copy Functionality

* Hover over code blocks to see copy button
* Visual feedback when copied
* Automatic timeout after 2 seconds

### Language Support

```tsx
<MarkdownRenderer
	content={`
\`\`\`typescript
interface User {
  name: string;
  email: string;
}
\`\`\`
`}
/>
```

### Inline Code

```tsx
<MarkdownRenderer content='Use the `useState` hook for state management.' />
```

## Styling Integration

The component automatically integrates with Cedar OS theming:

```tsx
// Uses current styling context
const { styling } = useStyling();

// Applied to:
// - Accent colors for borders and highlights
// - Text colors for consistency
// - Background colors for code blocks
// - Table styling and borders
```

## Advanced Examples

### Chat Message Rendering

````tsx
<MarkdownRenderer
	content="@@PREFIX@@System: @@ENDPREFIX@@**Task completed!** Here's your code:\n\n```javascript\nconsole.log('Done!');\n```"
	processPrefix={true}
	className='chat-message'
/>
````

### Documentation Content

```tsx
<MarkdownRenderer
	content={`
# API Documentation

## Overview
This API provides **powerful** functionality.

### Example Request
\`\`\`bash
curl -X POST https://api.example.com/data
\`\`\`

| Parameter | Type | Description |
|-----------|------|-------------|
| \`name\` | string | User name |
| \`email\` | string | User email |
  `}
/>
```

### Inline Rich Text

```tsx
<p>
	Welcome to our platform!
	<MarkdownRenderer
		content='Read our **documentation** and try the `API`'
		inline={true}
	/>
	to get started.
</p>
```

## Accessibility

* Proper semantic HTML structure
* Keyboard navigation support
* Screen reader friendly
* High contrast code blocks
* Focus indicators on interactive elements

## Performance

* Efficient re-rendering with React.memo patterns
* Lazy loading for complex content
* Optimized regex processing
* Minimal DOM updates

## Browser Support

* Modern browsers with ES6+ support
* Clipboard API for copy functionality
* CSS Grid/Flexbox for layout
* Graceful degradation for older browsers

## Integration Notes

* Works seamlessly with TypewriterText component
* Consistent with other Cedar OS components
* Supports dark/light theme switching
* Responsive design for all screen sizes


# null
Source: https://docs.cedarcopilot.com/components/typewriter-text



# TypewriterText Component

The `TypewriterText` component provides an animated typing effect with adaptive speed based on text length. It supports markdown rendering, customizable delays, and optional prefix styling.

## Features

* **Adaptive Speed**: Automatically adjusts typing speed based on text length
* **Markdown Support**: Built-in markdown rendering with prefix support
* **Customizable Cursor**: Optional blinking cursor with styling
* **Prefix Support**: Special styling for prefixed content
* **Performance Optimized**: Uses Framer Motion for smooth animations

## Import

```tsx
import { TypewriterText } from 'cedar-os-components/text/TypewriterText';
```

## Basic Usage

```tsx
<TypewriterText text='Hello, this text will type out automatically!' />
```

## Props

| Prop               | Type         | Default | Description                                                         |
| ------------------ | ------------ | ------- | ------------------------------------------------------------------- |
| `text`             | `string`     | -       | The text to be typed out (required)                                 |
| `className`        | `string`     | `''`    | Additional CSS classes                                              |
| `charDelay`        | `number`     | `0.025` | Base character delay in seconds. If default, adaptive speed is used |
| `minCharDelay`     | `number`     | `0.002` | Minimum delay for very long texts (fastest speed)                   |
| `maxCharDelay`     | `number`     | `0.04`  | Maximum delay for short texts (slowest speed)                       |
| `showCursor`       | `boolean`    | `true`  | Whether to show the typing cursor                                   |
| `onTypingStart`    | `() => void` | -       | Callback when typing animation starts                               |
| `onTypingComplete` | `() => void` | -       | Callback when typing animation completes                            |
| `blinking`         | `boolean`    | `false` | Whether the cursor should blink                                     |
| `renderAsMarkdown` | `boolean`    | `true`  | Whether to render content as markdown                               |
| `prefix`           | `string`     | `''`    | Optional prefix text with special styling                           |

## Adaptive Speed

The component automatically adjusts typing speed based on text length:

* **Short texts** (≤30 words): Use `maxCharDelay` (slower)
* **Long texts** (≥400 characters): Use `minCharDelay` (faster)
* **Medium texts**: Exponential interpolation between min and max

```tsx
// Short text - types slowly
<TypewriterText text="Hi!" />

// Long text - types faster automatically
<TypewriterText text="This is a very long piece of text that will type out much faster than shorter text because the component automatically adjusts the speed based on content length..." />
```

## Custom Speed

Override adaptive speed by setting a custom `charDelay`:

```tsx
<TypewriterText
	text='Custom speed text'
	charDelay={0.05} // Fixed delay of 50ms per character
/>
```

## Prefix Styling

Use prefixes for special highlighting (useful for chat interfaces):

```tsx
<TypewriterText
	text='This is the main message content'
	prefix='Assistant: '
	renderAsMarkdown={true}
/>
```

## Callbacks

```tsx
<TypewriterText
	text='Watch the console!'
	onTypingStart={() => console.log('Started typing')}
	onTypingComplete={() => console.log('Finished typing')}
/>
```

## Markdown Support

The component supports full markdown rendering when `renderAsMarkdown` is true:

```tsx
<TypewriterText
	text='**Bold text**, *italic text*, and `code blocks` all work!'
	renderAsMarkdown={true}
/>
```

## Custom Cursor

```tsx
<TypewriterText
	text='Blinking cursor example'
	showCursor={true}
	blinking={true}
/>
```

## Advanced Example

```tsx
<TypewriterText
	text='# Welcome to Cedar OS\n\nThis is a **powerful** component with `adaptive speed`!'
	prefix='System: '
	className='text-lg font-medium'
	minCharDelay={0.001}
	maxCharDelay={0.06}
	blinking={true}
	onTypingComplete={() => setShowNextSection(true)}
/>
```

## Integration with Cedar OS

The component automatically integrates with Cedar OS styling system:

* Uses `useStyling()` hook for consistent theming
* Cursor color matches the current color scheme
* Prefix styling uses accent colors
* Markdown rendering follows Cedar OS design patterns

## Performance Notes

* Uses Framer Motion's `animate` function for optimal performance
* Automatically cleans up animations on unmount
* Efficient re-rendering with dependency tracking
* Smooth 60fps animations with hardware acceleration


# Customising Cedar
Source: https://docs.cedarcopilot.com/customising/customising-cedar

How to customize Cedar-OS for your needs

This page will cover how to customize Cedar-OS for your specific needs.

## Coming Soon

Content for this section will be added in the next phase.


# Styling
Source: https://docs.cedarcopilot.com/customising/styling

Customize the appearance of Cedar-OS components

Cedar-OS provides comprehensive styling options through the `stylingSlice` and integrates seamlessly with Tailwind CSS. This guide covers how to customize the appearance of Cedar components and ensure proper Tailwind configuration.

## Tailwind CSS Setup

Cedar-OS components use Tailwind CSS for styling. To ensure all Cedar styles work correctly, you need to configure Tailwind to scan Cedar's component files.

### Tailwind 4.0+ Configuration

If you're using Tailwind CSS 4.0 or later, add the following `@source` directive to your main CSS file:

```css
/* app/globals.css or your main CSS file */
@import 'tailwindcss';

/* Add Cedar-OS components to Tailwind's content sources */
@source "../node_modules/cedar-os/**/*.{js,ts,jsx,tsx}";
```

### Tailwind 3.x Configuration

For Tailwind CSS 3.x, add Cedar's paths to your `tailwind.config.js`:

```javascript
// tailwind.config.js
module.exports = {
	content: [
		// Your app's content paths
		'./src/pages/**/*.{js,ts,jsx,tsx,mdx}',
		'./src/components/**/*.{js,ts,jsx,tsx,mdx}',
		'./src/app/**/*.{js,ts,jsx,tsx,mdx}',
		'./src/lib/**/*.{js,ts,jsx,tsx,mdx}',
		'./src/**/*.{js,ts,jsx,tsx}',

		// Add Cedar-OS component paths
		'./node_modules/cedar-os/**/*.{js,ts,jsx,tsx}',
	],
	// ... rest of your config
};
```

## Using the Styling Slice

Cedar-OS includes a `stylingSlice` that manages global styling configuration for all Cedar components. This allows you to customize colors, dark mode, and other visual properties across your entire application.

### Accessing Styling State

```tsx
import { useCedarStore } from 'cedar-os';

function MyComponent() {
	const { styling, setStyling, toggleDarkMode } = useCedarStore();

	return (
		<div>
			<p>Current theme: {styling.darkMode ? 'Dark' : 'Light'}</p>
			<p>Primary color: {styling.color}</p>
			<button onClick={toggleDarkMode}>Toggle Dark Mode</button>
		</div>
	);
}
```

### Styling Configuration

The styling slice provides the following configuration options:

```typescript
interface StylingConfig {
	darkMode: boolean; // Enable/disable dark mode
	color: string; // Primary color (hex format)
	secondaryColor: string; // Secondary color (hex format)
	accentColor: string; // Accent color (hex format)
}
```

### Setting Custom Colors

You can update the color scheme programmatically:

```tsx
import { useCedarStore } from 'cedar-os';

function ThemeCustomizer() {
	const { setStyling } = useCedarStore();

	const applyBrandColors = () => {
		setStyling({
			color: '#3B82F6', // Blue primary
			secondaryColor: '#1E40AF', // Darker blue
			accentColor: '#F59E0B', // Amber accent
		});
	};

	const applyDarkTheme = () => {
		setStyling({
			darkMode: true,
			color: '#6366F1', // Indigo for dark mode
			secondaryColor: '#4F46E5',
			accentColor: '#EC4899', // Pink accent
		});
	};

	return (
		<div>
			<button onClick={applyBrandColors}>Apply Brand Colors</button>
			<button onClick={applyDarkTheme}>Apply Dark Theme</button>
		</div>
	);
}
```

## Component-Level Styling

### Using Styled Components

Cedar components automatically respond to the global styling configuration. For example, the chat components will use your configured colors:

```tsx
import { FloatingCedarChat } from '@/chatComponents/FloatingCedarChat';
import { useCedarStore } from 'cedar-os';

function StyledChat() {
	const { setStyling } = useCedarStore();

	// Set custom colors before rendering
	useEffect(() => {
		setStyling({
			color: '#8B5CF6', // Purple
			secondaryColor: '#7C3AED',
			accentColor: '#F97316', // Orange
		});
	}, []);

	return <FloatingCedarChat side='right' title='Support Chat' />;
}
```

### CSS Variables

Cedar-OS components use CSS variables that integrate with Tailwind's design system. These variables are automatically updated when you change the styling configuration:

```css
/* These variables are set by Cedar-OS */
:root {
	--cedar-primary: /* Your primary color */ ;
	--cedar-secondary: /* Your secondary color */ ;
	--cedar-accent: /* Your accent color */ ;
	--cedar-background: /* Background color based on theme */ ;
	--cedar-foreground: /* Text color based on theme */ ;
}
```

## Utility Functions

Cedar-OS provides several utility functions for working with colors:

### Color Manipulation

```tsx
import {
	desaturateColor,
	getShadedColor,
	getLightenedColor,
	getTextColorForBackground,
} from 'cedar-os';

// Desaturate a color (reduce vibrancy)
const mutedBlue = desaturateColor('#3B82F6');

// Create a darker shade
const darkBlue = getShadedColor('#3B82F6', 0.2); // 20% darker

// Create a lighter tint
const lightBlue = getLightenedColor('#3B82F6', 0.3); // 30% lighter

// Get appropriate text color for a background
const textColor = getTextColorForBackground('#3B82F6'); // Returns white or black
```

### Class Name Utilities

Cedar uses the `cn` utility (similar to `clsx`) for combining class names:

```tsx
import { cn } from 'cedar-os';

function MyComponent({ className, isActive }) {
	return (
		<div
			className={cn(
				'base-styles px-4 py-2',
				isActive && 'bg-blue-500 text-white',
				className
			)}>
			Content
		</div>
	);
}
```

## Dark Mode Support

Cedar-OS components automatically support dark mode through Tailwind's dark mode classes. When you toggle dark mode using `toggleDarkMode()`, all Cedar components will update their appearance.

### Implementing Dark Mode Toggle

```tsx
import { useCedarStore } from 'cedar-os';
import { Moon, Sun } from 'lucide-react';

function DarkModeToggle() {
	const { styling, toggleDarkMode } = useCedarStore();

	useEffect(() => {
		// Apply dark mode class to document
		if (styling.darkMode) {
			document.documentElement.classList.add('dark');
		} else {
			document.documentElement.classList.remove('dark');
		}
	}, [styling.darkMode]);

	return (
		<button
			onClick={toggleDarkMode}
			className='p-2 rounded-lg bg-gray-200 dark:bg-gray-800'>
			{styling.darkMode ? <Sun /> : <Moon />}
		</button>
	);
}
```

## Best Practices

1. **Configure Tailwind First**: Always ensure your Tailwind configuration includes Cedar's component paths before using Cedar components.

2. **Set Colors Early**: Configure your color scheme early in your app's lifecycle, preferably in your root component:

   ```tsx
   function App() {
   	const { setStyling } = useCedarStore();

   	useEffect(() => {
   		setStyling({
   			color: '#YourBrandColor',
   			secondaryColor: '#YourSecondaryColor',
   			accentColor: '#YourAccentColor',
   		});
   	}, []);

   	return <YourApp />;
   }
   ```

3. **Consistent Theme**: Use the styling slice to maintain consistent theming across all Cedar components rather than overriding styles individually.

4. **Responsive Dark Mode**: Always test your color choices in both light and dark modes to ensure good contrast and readability.

## Troubleshooting

### Styles Not Applying

If Cedar component styles aren't working:

1. **Check Tailwind Config**: Ensure Cedar paths are included in your Tailwind configuration
2. **Clear Cache**: Try clearing your build cache and rebuilding
3. **Check CSS Import**: Make sure you're importing your global CSS file that includes Tailwind directives
4. **Verify Installation**: Ensure both `cedar-os` and components are properly installed

### Dark Mode Issues

If dark mode isn't working:

1. **Check HTML Class**: Ensure the `dark` class is being applied to your `<html>` element
2. **Tailwind Dark Mode**: Verify your Tailwind config uses `class` strategy for dark mode:
   ```javascript
   module.exports = {
   	darkMode: 'class',
   	// ... rest of config
   };
   ```

## Next Steps

* Explore [component customization](/customising/customising-cedar) for more advanced styling options
* Learn about [custom message rendering](/chat/custom-message-rendering) to style chat messages
* Check out the [Cedar Playground](https://cedarcopilot.com/examples/cedar-playground) to see styling in action


# Application Based Triggers
Source: https://docs.cedarcopilot.com/external-integrations/application-triggers

Application-based trigger integrations

This page will cover application-based trigger integrations.

## Coming Soon

Content for this section will be added in the next phase.


# Email
Source: https://docs.cedarcopilot.com/external-integrations/email

Email-based external integrations

This page will cover email-based external integrations.

## Coming Soon

Content for this section will be added in the next phase.


# External Integrations
Source: https://docs.cedarcopilot.com/external-integrations/external-integrations

Overview of external integrations

This page will cover external integrations with Cedar-OS.

## Coming Soon

Content for this section will be added in the next phase.


# Text
Source: https://docs.cedarcopilot.com/external-integrations/text

Text-based external integrations

This page will cover text-based external integrations.

## Coming Soon

Content for this section will be added in the next phase.


# Getting Started Overview
Source: https://docs.cedarcopilot.com/getting-started/getting-started

Complete guide to getting started with Cedar-OS

Cedar-OS is an open-source framework for building AI-native applications. This guide will walk you through setting up your first Cedar-OS application with chat functionality, AI backend integration, and state management.

# Installing Cedar-OS

## With Cedar-CLI

<Steps>
  <Step title="Check Prerequisites">
    Cedar-OS requires:

    * React 18.2.0 or higher
    * React DOM 18.2.0 or higher
    * Node.js 18+ (for development)

    **Framework Support:**

    * ✅ **Next.js (Recommended)** - Full support with optimal performance
    * ✅ **Create React App** - Supported with additional configuration
    * ✅ **Vite + React** - Supported with additional configuration
    * ✅ **Other React frameworks** - May require manual setup
  </Step>

  <Step title="Install using the cedar-os-cli">
    Our CLI is the fastest way to get started with Cedar-OS. For most users, run this command - it automatically detects your setup and does the right thing:

    ```bash
    npx cedar-os-cli plant-seed
    ```

    **What `plant-seed` does:**

    * **New project**: Offers template selection → Creates project → Adds Cedar components

      <Info>
        **Recommended:** We highly recommend starting with the Mastra template. It requires the least configuration and is the most powerful way to use Cedar-OS.
      </Info>

    * **Existing Next.js project**: Automatically adds Cedar components and required dependencies to your project

    * **Existing React project**: Adds Cedar components with a note about Next.js being optimal

    **About Cedar components:** Cedar-OS uses a [shadcn](https://ui.shadcn.com/)-style approach - components are copied directly into your project (not installed as dependencies). This gives you full control to customize, modify, and style the components as needed for your application.

    **When to use `add-sapling` instead:**

    Use this command to install cedar-os and download Cedar components in an existing project if `plant-seed` fails.

    ```bash
    # Add Cedar components and install dependencies only
    npx cedar-os-cli add-sapling
    ```

    **Using CLI flags**

    You can skip prompts using the `--yes` flag:

    ```bash
    npx cedar-os-cli plant-seed --project-name my-cedar-tree --yes
    ```
  </Step>

  <Step title="Add your API key">
    Create a `.env.local` file in your project root and add your OpenAI API key:

    ```bash
    # For client-side access (Cedar-OS components in browser)
    NEXT_PUBLIC_OPENAI_API_KEY=your-openai-api-key

    # For server-side access (API routes, server components)
    OPENAI_API_KEY=your-openai-api-key
    ```

    **Choose based on your setup:**

    * Use `NEXT_PUBLIC_OPENAI_API_KEY` if Cedar-OS will make API calls directly from the browser
    * Use `OPENAI_API_KEY` if you're routing calls through your backend/API routes
    * You can include both if you need access from both client and server

    This will be based on your agent configuration, see [Agent Backend Connection](/agent-backend-connection/agent-backend-connection) for detailed configuration options.
  </Step>

  <Step title="Initialize CedarCopilot">
    Wrap your app with the `CedarCopilot` component and configure your AI provider:

    <Warning>
      **Important:** CedarCopilot must be used in a client component. Add `"use client"` at the top of any file that uses CedarCopilot.
    </Warning>

    <CodeGroup>
      ```tsx Mastra
      "use client";

      import { CedarCopilot } from 'cedar-os';

      function App() {
      	return (
      		<CedarCopilot
      			llmProvider={{
      				provider: 'mastra',
      				baseURL: 'http://localhost:4111', // Your Mastra backend URL
      				apiKey: process.env.MASTRA_API_KEY, // Optional: only if your backend requires auth
      			}}>
      			<YourApp />
      		</CedarCopilot>
      	);
      }
      ```

      ```tsx AI SDK
      "use client";

      import { CedarCopilot } from 'cedar-os';

      function App() {
      	return (
      		// You don't need to put every model,
      		// but if you try to use a model without a key it will fail
      		<CedarCopilot
      			llmProvider={{
      				provider: 'ai-sdk',
      				providers: {
      					openai: {
      						apiKey: process.env.NEXT_PUBLIC_OPENAI_API_KEY,
      					},
      					anthropic: {
      						apiKey: process.env.NEXT_PUBLIC_ANTHROPIC_API_KEY,
      					},
      					google: {
      						apiKey: process.env.NEXT_PUBLIC_GOOGLE_API_KEY,
      					},
      				},
      			}}>
      			<YourApp />
      		</CedarCopilot>
      	);
      }
      ```

      ```tsx OpenAI
      "use client";

      import { CedarCopilot } from 'cedar-os';

      function App() {
      	return (
      		<CedarCopilot
      			llmProvider={{
      				provider: 'openai',
      				apiKey: process.env.NEXT_PUBLIC_OPENAI_API_KEY,
      			}}>
      			<YourApp />
      		</CedarCopilot>
      	);
      }
      ```

      ```tsx Anthropic
      "use client";

      import { CedarCopilot } from 'cedar-os';

      function App() {
      	return (
      		<CedarCopilot
      			llmProvider={{
      				provider: 'anthropic',
      				apiKey: process.env.NEXT_PUBLIC_ANTHROPIC_API_KEY,
      			}}>
      			<YourApp />
      		</CedarCopilot>
      	);
      }
      ```

      ```tsx Custom Backend
      "use client";

      import { CedarCopilot } from 'cedar-os';

      function App() {
      	return (
      		<CedarCopilot
      			llmProvider={{
      				provider: 'custom',
      				config: {
      					baseURL: 'https://your-api.com',
      					apiKey: 'your-api-key',
      					// Any additional config your backend needs
      					organizationId: 'org-123',
      					projectId: 'project-456',
      				},
      			}}>
      			<YourApp />
      		</CedarCopilot>
      	);
      }
      ```
    </CodeGroup>

    See [Agent Backend Connection](/agent-backend-connection/agent-backend-connection) for more details. Now you're ready to add a chat!
  </Step>

  <Step title="Using Cedar">
    Now that you have Cedar-OS installed and configured, here are your next steps to build your AI-native application:

    **Choose your next feature:**

    * **[Configuring a Chat](/chat/chat-overview)** - Set up chat interfaces with `ChatInput` and `ChatBubbles` components for seamless AI conversations
    * **[Adding State Access](/state-access/agentic-state-access)** - Enable AI agents to read and modify your application state using `useCedarState`
    * **[Using Spells](/spells/spells)** - Create interactive radial menus and quick actions for enhanced user experience

    **Quick start examples:**

    <CodeGroup>
      ```tsx Chat Setup
      import { FloatingCedarChat } from 'cedar-os';

      function ChatApp() {
        return (
          <div className="h-screen">
            {/* This automatically works */}
            <FloatingCedarChat />
          </div>
        );
      }
      ```

      ```tsx State Access
      import { useRegisterState } from 'cedar-os';
      import { useState } from 'react';

      function TodoApp() {
        const [todos, setTodos] = useState([]);

        useRegisterState({
            key: 'todos',
            value: todos,
            setValue: setTodos,
            description: 'User todo list manageable by AI',
            customSetters: {
              addTodo: {
                name: 'addTodo',
                description: 'Add a new todo item',
                parameters: [
                  { name: 'text', type: 'string', description: 'Todo text' }
                ],
                execute: (currentTodos, text) => {
                  setTodos([...currentTodos, {
                    id: Date.now(),
                    text,
                    completed: false
                  }]);
                }
              },
              toggleTodo: {
                name: 'toggleTodo',
                description: 'Toggle todo completion status',
                parameters: [
                  { name: 'id', type: 'number', description: 'Todo ID' }
                ],
                execute: (currentTodos, id) => {
                  setTodos(currentTodos.map(t =>
                    t.id === id ? { ...t, completed: !t.completed } : t
                  ));
                }
              }
          }
        });

        return (
          <div>
            {todos.map(todo => (
              <div key={todo.id}>
                <input
                  type="checkbox"
                  checked={todo.completed}
                  onChange={() => setTodos(todos.map(t =>
                    t.id === todo.id ? { ...t, completed: !t.completed } : t
                  ))}
                />
                {todo.text}
              </div>
            ))}
          </div>
        );
      }
      ```

      ```tsx Spells Integration
      import { useSpells } from 'cedar-os';
      import { useEffect } from 'react';

      function SpellsApp() {
        const { registerSpell } = useSpells();

        useEffect(() => {
          registerSpell({
            id: 'quick-action',
            name: 'Quick Action',
            icon: '⚡',
            action: () => console.log('Spell activated!'),
            trigger: 'cmd+k'
          });
        }, []);

        return (
          <div>
            <p>Press Cmd+K to activate spells</p>
          </div>
        );
      }
      ```
    </CodeGroup>
  </Step>
</Steps>

## Next Steps

Now that you have Cedar-OS set up with basic functionality:

1. **Explore Advanced Features**: Check out [Voice Integration](/voice/voice-integration), [Custom Message Types](/chat/custom-message-rendering), and [Advanced State Management](/state-access/agentic-state-access).

2. **Try Different Providers**: Experiment with [Mastra](/agent-backend-connection/mastra) for full-featured agents or [Direct Provider Integration](/agent-backend-connection/custom).

3. **Customize the UI**: Learn about [Styling and Theming](/customising/styling) to match your brand.

4. **Build Complex Workflows**: Explore our [Examples](/examples) to see Cedar-OS in action with real applications.

## If Things Are Going Wrong

### Manual Installation (CLI Fallback)

If all CLI commands are failing, you can manually install Cedar-OS:

<Steps>
  <Step title="Install the cedar-os package">
    <CodeGroup>
      ```bash npm
      npm install cedar-os
      ```

      ```bash yarn
      yarn add cedar-os
      ```

      ```bash pnpm
      pnpm add cedar-os
      ```

      ```bash bun
      bun add cedar-os
      ```
    </CodeGroup>
  </Step>

  <Step title="Copy component source code">
    You will need to manually copy the component source code locally. The component files can be found here: [Cedar-OS Components](https://github.com/CedarCopilot/cedar-OS/tree/main/packages/cedar-os-components)

    Copy the components you need into your project's component directory (typically `src/components/cedar-os/`).
  </Step>

  <Step title="Install dependencies">
    Install all the required dependencies for Cedar-OS components:

    <CodeGroup>
      ```bash npm
      npm install lucide-react motion motion-plus-react uuid react-markdown framer-motion @radix-ui/react-slot class-variance-authority
      ```

      ```bash yarn
      yarn add lucide-react motion motion-plus-react uuid react-markdown framer-motion @radix-ui/react-slot class-variance-authority
      ```

      ```bash pnpm
      pnpm add lucide-react motion motion-plus-react uuid react-markdown framer-motion @radix-ui/react-slot class-variance-authority
      ```

      ```bash bun
      bun add lucide-react motion motion-plus-react uuid react-markdown framer-motion @radix-ui/react-slot class-variance-authority
      ```
    </CodeGroup>
  </Step>
</Steps>

### Troubleshooting Common Issues

**Components not rendering**: Make sure you've wrapped your app with `CedarCopilot`.

**Components missing ALL styling**: If Cedar-OS components are missing all styling (we promise they are beautiful!), make sure you have [Tailwind CSS configured](https://tailwindcss.com/docs/installation) in your project.

**Don't see the chat?**: Make sure you add the correct chat component inside the `<CedarCopilot>` boundary. See [Chat Overview](/chat/chat-overview) for setup instructions.

**AI calls failing**: Check that your API keys are correctly set in environment variables and accessible in your client code.

**Using with non-Next.js frameworks**: While Cedar-OS works with Create React App, Vite, and other React frameworks, you may need to manually configure Tailwind CSS and ensure proper client-side rendering for optimal performance.

**TypeScript errors**: Cedar-OS is fully typed - check that you're importing types correctly and using the right component props.

Need help? Check our [Community Discord](https://discord.gg/4AWawRjNdZ) or [GitHub Issues](https://github.com/cedar-os/cedar/issues). We're always here to help :-D


# Cedar-OS Demos
Source: https://docs.cedarcopilot.com/introduction/ai-native-experiences

Understanding AI-native software and seeing Cedar-OS in action

# What are AI-native experiences?

Traditionally, applications combine *functionality* (e.g. the ability to send an email), with *user interfaces* (the windows, text areas and buttons) that allow users to control and access the functionality.

Now AI can do stuff. But it can really communicate only in one way.

> If I want to tell you there is a spot on your shirt, I'm not going to do it linguistically: "There's a spot on your shirt 14 centimeters down from the collar and three centimeters to the left of your button." If you have a spot—"There!" \[He points]—I'll point to it. Pointing is a metaphor we all know. We've done a lot of studies and tests on that, and it's much faster to do all kinds of functions, such as cutting and pasting, with a mouse, so it's not only easier to use but more efficient.

We help the AI understand the pointing and selecting you do.

> Graphic design, which has evolved over centuries, influences how we see, interpret, and use information. Typeface choices, layout, colors—all these details matter. They aren't just aesthetics; they shape understanding and productivity.

> *And it takes a long time for a medium to really fulfill its potential. Television itself wasn't just another form of radio. For years, it was just radio with pictures, but it took decades until television came into its own as a unique medium with storytelling, entertainment, and information that couldn't just be done on radio.*
>
> *The same is true of computers. We're still in the early days. We're just figuring out what a computer medium really means to us socially, creatively, and educationally. But I believe we're at the start of a revolution, the computer generation replacing the television generation. And in time, it will change everything about how people communicate, learn, and work.*

AI applications require new interfaces. Cedar-OS is the building blocks for those interfaces.

* For example, we can allow your user to control your app by talking to it.
* We allow the agent to know what's going on the application, and control it to do the work.

Applications traditionally allows the user to do work in pre-defined ways.

We are firm and deep believers in craft, and making software *human*.

## Demos

Coming soon - interactive demos showcasing Cedar-OS capabilities.


# Architecture
Source: https://docs.cedarcopilot.com/introduction/architecture

Understanding how Cedar-OS enables AI-native experiences

## Our Core Architecture

Cedar-OS bridges the gap between AI agents and React applications, providing a comprehensive framework for AI agents to read, write, and interact with your application state, just like users can.

<img className="block dark:hidden" src="https://mintlify.s3.us-west-1.amazonaws.com/cedar/images/architecture.png" alt="Cedar-OS Architecture Diagram" />

<img className="hidden dark:block" src="https://mintlify.s3.us-west-1.amazonaws.com/cedar/images/architecture.png" alt="Cedar-OS Architecture Diagram" />

## Zustand State Management

Let's say you want an AI to be able to read and write different parts of your react app. For example, you're making an AI-native email, and you want it to be able to help with drafts and categorise emails. You have an AI chat on the side that the user can ask it to do things. You'd find some immediate problems with this:

1. **Component-scoped State**: React typically has state inside of the component that uses it. This totally makes sense, and makes for a really great developer experience where you can combine logic and rendering together!

   But this also means that floating up and collecting different states across the application to the AI involves a lot of scoping problems, prop drilling, and messy re-render optimisation.

2. **Lack of Predefined Interactions**: The AI needs much more predefined ways of working with state! How does it interpret what this state means? How does it know how it can and should change it?

3. **State Lifecycle Issues**: State disappears when the component is unmounted, or might have never loaded in! AI should get to make changes, then navigate the user to see those changes.

Cedar fixes this by:

1. **Register States**: Each "RegisterCedarState" acts like a portal the agent can reach the specific state. You don't need to worry about contexts, their scopes, and most importantly the fact that if one context field changes, everything that subscribes to it changes.
2. **States for agents**: Our types support agent understanding and allow you to create custom setters that allow the agent to interact in predefined, safe ways.
3. **Persistent State Registration**: State can be registered so the agent can see the contents even if the user doesn't have the component rendered in that moment, and backend loading behaviour can be configured.

### Cedar's Zustand Implementation

* **Centralized Data Store**: It stores all the data the agent needs in one centralised place
* **State Persistence**: It persists state across component lifecycles
* **Override Capability**: It allows you to override any internal function simply by registering a slice with the same field key
* **Optimized Hooks**: We provide optimised hooks (and every internal zustand slice)
* **Pre-built Components**: We create components that use these hooks and internal state
* **Complete Solutions**: We put together these components into completed functionality, so you can get a working chat in one line

### Hooks Layer - The Foundation

At the base, we provide optimized hooks that connect directly to our Zustand store. Examples:

* **`useCedarState`**: Access and modify registered state with type safety
* **`useTypedAgentConnection`**: Manage AI agent connections with full typing
* **`useCedarEditor`**: Rich text editing with AI-aware features
* **`useMessageRenderer`**: Handle streaming messages and custom components

These hooks abstract away the complexity of state management, context, and real-time updates while maintaining full type safety and performance optimization. You can of course create your own hooks that plug into the Zustand store.

### Components Layer - Shadcn-Style Customization

Our component layer adopts the shadcn/ui philosophy of **copy-and-own** rather than traditional npm packages.

**Why Copy-and-Own Works Better for AI Components:**

* **Full Customization**: You own the component code, so you can modify styling, behavior, and logic without fighting against package constraints
* **No Version Lock-in**: Updates are opt-in. You choose when and what to update, preventing breaking changes from disrupting your AI experiences
* **Transparent Implementation**: You can see exactly how components work with AI state, making debugging and customization straightforward

**Component Examples:**

* `ChatInput` - AI-aware input with mentions and context
* `StreamingText` - Real-time text rendering with typewriter effects
* `MessageRenderer` - Renders different messages with unique styling or interactivity.
* `VoiceIndicator` - Visual feedback for voice interactions

Each component is designed to be sophisticated out-of-the-box while remaining completely customizable to your design system and requirements.

### Complete Solutions - One-Line AI Experiences

At the top layer, we provide complete, production-ready AI experiences. We can provide sophisticated solutions like this because you own the code, and so you can customise it however you want. Example:

* **`<FloatingCedarChat />`**: Floating chat interface that can be summoned anywhere. This combines `<ChatInput />`, `<ContextBadge />`, `<ChatMessages />`, and positional Containers like `<FloatingContainer />` to create a floating chat.

**The Power of This Architecture:**

1. **Start Simple**: Drop in a complete solution like `CedarCopilot` and `FloatingCedarChat` and have AI working in minutes
2. **Customize Gradually**: Customise individual components as your needs evolve
3. **Go Deep**: Use hooks directly for completely custom AI experiences
4. **Never Hit Walls**: Since you own all the code, there are no limitations on what you can build.


# Cedar-OS
Source: https://docs.cedarcopilot.com/introduction/overview

An open-source framework for building the next generation of AI native software

For the first time in history, products can come to life. We want to help you build something with life.

## Quick Navigation

<CardGroup cols={2}>
  <Card title="Is Cedar right for you?" icon="lightbulb" href="/introduction/philosophy">
    Our philosophy, limitations & strengths, what Cedar is made for
  </Card>

  <Card title="Architecture" icon="diagram-project" href="/introduction/architecture">
    Understand how Cedar-OS is structured and works
  </Card>

  <Card title="Getting Started" icon="rocket" href="/getting-started">
    Start building with Cedar-OS
  </Card>

  <Card title="Demos of Cedar-OS" icon="play" href="/introduction/ai-native-experiences">
    See Cedar-OS in action
  </Card>
</CardGroup>

## Core Features

<CardGroup cols={3}>
  <Card title="Streaming & Tool Calls" icon="bolt" href="/chat/streaming">
    Render real-time message streaming and agent tool execution
  </Card>

  <Card title="Voice Integration" icon="microphone" href="/voice/voice-integration">
    Voice-powered agent interactions
  </Card>

  <Card title="State Access" icon="database" href="/state-access/agentic-state-access">
    Let agents read and write application state
  </Card>

  <Card title="Diff & History" icon="code-compare" href="/state-access/diff-history-manager">
    Track what AI changes, allow user approvals, and manage history
  </Card>

  <Card title="Chat" icon="comments" href="/chat/chat-overview">
    Floating, SidePanel, Caption, or Embedded chat components
  </Card>

  <Card title="Spells" icon="wand-magic-sparkles" href="/spells/spells">
    Powerful agent interactions and workflows
  </Card>

  <Card title="Mentions" icon="at" href="/agent-input-context/mentions">
    Context-aware mention system for chat
  </Card>

  <Card title="Agent Connection Modules" icon="plug" href="/agent-backend-connection/agent-backend-connection">
    Connect to various AI backends and services
  </Card>

  <Card title="Component Library" icon="cube" href="/customising/customising-cedar">
    Pre-built UI components you own the code for, Shadcn style
  </Card>
</CardGroup>


# Is Cedar right for you?
Source: https://docs.cedarcopilot.com/introduction/philosophy

What we built Cedar for, our principles, limitations & strengths

# Should you use Cedar?

Our goal is to enable people building **the most ambitious AI-native applications of the future**.

We believe in the craft and creativity of the community. We believe that giving you control allows you to customise Cedar to make the user experiences better.

## Cedar is fully extensible and customisable

There are good reasons not to use Cedar (see "Cedar is opinionated" below). However, worries about hitting customisability, control, and hitting the limits of Cedar shouldn't be one of them.

1. **Every single component is downloaded Shadcn style. You own all the code and can style it however you want**
2. **Every single internal function can be overridden in one line.**
3. **We will always consider the developer experience in extending and customising Cedar a core part of what we enable.**

## Cedar is simple.

Cedar is built so that everything works out of the box. You want a working chat? Import `<CedarFloatingChat/>` and *it just works*. Want streaming? Add `streaming={true}` to the messages component. We automatically handle the functionality switches.

Because you own the code, we feel much more comfortable creating configurations that work out of the box and should handle 100% of most use cases, knowing you can customise anything.

## Cedar is opinionated

We've been building copilots in production for different companies for the past month, and have converged on systems and structures that work well across the board.

We also believe that good design reflects the nature of the product, and so we're very big on animation to represent the fluidity of AI.

We use tailwind, zustand, and are built on top of react. We're working on mobile, but we want to handle react really well before moving on to different web frameworks.

See [Architecture](/introduction/architecture) for how Cedar works behind the scenes.

## We haven't figured out the future of human-software interaction

> What happens when a new medium enters the scene, is that we tend to fall back to old medium habits. If you go back to first televion shows, they were basically radio shows with a camera pointed at them. It took us the better part of the 50’s to really understand how television was gonna come into its own as it's own medium. **-Steve Jobs**

We think we've had a glimpse of the future, and are working on brand new interfaces for communicating back and forth with LLM's, but we know there are going to be brand new interactions and functionality that we'll need to support.

That's why we've built a pretty generalisable, extensible system as a base, but we expect Cedar to change a lot over the coming months. We hope you'll stick with us to get some of the new exciting things coming out.


# Radial Menu
Source: https://docs.cedarcopilot.com/spells/radial-menu

Using the Radial Menu for spell interactions

The **Radial Menu** is Cedar's flagship spell interface that enables instant AI command execution through an intuitive circular menu system. It provides users with lightning-fast access to complex queries and context-dependent actions with sub-100ms response times.

<img src="https://vrhlhwfhghqbpdpfnpdq.supabase.co/storage/v1/object/public/logos//radialMenu.gif" alt="Radial Menu Demo" />

## What is the Radial Menu?

The Radial Menu is a **context-aware command palette** that appears as a circular interface around your cursor. It's designed to give users magical control over AI interactions without requiring text input - you simply right-click or use a hotkey to instantly access powerful AI commands.

### Key Features

1. **Instant Activation**: Right-click or hotkey activation
2. **Complex Query Support**: Pre-configured prompts for sophisticated AI interactions at the users fingertips
3. **Context-Dependent Commands**: Menu options can change based on your current application state
4. **Sub-100ms Execution**: Commands execute immediately without delay
5. **State-Aware**: Automatically includes relevant application context

## How It Works

### 1. Activation Methods

The Radial Menu can be triggered through multiple methods:

```tsx
import { useRadialMenu } from 'cedar-os';

function MyComponent() {
	const { openRadialMenu, registerHotkey } = useRadialMenu();

	// Method 1: Right-click activation
	const handleContextMenu = (e: React.MouseEvent) => {
		e.preventDefault();
		openRadialMenu({
			x: e.clientX,
			y: e.clientY,
			context: getCurrentContext(),
		});
	};

	// Method 2: Hotkey activation
	useEffect(() => {
		registerHotkey('cmd+k', () => {
			openRadialMenu({
				x: window.innerWidth / 2,
				y: window.innerHeight / 2,
				context: getCurrentContext(),
			});
		});
	}, []);

	return <div onContextMenu={handleContextMenu}>{/* Your content */}</div>;
}
```

### 2. Context-Dependent Commands

The Radial Menu automatically adapts its available commands based on your current context:

```tsx
import { useRadialMenuProvider } from 'cedar-os';

function DocumentEditor() {
	const [selectedText, setSelectedText] = useState('');
	const [currentDocument, setCurrentDocument] = useState(null);

	// Register context-dependent commands
	useRadialMenuProvider({
		id: 'document-editor',
		getCommands: (context) => {
			const commands = [];

			// Text selection commands
			if (context.selectedText) {
				commands.push(
					{
						id: 'explain-selection',
						label: 'Explain This',
						icon: '🤔',
						prompt: `Explain the following text in simple terms: "${context.selectedText}"`,
						color: '#3b82f6',
					},
					{
						id: 'improve-writing',
						label: 'Improve Writing',
						icon: '✨',
						prompt: `Improve the writing and clarity of: "${context.selectedText}"`,
						color: '#10b981',
					}
				);
			}

			// Document-level commands
			if (context.document) {
				commands.push(
					{
						id: 'summarize-doc',
						label: 'Summarize Document',
						icon: '📝',
						prompt: `Summarize the key points of this document: ${context.document.content}`,
						color: '#f59e0b',
					},
					{
						id: 'find-issues',
						label: 'Find Issues',
						icon: '🔍',
						prompt: `Review this document for potential issues, inconsistencies, or improvements: ${context.document.content}`,
						color: '#ef4444',
					}
				);
			}

			return commands;
		},
	});

	return <div>{/* Document editor content */}</div>;
}
```

### 3. Instant Execution

Commands execute immediately when selected, with results appearing in your preferred chat interface:

```tsx
import { useRadialMenu, useCedarStore } from 'cedar-os';

function QuickActions() {
	const { addMessage, sendMessage } = useCedarStore();
	const { registerCommand } = useRadialMenu();

	useEffect(() => {
		// Register instant-execution commands
		registerCommand({
			id: 'quick-note',
			label: 'Quick Note',
			icon: '📋',
			execute: async (context) => {
				// Immediate execution - no text input required
				const prompt = `Create a quick note about: ${context.selectedContent}`;

				addMessage({
					role: 'user',
					type: 'text',
					content: prompt,
				});

				// Execute within 100ms
				await sendMessage({
					route: '/quick-actions',
					systemPrompt: 'You are a helpful note-taking assistant.',
				});
			},
		});
	}, []);
}
```

## Implementation Guide

### Basic Setup

1. **Install the Radial Menu Provider**:

```tsx
import { RadialMenuProvider } from 'cedar-os';

function App() {
	return (
		<RadialMenuProvider>
			<YourAppContent />
		</RadialMenuProvider>
	);
}
```

2. **Configure Default Commands**:

```tsx
import { useRadialMenuConfig } from 'cedar-os';

function AppConfig() {
	useRadialMenuConfig({
		// Global hotkey
		hotkey: 'cmd+k',

		// Default commands available everywhere
		defaultCommands: [
			{
				id: 'ask-anything',
				label: 'Ask Anything',
				icon: '💬',
				prompt: 'I have a question about: ',
				requiresInput: true,
			},
			{
				id: 'explain-page',
				label: 'Explain This Page',
				icon: '📖',
				execute: async (context) => {
					const pageContent = extractPageContent();
					return `Explain what this page is about: ${pageContent}`;
				},
			},
		],

		// Appearance settings
		appearance: {
			radius: 120,
			itemSize: 60,
			animationDuration: 200,
			theme: 'auto', // 'light' | 'dark' | 'auto'
		},
	});
}
```

### Advanced Context Integration

Integrate with Cedar's state system for powerful context-aware commands:

```tsx
import { useCedarState, useRadialMenuProvider } from 'cedar-os';

function TodoApp() {
	const [todos, setTodos] = useCedarState('todos', []);
	const [selectedTodo, setSelectedTodo] = useCedarState('selectedTodo', null);

	useRadialMenuProvider({
		id: 'todo-app',
		getCommands: (context) => {
			const commands = [];

			// Context: Todo selected
			if (selectedTodo) {
				commands.push(
					{
						id: 'analyze-todo',
						label: 'Analyze Task',
						icon: '🔍',
						prompt: `Analyze this task and suggest improvements: ${selectedTodo.title} - ${selectedTodo.description}`,
						color: '#8b5cf6',
					},
					{
						id: 'break-down-todo',
						label: 'Break Down',
						icon: '📋',
						execute: async () => {
							const prompt = `Break down this task into smaller subtasks: ${selectedTodo.title}`;
							// Execute immediately
							return prompt;
						},
					}
				);
			}

			// Context: Multiple todos
			if (todos.length > 0) {
				commands.push({
					id: 'prioritize-todos',
					label: 'Prioritize All',
					icon: '⚡',
					prompt: `Help me prioritize these tasks: ${todos
						.map((t) => t.title)
						.join(', ')}`,
				});
			}

			return commands;
		},
	});

	return <div>{/* Todo app content */}</div>;
}
```

### Custom Command Types

Create different types of commands for various use cases:

```tsx
import { RadialCommand } from 'cedar-os';

// Instant execution command
const instantCommand: RadialCommand = {
	id: 'format-code',
	label: 'Format Code',
	icon: '🎨',
	execute: async (context) => {
		const code = context.selectedText;
		// Process immediately
		return formatCode(code);
	},
	executionTime: 'instant', // <100ms
};

// Streaming command
const streamingCommand: RadialCommand = {
	id: 'explain-complex',
	label: 'Deep Explanation',
	icon: '🧠',
	prompt: (context) =>
		`Provide a detailed explanation of: ${context.selectedText}`,
	executionType: 'streaming', // Shows results as they come in
	estimatedTime: '5-10s',
};

// Interactive command
const interactiveCommand: RadialCommand = {
	id: 'custom-query',
	label: 'Custom Query',
	icon: '💭',
	requiresInput: true,
	inputPlaceholder: 'What would you like to know?',
	execute: async (context, userInput) => {
		return `${userInput} Context: ${context.selectedText}`;
	},
};
```

## Integration with Cedar Features

### With Mentions System

```tsx
import { useStateBasedMentionProvider, useRadialMenuProvider } from 'cedar-os';

function IntegratedApp() {
	const [documents] = useCedarState('documents', []);

	// Enable @mentions for documents
	useStateBasedMentionProvider({
		stateKey: 'documents',
		trigger: '@',
		labelField: 'title',
	});

	// Radial menu can reference mentioned items
	useRadialMenuProvider({
		id: 'document-actions',
		getCommands: (context) => [
			{
				id: 'compare-docs',
				label: 'Compare Documents',
				icon: '⚖️',
				condition: () => context.mentions?.length >= 2,
				prompt: `Compare these documents: ${context.mentions
					.map((m) => m.label)
					.join(' vs ')}`,
			},
		],
	});
}
```

### With Voice Integration

```tsx
import { useVoiceCommands, useRadialMenu } from 'cedar-os';

function VoiceEnabledRadialMenu() {
	const { registerVoiceCommand } = useVoiceCommands();
	const { openRadialMenu } = useRadialMenu();

	useEffect(() => {
		// Voice activation
		registerVoiceCommand('open menu', () => {
			openRadialMenu({
				x: window.innerWidth / 2,
				y: window.innerHeight / 2,
				activationMethod: 'voice',
			});
		});

		// Voice command execution
		registerVoiceCommand('explain this', async () => {
			const context = getCurrentContext();
			return `Explain: ${context.selectedText}`;
		});
	}, []);
}
```

## Best Practices

### 1. Command Organization

* **Group related commands** by color and position
* **Limit to 8-12 commands** per context to avoid cognitive overload
* **Use clear, action-oriented labels** (e.g., "Explain This" not "Explanation")

### 2. Context Sensitivity

* **Always check context availability** before showing commands
* **Provide fallback commands** when no specific context is available
* **Use progressive disclosure** - show more options for power users

### 3. Performance Optimization

* **Cache command generation** for frequently accessed contexts
* **Preload common prompts** to ensure sub-100ms execution
* **Use lazy loading** for complex command calculations

### 4. User Experience

* **Provide visual feedback** during command execution
* **Show keyboard shortcuts** in command labels when available
* **Animate menu appearance** for smooth interactions

## Next Steps

* Learn about [other spell types](/spells/spells)
* Explore [agent input context](/agent-input-context/agent-input-context) for advanced context handling
* Check out [voice integration](/voice/voice-integration) for hands-free operation


# Spells
Source: https://docs.cedarcopilot.com/spells/spells

Overview of Spells system

Spells are our aspirational name for intuitive and powerful interfaces for users to **control AI without text input**. They're called "spells" because we want the user to feel like they're casting a spell.

One of the goals of Cedar is to give people ways to be creative and design human software, and we hope that this will be filled by innovative, fun solutions.

I could talk more about them, but here are some first initial versions.

### Examples

* [Radial Menu](./radial-menu)

<img src="https://vrhlhwfhghqbpdpfnpdq.supabase.co/storage/v1/object/public/logos//radialMenu.gif" alt="Radial Menu Demo" />

* Questioning Cursor
* Tooltip
* Gestures (coming soon)


# Agentic Actions
Source: https://docs.cedarcopilot.com/state-access/agentic-actions

Set up structured responses and interpret them as actions

Cedar enables your agent to not only read state but also execute actions through structured responses. This guide shows you how to set up an end-to-end flow where your agent can understand your application state and perform actions on it.

## Overview

The agentic actions flow consists of three main parts:

1. **Register State with Custom Setters** - Define what actions can be performed
2. **Configure Structured Responses** - Set up your agent to return structured data
3. **Handle LLM Results** - Process the structured responses and execute actions

## Complete Example: Product Roadmap

Let's walk through a complete example using a product roadmap application where the agent can add features, remove nodes, and manage diffs.

### Step 1: Register State with Custom Setters

First, register your state with custom setters that define the available actions:

```tsx
import { useRegisterState } from 'cedar-os';
import { Node } from 'reactflow';

// Register nodes state with custom setters
useRegisterState({
	key: 'nodes',
	value: nodes,
	setValue: setNodes,
	description: 'Product roadmap nodes',
	customSetters: {
		addNode: {
			name: 'addNode',
			description: 'Add a new node to the roadmap',
			parameters: [
				{
					name: 'node',
					type: 'Node<FeatureNodeData>',
					description: 'The node to add',
				},
			],
			execute: (currentNodes, node) => {
				const newNode = {
					...node,
					id: node.id || uuidv4(),
					type: 'featureNode',
					position: { x: Math.random() * 400, y: Math.random() * 400 },
					data: {
						...node.data,
						nodeType: node.data.nodeType || 'feature',
						status: node.data.status || 'planned',
						upvotes: node.data.upvotes || 0,
						comments: node.data.comments || [],
						diff: 'added' as const,
					},
				};
				setNodes([...currentNodes, newNode]);
			},
		},
		removeNode: {
			name: 'removeNode',
			description: 'Remove a node from the roadmap',
			parameters: [
				{
					name: 'id',
					type: 'string',
					description: 'The ID of the node to remove',
				},
			],
			execute: async (currentNodes, id) => {
				// Mark as removed with diff instead of immediate deletion
				setNodes(
					currentNodes.map((node) =>
						node.id === id
							? { ...node, data: { ...node.data, diff: 'removed' } }
							: node
					)
				);
			},
		},
		acceptAllDiffs: {
			name: 'acceptAllDiffs',
			description: 'Accept all pending diffs',
			parameters: [],
			execute: async (currentNodes) => {
				const nodesWithDiffs = currentNodes.filter((n) => n.data.diff);

				// Process removals
				const removedNodeIds = nodesWithDiffs
					.filter((n) => n.data.diff === 'removed')
					.map((n) => n.id);

				for (const nodeId of removedNodeIds) {
					await deleteNode(nodeId);
				}

				// Update remaining nodes
				const remainingNodes = currentNodes.filter(
					(n) => !removedNodeIds.includes(n.id)
				);
				setNodes(
					remainingNodes.map((n) => ({
						...n,
						data: { ...n.data, diff: undefined },
					}))
				);
			},
		},
	},
});
```

### Step 2: Configure Your Agent for Structured Responses

Configure your agent backend to return structured responses. Here's an example using Mastra:

```typescript
// In your Mastra agent configuration
import { createTool } from '@mastra/core/tools';
import { z } from 'zod';

export const addFeatureTool = createTool({
	id: 'add-feature',
	description: 'Add a new feature to the product roadmap',
	inputSchema: z.object({
		title: z.string().describe('Title of the feature'),
		description: z.string().describe('Description of the feature'),
		status: z.enum(['done', 'planned', 'backlog', 'in progress']),
		nodeType: z.literal('feature').default('feature'),
	}),
	outputSchema: z.object({
		type: z.literal('action'),
		stateKey: z.literal('nodes'),
		setterKey: z.literal('addNode'),
		args: z.array(z.any()),
	}),
	execute: async ({ context }) => {
		// Return structured response for Cedar to interpret
		return {
			type: 'action',
			stateKey: 'nodes',
			setterKey: 'addNode',
			args: [
				{
					data: {
						title: context.title,
						description: context.description,
						status: context.status,
						nodeType: context.nodeType,
					},
				},
			],
		};
	},
});
```

### Step 3: Handle LLM Results

Cedar's `handleLLMResult` function automatically processes structured responses:

```typescript
// This is built into Cedar - showing for understanding
handleLLMResult: (response: LLMResponse) => {
	const state = get();

	// Check for structured output
	if (response.object && typeof response.object === 'object') {
		const structuredResponse = response.object;

		// Handle action type responses
		if (structuredResponse.type === 'action') {
			// Execute the custom setter with provided parameters
			state.executeCustomSetter(
				structuredResponse.stateKey,
				structuredResponse.setterKey,
				...structuredResponse.args
			);
			return; // Action executed, no message needed
		}
	}

	// Default: add response as message
	if (response.content) {
		state.addMessage({
			role: 'assistant',
			type: 'text',
			content: response.content,
		});
	}
};
```

## Structured Response Types

Cedar supports different types of structured responses:

### Action Type

Execute a custom setter on registered state:

```json
{
	"type": "action",
	"stateKey": "nodes",
	"setterKey": "addNode",
	"args": [
		{
			/* node data */
		}
	]
}
```

### Message Type

Add a custom message with specific role/content:

```json
{
	"type": "message",
	"role": "assistant",
	"content": "I've added the new feature to your roadmap."
}
```

## Using Actions in Your UI

You can trigger actions directly from your UI components:

```tsx
import { useCedarStore } from 'cedar-os';

function ActionButtons() {
	const executeCustomSetter = useCedarStore(
		(state) => state.executeCustomSetter
	);

	const handleAddFeature = () => {
		executeCustomSetter('nodes', 'addNode', {
			data: {
				title: 'New Feature',
				description: 'Describe your feature here',
				status: 'planned',
				nodeType: 'feature',
			},
		});
	};

	const handleAcceptAllDiffs = () => {
		executeCustomSetter('nodes', 'acceptAllDiffs');
	};

	return (
		<div className='flex gap-2'>
			<button onClick={handleAddFeature}>Add Feature</button>
			<button onClick={handleAcceptAllDiffs}>Accept All Changes</button>
		</div>
	);
}
```

## Best Practices

### 1. Use Descriptive Action Names

Make your setter names clear and action-oriented:

```tsx
// Good
customSetters: {
  addTodo: { /* ... */ },
  toggleTodoComplete: { /* ... */ },
  deleteTodo: { /* ... */ }
}

// Avoid
customSetters: {
  setter1: { /* ... */ },
  update: { /* ... */ },
  change: { /* ... */ }
}
```

### 2. Include Parameter Descriptions

Help your agent understand what parameters to provide:

```tsx
parameters: [
	{
		name: 'priority',
		type: 'string',
		description: 'Priority level: low, medium, high, or critical',
	},
];
```

### 3. Handle Errors Gracefully

Add error handling in your custom setters:

```tsx
execute: async (currentState, id) => {
	try {
		const item = currentState.find((item) => item.id === id);
		if (!item) {
			console.error(`Item with id ${id} not found`);
			return;
		}
		// Perform action
	} catch (error) {
		console.error('Failed to execute action:', error);
	}
};
```

### 4. Use Diff Patterns for Reversible Actions

Implement diff patterns for actions that users might want to review:

```tsx
execute: (currentNodes, nodeData) => {
	// Add with diff marker
	const newNode = {
		...nodeData,
		data: { ...nodeData.data, diff: 'added' },
	};
	setNodes([...currentNodes, newNode]);
};
```

## Advanced: Multi-Step Actions

For complex workflows, you can chain multiple actions:

```typescript
// Agent returns multiple actions
{
  "type": "multi-action",
  "actions": [
    {
      "type": "action",
      "stateKey": "nodes",
      "setterKey": "addNode",
      "args": [/* node 1 */]
    },
    {
      "type": "action",
      "stateKey": "edges",
      "setterKey": "connectNodes",
      "args": ["node1", "node2"]
    }
  ]
}
```

## Debugging Actions

Use Cedar's built-in debugging tools to monitor action execution:

```tsx
import { useCedarStore } from 'cedar-os';

// Enable debug mode to see all state changes
const debugMode = useCedarStore((state) => state.debugMode);

// View registered states and their setters
const registeredStates = useCedarStore((state) => state.registeredStates);
console.log('Available actions:', registeredStates);
```

## Next Steps

* Learn about [State Access](/state-access/agentic-state-access) for more state management patterns
* Explore [Agent Input Context](/agent-input-context/agent-input-context) to provide context for actions
* See [Custom Message Rendering](/chat/custom-message-rendering) to display action results


# Agentic State
Source: https://docs.cedarcopilot.com/state-access/agentic-state-access



Cedar allows your agent to understand what is happening in your application, and the ability to change it

In other words, it allows your agent to read and write to the local react state.

## useRegisterState Hook

Register your existing React state with Cedar using the `useRegisterState` hook:

<CodeGroup>
  ```tsx Example
  import { useRegisterState } from 'cedar-os';

  const [todos, setTodos] = useState([
  	{ id: 1, text: 'Learn Cedar-OS', completed: false },
  	{ id: 2, text: 'Build amazing AI apps', completed: false },
  ]);

  // Now the agent will know what the state is, how to change it,
  // and have access to calling these setters
  useRegisterState({
  	key: 'todos',
  	description: 'A list of todo items that users can check off',
  	value: todos,
  	setValue: setTodos,
  	customSetters: {
  		addTodo: {
  			name: 'addTodo',
  			description: 'Add a new todo item',
  			execute: (currentTodos, text: string) => {
  				const newTodo = {
  					id: Date.now(),
  					text,
  					completed: false,
  				};
  				setTodos([...currentTodos, newTodo]);
  			},
  		},
  		toggleTodo: {
  			name: 'toggleTodo',
  			description: 'Toggle completion status of a todo',
  			execute: (currentTodos, id: number) => {
  				setTodos(
  					currentTodos.map((todo) =>
  						todo.id === id ? { ...todo, completed: !todo.completed } : todo
  					)
  				);
  			},
  		},
  		removeTodo: {
  			name: 'removeTodo',
  			description: 'Remove a todo item',
  			execute: (currentTodos, id: number) => {
  				setTodos(currentTodos.filter((todo) => todo.id !== id));
  			},
  		},
  	},
  });
  ```

  ```tsx Type
  interface RegisterStateOptions<T> {
  	key: string;
  	description?: string;
  	value: T;
  	setValue?: (state: T, ...args: any[]) => void;
  	customSetters?: Record<string, Setter<T>>;
  	schema?: ZodSchema<T>;
  }

  interface Setter<T, Args extends unknown[] = unknown[]> {
  	name: string;
  	description: string;
  	parameters?: SetterParameter[];
  	execute: (state: T, ...args: Args) => void;
  }

  interface Todo {
  	id: number;
  	text: string;
  	completed: boolean;
  }
  ```
</CodeGroup>

### useCedarState

Use `useCedarState` to create and manage state directly in the Cedar store. It directly replaces useState and works the exact same way, but

<CodeGroup>
  ```tsx Example
  import { useCedarState } from 'cedar-os';

  function TodoComponent() {
  	const [todos, setTodos] = useCedarState(
  		'todos',
  		[
  			{ id: 1, text: 'Learn Cedar-OS', completed: false },
  			{ id: 2, text: 'Build amazing AI apps', completed: false },
  		],
  		'A list of todo items that users can check off',
  		{
  			addTodo: {
  				name: 'addTodo',
  				description: 'Add a new todo item',
  				execute: (currentTodos, text: string) => {
  					const newTodo = {
  						id: Date.now(),
  						text,
  						completed: false,
  					};
  					setTodos([...currentTodos, newTodo]);
  				},
  			},
  			toggleTodo: {
  				name: 'toggleTodo',
  				description: 'Toggle completion status of a todo',
  				execute: (currentTodos, id: number) => {
  					setTodos(
  						currentTodos.map((todo) =>
  							todo.id === id ? { ...todo, completed: !todo.completed } : todo
  						)
  					);
  				},
  			},
  			removeTodo: {
  				name: 'removeTodo',
  				description: 'Remove a todo item',
  				execute: (currentTodos, id: number) => {
  					setTodos(currentTodos.filter((todo) => todo.id !== id));
  				},
  			},
  		}
  	);

  	return (
  		<div>
  			<h2>My Todos</h2>
  			{todos.map((todo) => (
  				<div key={todo.id}>
  					<input
  						type='checkbox'
  						checked={todo.completed}
  						onChange={() => {
  							// You can call the setter directly or use custom setters
  							setTodos(
  								todos.map((t) =>
  									t.id === todo.id ? { ...t, completed: !t.completed } : t
  								)
  							);
  						}}
  					/>
  					<span>{todo.text}</span>
  					<button
  						onClick={() => {
  							setTodos(todos.filter((t) => t.id !== todo.id));
  						}}>
  						Delete
  					</button>
  				</div>
  			))}
  			<button
  				onClick={() => {
  					const newTodo = {
  						id: Date.now(),
  						text: 'New todo',
  						completed: false,
  					};
  					setTodos([...todos, newTodo]);
  				}}>
  				Add Todo
  			</button>
  		</div>
  	);
  }
  ```

  ```tsx Type
  function useCedarState<T>(
  	key: string,
  	initialValue: T,
  	description?: string,
  	customSetters?: Record<string, Setter<T>>,
  	schema?: ZodSchema<T>
  ): [T, (newValue: T) => void];
  ```
</CodeGroup>

## Accessing Cedar State Functions

### executeCustomSetter

Execute custom setter functions programmatically using the Cedar store:

<CodeGroup>
  ```tsx Example
  import { useCedarStore } from 'cedar-os';

  function TodoActions() {
  	const executeCustomSetter = useCedarStore(
  		(state) => state.executeCustomSetter
  	);

  	const handleAddTodo = async () => {
  		executeCustomSetter('todos', 'addTodo', 'Learn about Cedar State');
  	};

  	const handleToggleTodo = async () => {
  		executeCustomSetter('todos', 'toggleTodo', 1);
  	};

  	const handleRemoveTodo = async () => {
  		executeCustomSetter('todos', 'removeTodo', 1);
  	};

  	return (
  		<div>
  			<button onClick={handleAddTodo}>Add Todo</button>
  			<button onClick={handleToggleTodo}>Toggle First Todo</button>
  			<button onClick={handleRemoveTodo}>Remove First Todo</button>
  		</div>
  	);
  }
  ```

  ```tsx Type
  // Access through useCedarStore
  const executeCustomSetter = useCedarStore((state) => state.executeCustomSetter);

  // Function signature
  executeCustomSetter: (
  	key: string,
  	setterKey: string,
  	...args: unknown[]
  ) => void;
  ```
</CodeGroup>

## getCedarState

Retrieve the current state value for a registered Cedar state key:

<CodeGroup>
  ```tsx Example
  import { getCedarState } from 'cedar-os';

  // Get current todos state
  const currentTodos = getCedarState('todos');
  console.log('Current todos:', currentTodos);

  // Use in a function
  function logTodoCount() {
  	const todos = getCedarState('todos');
  	if (todos && Array.isArray(todos)) {
  		console.log(`You have ${todos.length} todos`);
  	}
  }

  // Check if state exists
  function checkTodosExist() {
  	const todos = getCedarState('todos');
  	if (todos) {
  		console.log('Todos state is available');
  	} else {
  		console.log('Todos state not found');
  	}
  }
  ```

  ```tsx Type
  function getCedarState(key: string): BasicStateValue | undefined;
  ```
</CodeGroup>

## setCedarState

Directly update the state value for a registered Cedar state key:

<CodeGroup>
  ```tsx Example
  import { setCedarState } from 'cedar-os';

  // Set new todos state
  const newTodos = [
  	{ id: 1, text: 'Updated todo', completed: true },
  	{ id: 2, text: 'Another todo', completed: false },
  ];
  setCedarState('todos', newTodos);

  // Update based on current state
  const currentTodos = getCedarState('todos');
  if (currentTodos && Array.isArray(currentTodos)) {
  	const updatedTodos = currentTodos.map((todo) => ({
  		...todo,
  		completed: true,
  	}));
  	setCedarState('todos', updatedTodos);
  }

  // Reset state
  setCedarState('todos', []);
  ```

  ```tsx Type
  function setCedarState<T>(key: string, value: T): void;
  ```
</CodeGroup>


# Diff & History Manager (Beta)
Source: https://docs.cedarcopilot.com/state-access/diff-history-manager

AI should be creating diffs all the time, so we're tracking it

AI should be creating diffs all the time, so we're building a system to track and manage these changes.

## Coming Soon

Content for this section will be added in the next phase.


# Backend Integration
Source: https://docs.cedarcopilot.com/voice/agentic-backend

Setting up your backend to handle voice requests and responses

Cedar's voice system sends audio data to your backend for processing and expects either audio responses or structured JSON responses. This page covers how to implement the backend endpoint to handle voice interactions.

## Endpoint Configuration

### Automatic Configuration with Mastra

When using the Mastra provider, you can automatically configure the voice endpoint through the provider configuration:

```typescript
<CedarCopilot
	llmProvider={{
		provider: 'mastra',
		baseURL: 'http://localhost:3000/api',
		voiceRoute: '/chat/voice-execute', // Automatically sets voice endpoint to: http://localhost:3000/api/chat/voice-execute
	}}>
	<YourApp />
</CedarCopilot>
```

This eliminates the need to manually call `setVoiceEndpoint()` and ensures consistency between your chat and voice endpoints.

### Manual Configuration

For other providers or custom setups, configure the endpoint manually:

```typescript
const voice = useCedarStore((state) => state.voice);
voice.setVoiceEndpoint('https://your-backend.com/api/voice');
```

## Request Format

The voice system sends a `multipart/form-data` POST request to your configured endpoint with the following fields:

```typescript
FormData {
  audio: Blob,      // WebM format with Opus codec
  settings: string, // JSON string of voice settings
  context: string   // JSON string of additional context
}
```

### Audio Data

* **Format**: WebM container with Opus codec
* **Type**: Binary blob from MediaRecorder API
* **Quality**: Optimized for speech recognition

### Voice Settings

```typescript
{
  language: string;     // e.g., 'en-US'
  voiceId?: string;     // Optional voice ID for TTS
  pitch?: number;       // 0.5 to 2.0
  rate?: number;        // 0.5 to 2.0
  volume?: number;      // 0.0 to 1.0
  useBrowserTTS?: boolean;
  autoAddToMessages?: boolean;
}
```

### Additional Context

The context includes Cedar's additional context data (file contents, state information, etc.) that can be used to provide better responses.

## Response Formats

Your backend can respond in several ways:

### 1. Direct Audio Response

Return audio data directly with the appropriate content type:

```typescript
// Response headers
Content-Type: audio/mpeg
// or audio/wav, audio/ogg, etc.

// Response body: Raw audio data
```

### 2. JSON Response with Audio URL

```typescript
{
  "audioUrl": "https://example.com/response.mp3",
  "text": "Optional text transcript",
  "transcription": "What the user said"
}
```

### 3. JSON Response with Base64 Audio

```typescript
{
  "audioData": "base64-encoded-audio-data",
  "audioFormat": "mp3", // or "wav", "ogg"
  "text": "Response text",
  "transcription": "User input transcription"
}
```

### 4. Structured Response with Actions

```typescript
{
  "transcription": "Show me the user dashboard",
  "text": "I'll show you the user dashboard",
  "object": {
    "type": "action",
    "stateKey": "ui",
    "setterKey": "navigateTo",
    "args": ["/dashboard"]
  },
  "audioUrl": "https://example.com/response.mp3"
}
```

## Implementation Examples

### Node.js with Express

```typescript
import express from 'express';
import multer from 'multer';
import OpenAI from 'openai';

const app = express();
const upload = multer();
const openai = new OpenAI();

app.post(
	'/api/chat/voice',
	upload.fields([
		{ name: 'audio', maxCount: 1 },
		{ name: 'settings', maxCount: 1 },
		{ name: 'context', maxCount: 1 },
	]),
	async (req, res) => {
		try {
			const audioFile = req.files.audio[0];
			const settings = JSON.parse(req.body.settings);
			const context = JSON.parse(req.body.context);

			// 1. Transcribe audio to text
			const transcription = await openai.audio.transcriptions.create({
				file: new File([audioFile.buffer], 'audio.webm', {
					type: 'audio/webm',
				}),
				model: 'whisper-1',
				language: settings.language?.split('-')[0] || 'en',
			});

			// 2. Process with your AI agent
			const messages = [
				{ role: 'system', content: 'You are a helpful assistant.' },
				{ role: 'user', content: transcription.text },
			];

			const completion = await openai.chat.completions.create({
				model: 'gpt-4',
				messages: messages,
			});

			const responseText = completion.choices[0].message.content;

			// 3. Convert response to speech
			const speech = await openai.audio.speech.create({
				model: 'tts-1',
				voice: settings.voiceId || 'alloy',
				input: responseText,
				speed: settings.rate || 1.0,
			});

			const audioBuffer = Buffer.from(await speech.arrayBuffer());

			// 4. Return audio response
			res.set({
				'Content-Type': 'audio/mpeg',
				'Content-Length': audioBuffer.length,
			});
			res.send(audioBuffer);
		} catch (error) {
			console.error('Voice processing error:', error);
			res.status(500).json({ error: 'Failed to process voice request' });
		}
	}
);
```

### Python with FastAPI

```python
from fastapi import FastAPI, File, Form, UploadFile
from fastapi.responses import Response
import openai
import json
import io

app = FastAPI()

@app.post("/api/chat/voice")
async def handle_voice(
    audio: UploadFile = File(...),
    settings: str = Form(...),
    context: str = Form(...)
):
    try:
        # Parse settings and context
        voice_settings = json.loads(settings)
        additional_context = json.loads(context)

        # Read audio data
        audio_data = await audio.read()

        # 1. Transcribe audio
        audio_file = io.BytesIO(audio_data)
        audio_file.name = "audio.webm"

        transcription = openai.Audio.transcribe(
            model="whisper-1",
            file=audio_file,
            language=voice_settings.get("language", "en")[:2]
        )

        # 2. Process with AI
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": transcription["text"]}
            ]
        )

        response_text = response.choices[0].message.content

        # 3. Generate speech
        speech_response = openai.Audio.speech.create(
            model="tts-1",
            voice=voice_settings.get("voiceId", "alloy"),
            input=response_text,
            speed=voice_settings.get("rate", 1.0)
        )

        # 4. Return audio
        return Response(
            content=speech_response.content,
            media_type="audio/mpeg"
        )

    except Exception as e:
        return {"error": str(e)}, 500
```

### Mastra Agent Integration

When using Cedar-OS with the Mastra provider and `voiceRoute` configuration, your Mastra backend should handle requests at the specified route. Here's how to implement the voice handler:

```typescript
import { Agent } from '@mastra/core';
import { openai } from '@mastra/openai';

const agent = new Agent({
	name: 'voice-assistant',
	instructions: 'You are a helpful voice assistant.',
	model: openai.gpt4o(),
});

export async function handleVoiceRequest(
	audioBlob: Blob,
	settings: VoiceSettings,
	context: string
) {
	// 1. Transcribe audio
	const transcription = await openai.audio.transcriptions.create({
		file: audioBlob,
		model: 'whisper-1',
		language: settings.language?.split('-')[0] || 'en',
	});

	// 2. Add context to the conversation
	const contextData = JSON.parse(context);
	const systemMessage = `Additional context: ${JSON.stringify(contextData)}`;

	// 3. Generate response with agent
	const response = await agent.generate([
		{ role: 'system', content: systemMessage },
		{ role: 'user', content: transcription.text },
	]);

	// 4. Convert to speech
	const speech = await openai.audio.speech.create({
		model: 'tts-1',
		voice: settings.voiceId || 'alloy',
		input: response.text,
		speed: settings.rate || 1.0,
	});

	return {
		transcription: transcription.text,
		text: response.text,
		audioData: await speech.arrayBuffer(),
		audioFormat: 'mp3',
	};
}

// Example Mastra route setup
// If you configured voiceRoute: '/chat/voice-execute' in Cedar-OS,
// your Mastra backend should handle POST requests to this route:

app.post(
	'/chat/voice-execute',
	upload.fields([
		{ name: 'audio', maxCount: 1 },
		{ name: 'settings', maxCount: 1 },
		{ name: 'context', maxCount: 1 },
	]),
	async (req, res) => {
		const audioFile = req.files.audio[0];
		const settings = JSON.parse(req.body.settings);
		const context = req.body.context;

		const result = await handleVoiceRequest(
			new Blob([audioFile.buffer]),
			settings,
			context
		);

		// Return the structured response
		res.json(result);
	}
);
```

## Error Handling

Your backend should handle various error cases:

```typescript
app.post('/api/chat/voice', async (req, res) => {
	try {
		// ... processing logic
	} catch (error) {
		console.error('Voice processing error:', error);

		// Return appropriate error response
		if (error.code === 'AUDIO_TRANSCRIPTION_FAILED') {
			return res.status(400).json({
				error: 'Could not understand the audio. Please try again.',
				code: 'TRANSCRIPTION_ERROR',
			});
		}

		if (error.code === 'TTS_FAILED') {
			return res.status(500).json({
				error: 'Failed to generate speech response.',
				code: 'TTS_ERROR',
				// Fallback to text-only response
				text: responseText,
				transcription: userInput,
			});
		}

		// Generic error
		res.status(500).json({
			error: 'Internal server error processing voice request',
			code: 'INTERNAL_ERROR',
		});
	}
});
```

## CORS Configuration

Ensure your backend allows CORS for the frontend domain:

```typescript
import cors from 'cors';

app.use(
	cors({
		origin: ['http://localhost:3000', 'https://your-app-domain.com'],
		methods: ['POST'],
		allowedHeaders: ['Content-Type', 'Authorization'],
	})
);
```

## Performance Considerations

### Audio Processing

* Use streaming transcription for real-time responses
* Implement audio compression to reduce bandwidth
* Cache TTS responses for common phrases

### Response Optimization

* Stream audio responses when possible
* Use CDN for serving generated audio files
* Implement request queuing for high-traffic scenarios

```typescript
// Example streaming response
app.post('/api/chat/voice-stream', async (req, res) => {
	res.writeHead(200, {
		'Content-Type': 'audio/mpeg',
		'Transfer-Encoding': 'chunked',
	});

	const audioStream = await generateAudioStream(transcription);

	audioStream.on('data', (chunk) => {
		res.write(chunk);
	});

	audioStream.on('end', () => {
		res.end();
	});
});
```

## Security Best Practices

1. **Rate Limiting**: Prevent abuse of voice endpoints
2. **Authentication**: Verify user permissions
3. **Input Validation**: Sanitize audio data and settings
4. **Content Filtering**: Screen transcriptions for inappropriate content

```typescript
import rateLimit from 'express-rate-limit';

const voiceRateLimit = rateLimit({
	windowMs: 15 * 60 * 1000, // 15 minutes
	max: 50, // 50 requests per window
	message: 'Too many voice requests, please try again later.',
});

app.post('/api/chat/voice', voiceRateLimit, handleVoiceRequest);
```

## Testing Your Integration

Test your voice endpoint with curl:

```bash
# Create test audio file
curl -X POST http://localhost:3456/api/chat/voice \
  -F "audio=@test-audio.webm" \
  -F "settings={\"language\":\"en-US\",\"rate\":1.0}" \
  -F "context={\"files\":[],\"state\":{}}" \
  -o response.mp3
```

Or use the Cedar voice system directly for end-to-end testing:

```typescript
// With Mastra provider (automatic configuration)
<CedarCopilot
	llmProvider={{
		provider: 'mastra',
		baseURL: 'http://localhost:3456/api',
		voiceRoute: '/chat/voice-execute',
	}}>
	<YourApp />
</CedarCopilot>;

// Or manually configure the endpoint
const voice = useCedarStore((state) => state.voice);
voice.setVoiceEndpoint('http://localhost:3456/api/chat/voice');
await voice.requestVoicePermission();
voice.startListening();
```


# Voice Components
Source: https://docs.cedarcopilot.com/voice/components

UI components for voice interactions including indicators, controls, and visualizations

Cedar provides pre-built React components for voice interactions, along with guidance for creating custom voice UI components. These components integrate seamlessly with the voice state management system.

## VoiceIndicator Component

The `VoiceIndicator` component provides visual feedback for voice states with smooth animations powered by Motion for React.

### Basic Usage

```typescript
import { VoiceIndicator } from '@cedar/voice';
import { useCedarStore } from '@cedar/core';

function MyVoiceApp() {
	const voice = useCedarStore((state) => state.voice);

	return (
		<div>
			<VoiceIndicator voiceState={voice} />
			{/* Your other components */}
		</div>
	);
}
```

### Component Features

* **Listening State**: Animated microphone icon with pulsing bars
* **Speaking State**: Animated speaker icon with scaling effect
* **Error State**: Alert icon with error message
* **Auto-hide**: Only shows when voice is active or there's an error

### Animation Details

The component uses Motion for React with optimized animations:

```typescript
// Listening animation - pulsing bars
{
	[0, 1, 2].map((i) => (
		<motion.div
			key={i}
			className='w-1 h-3 bg-red-500 rounded-full'
			animate={{
				scaleY: [1, 1.5, 1],
			}}
			transition={{
				duration: 0.5,
				repeat: Infinity,
				delay: i * 0.1,
			}}
		/>
	));
}

// Speaking animation - scaling effect
<motion.div
	className='w-4 h-4'
	animate={{
		scale: [1, 1.2, 1],
	}}
	transition={{
		duration: 0.8,
		repeat: Infinity,
	}}>
	<div className='w-full h-full bg-green-500 rounded-full opacity-30' />
</motion.div>;
```

## Custom Voice Button

Create a custom voice button with enhanced visual feedback:

```typescript
import React from 'react';
import { motion } from 'motion/react';
import { Mic, MicOff, Loader2 } from 'lucide-react';
import { useCedarStore } from '@cedar/core';

interface VoiceButtonProps {
	size?: 'small' | 'medium' | 'large';
	variant?: 'primary' | 'secondary' | 'outline';
	className?: string;
}

export const VoiceButton: React.FC<VoiceButtonProps> = ({
	size = 'medium',
	variant = 'primary',
	className = '',
}) => {
	const voice = useCedarStore((state) => state.voice);

	const handleClick = async () => {
		if (voice.voicePermissionStatus === 'prompt') {
			await voice.requestVoicePermission();
		}

		if (voice.voicePermissionStatus === 'granted') {
			voice.toggleVoice();
		}
	};

	const sizeClasses = {
		small: 'w-8 h-8',
		medium: 'w-12 h-12',
		large: 'w-16 h-16',
	};

	const variantClasses = {
		primary: 'bg-blue-500 hover:bg-blue-600 text-white',
		secondary: 'bg-gray-500 hover:bg-gray-600 text-white',
		outline: 'border-2 border-blue-500 text-blue-500 hover:bg-blue-50',
	};

	const isDisabled =
		voice.voicePermissionStatus === 'denied' ||
		voice.voicePermissionStatus === 'not-supported';

	return (
		<motion.button
			onClick={handleClick}
			disabled={isDisabled}
			className={`
        ${sizeClasses[size]}
        ${variantClasses[variant]}
        ${className}
        rounded-full flex items-center justify-center
        transition-colors duration-200
        disabled:opacity-50 disabled:cursor-not-allowed
        focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-offset-2
      `}
			whileHover={!isDisabled ? { scale: 1.05 } : {}}
			whileTap={!isDisabled ? { scale: 0.95 } : {}}
			animate={{
				backgroundColor: voice.isListening ? '#ef4444' : undefined,
			}}
			style={{ willChange: 'transform' }}>
			{voice.voicePermissionStatus === 'prompt' && (
				<Loader2 className='w-5 h-5 animate-spin' />
			)}

			{voice.voicePermissionStatus === 'granted' && (
				<>
					{voice.isListening ? (
						<motion.div
							initial={{ scale: 0 }}
							animate={{ scale: 1 }}
							transition={{ type: 'spring', stiffness: 500, damping: 30 }}>
							<MicOff className='w-5 h-5' />
						</motion.div>
					) : (
						<motion.div
							initial={{ scale: 0 }}
							animate={{ scale: 1 }}
							transition={{ type: 'spring', stiffness: 500, damping: 30 }}>
							<Mic className='w-5 h-5' />
						</motion.div>
					)}
				</>
			)}

			{voice.voicePermissionStatus === 'denied' && (
				<MicOff className='w-5 h-5 opacity-50' />
			)}
		</motion.button>
	);
};
```

## Voice Waveform Visualizer

Create an animated waveform visualizer for audio input:

```typescript
import React, { useEffect, useRef, useState } from 'react';
import { motion } from 'motion/react';
import { useCedarStore } from '@cedar/core';

interface VoiceWaveformProps {
	barCount?: number;
	height?: number;
	color?: string;
	className?: string;
}

export const VoiceWaveform: React.FC<VoiceWaveformProps> = ({
	barCount = 20,
	height = 40,
	color = '#3b82f6',
	className = '',
}) => {
	const voice = useCedarStore((state) => state.voice);
	const [audioData, setAudioData] = useState<number[]>([]);
	const analyserRef = useRef<AnalyserNode | null>(null);
	const animationRef = useRef<number>();

	useEffect(() => {
		if (voice.isListening && voice.audioContext && voice.audioStream) {
			// Create audio analyser
			const analyser = voice.audioContext.createAnalyser();
			const source = voice.audioContext.createMediaStreamSource(
				voice.audioStream
			);

			source.connect(analyser);
			analyser.fftSize = 256;

			const bufferLength = analyser.frequencyBinCount;
			const dataArray = new Uint8Array(bufferLength);

			analyserRef.current = analyser;

			const updateWaveform = () => {
				if (analyser) {
					analyser.getByteFrequencyData(dataArray);

					// Sample data for visualization
					const step = Math.floor(bufferLength / barCount);
					const samples = [];

					for (let i = 0; i < barCount; i++) {
						const sample = dataArray[i * step] || 0;
						samples.push(sample / 255); // Normalize to 0-1
					}

					setAudioData(samples);
				}

				if (voice.isListening) {
					animationRef.current = requestAnimationFrame(updateWaveform);
				}
			};

			updateWaveform();
		} else {
			// Reset when not listening
			setAudioData(new Array(barCount).fill(0));
			if (animationRef.current) {
				cancelAnimationFrame(animationRef.current);
			}
		}

		return () => {
			if (animationRef.current) {
				cancelAnimationFrame(animationRef.current);
			}
		};
	}, [voice.isListening, voice.audioContext, voice.audioStream, barCount]);

	return (
		<div
			className={`flex items-end justify-center gap-1 ${className}`}
			style={{ height: `${height}px` }}>
			{Array.from({ length: barCount }).map((_, index) => {
				const amplitude = audioData[index] || 0;
				const barHeight = Math.max(2, amplitude * height);

				return (
					<motion.div
						key={index}
						className='w-1 rounded-full'
						style={{
							backgroundColor: color,
							willChange: 'height',
						}}
						animate={{
							height: barHeight,
						}}
						transition={{
							duration: 0.1,
							ease: 'easeOut',
						}}
					/>
				);
			})}
		</div>
	);
};
```

## Voice Status Panel

A comprehensive status panel showing all voice information:

```typescript
import React from 'react';
import { motion, AnimatePresence } from 'motion/react';
import {
	Mic,
	MicOff,
	Volume2,
	VolumeX,
	Wifi,
	WifiOff,
	AlertCircle,
	CheckCircle,
} from 'lucide-react';
import { useCedarStore } from '@cedar/core';

export const VoiceStatusPanel: React.FC = () => {
	const voice = useCedarStore((state) => state.voice);

	const getPermissionStatus = () => {
		switch (voice.voicePermissionStatus) {
			case 'granted':
				return {
					icon: CheckCircle,
					color: 'text-green-500',
					text: 'Microphone access granted',
				};
			case 'denied':
				return {
					icon: MicOff,
					color: 'text-red-500',
					text: 'Microphone access denied',
				};
			case 'not-supported':
				return {
					icon: AlertCircle,
					color: 'text-orange-500',
					text: 'Voice not supported',
				};
			default:
				return {
					icon: Mic,
					color: 'text-gray-500',
					text: 'Microphone permission needed',
				};
		}
	};

	const getConnectionStatus = () => {
		// Assuming WebSocket connection status is available
		const isConnected = voice.voiceEndpoint && !voice.voiceError;
		return {
			icon: isConnected ? Wifi : WifiOff,
			color: isConnected ? 'text-green-500' : 'text-red-500',
			text: isConnected
				? 'Connected to voice service'
				: 'Disconnected from voice service',
		};
	};

	const permissionStatus = getPermissionStatus();
	const connectionStatus = getConnectionStatus();

	return (
		<motion.div
			initial={{ opacity: 0, y: 20 }}
			animate={{ opacity: 1, y: 0 }}
			className='bg-white rounded-lg shadow-lg p-6 max-w-md mx-auto'>
			<h3 className='text-lg font-semibold mb-4'>Voice Status</h3>

			{/* Permission Status */}
			<div className='flex items-center gap-3 mb-4'>
				<permissionStatus.icon
					className={`w-5 h-5 ${permissionStatus.color}`}
				/>
				<span className='text-sm'>{permissionStatus.text}</span>
			</div>

			{/* Connection Status */}
			<div className='flex items-center gap-3 mb-4'>
				<connectionStatus.icon
					className={`w-5 h-5 ${connectionStatus.color}`}
				/>
				<span className='text-sm'>{connectionStatus.text}</span>
			</div>

			{/* Current State */}
			<div className='flex items-center gap-3 mb-4'>
				{voice.isListening && (
					<>
						<Mic className='w-5 h-5 text-red-500' />
						<span className='text-sm'>Listening...</span>
						<motion.div
							className='ml-auto flex gap-1'
							initial={{ opacity: 0 }}
							animate={{ opacity: 1 }}>
							{[0, 1, 2].map((i) => (
								<motion.div
									key={i}
									className='w-1 h-3 bg-red-500 rounded-full'
									animate={{
										scaleY: [1, 1.5, 1],
									}}
									transition={{
										duration: 0.5,
										repeat: Infinity,
										delay: i * 0.1,
									}}
								/>
							))}
						</motion.div>
					</>
				)}

				{voice.isSpeaking && (
					<>
						<Volume2 className='w-5 h-5 text-green-500' />
						<span className='text-sm'>Speaking...</span>
						<motion.div
							className='ml-auto w-4 h-4'
							animate={{
								scale: [1, 1.2, 1],
							}}
							transition={{
								duration: 0.8,
								repeat: Infinity,
							}}>
							<div className='w-full h-full bg-green-500 rounded-full opacity-30' />
						</motion.div>
					</>
				)}

				{!voice.isListening && !voice.isSpeaking && (
					<>
						<VolumeX className='w-5 h-5 text-gray-400' />
						<span className='text-sm text-gray-500'>Idle</span>
					</>
				)}
			</div>

			{/* Voice Settings */}
			<div className='border-t pt-4'>
				<h4 className='text-sm font-medium mb-2'>Settings</h4>
				<div className='text-xs text-gray-600 space-y-1'>
					<div>Language: {voice.voiceSettings.language}</div>
					<div>Voice: {voice.voiceSettings.voiceId || 'Default'}</div>
					<div>Rate: {voice.voiceSettings.rate || 1.0}x</div>
					<div>
						Auto-add to messages:{' '}
						{voice.voiceSettings.autoAddToMessages ? 'Yes' : 'No'}
					</div>
				</div>
			</div>

			{/* Error Display */}
			<AnimatePresence>
				{voice.voiceError && (
					<motion.div
						initial={{ opacity: 0, height: 0 }}
						animate={{ opacity: 1, height: 'auto' }}
						exit={{ opacity: 0, height: 0 }}
						className='mt-4 p-3 bg-red-50 border border-red-200 rounded-md'>
						<div className='flex items-center gap-2'>
							<AlertCircle className='w-4 h-4 text-red-500' />
							<span className='text-sm text-red-700'>{voice.voiceError}</span>
						</div>
					</motion.div>
				)}
			</AnimatePresence>
		</motion.div>
	);
};
```

## Voice Settings Component

A settings panel for configuring voice options:

```typescript
import React from 'react';
import { motion } from 'motion/react';
import { Settings, Volume2, Mic } from 'lucide-react';
import { useCedarStore } from '@cedar/core';

export const VoiceSettings: React.FC = () => {
	const voice = useCedarStore((state) => state.voice);

	const voiceOptions = [
		{ value: 'alloy', label: 'Alloy' },
		{ value: 'echo', label: 'Echo' },
		{ value: 'fable', label: 'Fable' },
		{ value: 'onyx', label: 'Onyx' },
		{ value: 'nova', label: 'Nova' },
		{ value: 'shimmer', label: 'Shimmer' },
	];

	const languageOptions = [
		{ value: 'en-US', label: 'English (US)' },
		{ value: 'en-GB', label: 'English (UK)' },
		{ value: 'es-ES', label: 'Spanish' },
		{ value: 'fr-FR', label: 'French' },
		{ value: 'de-DE', label: 'German' },
		{ value: 'it-IT', label: 'Italian' },
		{ value: 'pt-BR', label: 'Portuguese (Brazil)' },
		{ value: 'ja-JP', label: 'Japanese' },
		{ value: 'ko-KR', label: 'Korean' },
		{ value: 'zh-CN', label: 'Chinese (Simplified)' },
	];

	return (
		<motion.div
			initial={{ opacity: 0, scale: 0.95 }}
			animate={{ opacity: 1, scale: 1 }}
			className='bg-white rounded-lg shadow-lg p-6 max-w-md mx-auto'>
			<div className='flex items-center gap-2 mb-6'>
				<Settings className='w-5 h-5' />
				<h3 className='text-lg font-semibold'>Voice Settings</h3>
			</div>

			<div className='space-y-6'>
				{/* Language Selection */}
				<div>
					<label className='block text-sm font-medium text-gray-700 mb-2'>
						<Mic className='w-4 h-4 inline mr-1' />
						Language
					</label>
					<select
						value={voice.voiceSettings.language}
						onChange={(e) =>
							voice.updateVoiceSettings({ language: e.target.value })
						}
						className='w-full p-2 border border-gray-300 rounded-md focus:ring-2 focus:ring-blue-500 focus:border-transparent'>
						{languageOptions.map((option) => (
							<option key={option.value} value={option.value}>
								{option.label}
							</option>
						))}
					</select>
				</div>

				{/* Voice Selection */}
				<div>
					<label className='block text-sm font-medium text-gray-700 mb-2'>
						<Volume2 className='w-4 h-4 inline mr-1' />
						Voice
					</label>
					<select
						value={voice.voiceSettings.voiceId || 'alloy'}
						onChange={(e) =>
							voice.updateVoiceSettings({ voiceId: e.target.value })
						}
						className='w-full p-2 border border-gray-300 rounded-md focus:ring-2 focus:ring-blue-500 focus:border-transparent'>
						{voiceOptions.map((option) => (
							<option key={option.value} value={option.value}>
								{option.label}
							</option>
						))}
					</select>
				</div>

				{/* Speech Rate */}
				<div>
					<label className='block text-sm font-medium text-gray-700 mb-2'>
						Speech Rate: {voice.voiceSettings.rate || 1.0}x
					</label>
					<input
						type='range'
						min='0.5'
						max='2.0'
						step='0.1'
						value={voice.voiceSettings.rate || 1.0}
						onChange={(e) =>
							voice.updateVoiceSettings({ rate: parseFloat(e.target.value) })
						}
						className='w-full h-2 bg-gray-200 rounded-lg appearance-none cursor-pointer'
					/>
					<div className='flex justify-between text-xs text-gray-500 mt-1'>
						<span>0.5x</span>
						<span>1.0x</span>
						<span>2.0x</span>
					</div>
				</div>

				{/* Volume */}
				<div>
					<label className='block text-sm font-medium text-gray-700 mb-2'>
						Volume: {Math.round((voice.voiceSettings.volume || 1.0) * 100)}%
					</label>
					<input
						type='range'
						min='0'
						max='1'
						step='0.1'
						value={voice.voiceSettings.volume || 1.0}
						onChange={(e) =>
							voice.updateVoiceSettings({ volume: parseFloat(e.target.value) })
						}
						className='w-full h-2 bg-gray-200 rounded-lg appearance-none cursor-pointer'
					/>
				</div>

				{/* Auto-add to Messages */}
				<div className='flex items-center justify-between'>
					<label className='text-sm font-medium text-gray-700'>
						Auto-add to Messages
					</label>
					<motion.button
						onClick={() =>
							voice.updateVoiceSettings({
								autoAddToMessages: !voice.voiceSettings.autoAddToMessages,
							})
						}
						className={`
              relative inline-flex h-6 w-11 items-center rounded-full transition-colors
              ${
								voice.voiceSettings.autoAddToMessages
									? 'bg-blue-600'
									: 'bg-gray-200'
							}
            `}
						whileTap={{ scale: 0.95 }}>
						<motion.span
							className='inline-block h-4 w-4 transform rounded-full bg-white shadow-lg'
							animate={{
								x: voice.voiceSettings.autoAddToMessages ? 24 : 4,
							}}
							transition={{ type: 'spring', stiffness: 500, damping: 30 }}
						/>
					</motion.button>
				</div>

				{/* Browser TTS */}
				<div className='flex items-center justify-between'>
					<label className='text-sm font-medium text-gray-700'>
						Use Browser TTS
					</label>
					<motion.button
						onClick={() =>
							voice.updateVoiceSettings({
								useBrowserTTS: !voice.voiceSettings.useBrowserTTS,
							})
						}
						className={`
              relative inline-flex h-6 w-11 items-center rounded-full transition-colors
              ${
								voice.voiceSettings.useBrowserTTS
									? 'bg-blue-600'
									: 'bg-gray-200'
							}
            `}
						whileTap={{ scale: 0.95 }}>
						<motion.span
							className='inline-block h-4 w-4 transform rounded-full bg-white shadow-lg'
							animate={{
								x: voice.voiceSettings.useBrowserTTS ? 24 : 4,
							}}
							transition={{ type: 'spring', stiffness: 500, damping: 30 }}
						/>
					</motion.button>
				</div>
			</div>
		</motion.div>
	);
};
```

## Complete Voice Interface

Combining all components into a comprehensive voice interface:

```typescript
import React, { useState } from 'react';
import { motion, AnimatePresence } from 'motion/react';
import { Settings } from 'lucide-react';
import {
	VoiceButton,
	VoiceWaveform,
	VoiceStatusPanel,
	VoiceSettings,
} from './VoiceComponents';

export const VoiceInterface: React.FC = () => {
	const [showSettings, setShowSettings] = useState(false);

	return (
		<div className='voice-interface'>
			{/* Main Voice Controls */}
			<div className='flex flex-col items-center gap-6 p-6'>
				{/* Voice Button */}
				<VoiceButton size='large' />

				{/* Waveform Visualizer */}
				<VoiceWaveform className='w-64' />

				{/* Settings Toggle */}
				<motion.button
					onClick={() => setShowSettings(!showSettings)}
					className='flex items-center gap-2 text-gray-600 hover:text-gray-800 transition-colors'
					whileHover={{ scale: 1.05 }}
					whileTap={{ scale: 0.95 }}>
					<Settings className='w-4 h-4' />
					<span className='text-sm'>Settings</span>
				</motion.button>
			</div>

			{/* Status Panel */}
			<div className='mb-6'>
				<VoiceStatusPanel />
			</div>

			{/* Settings Panel */}
			<AnimatePresence>
				{showSettings && (
					<motion.div
						initial={{ opacity: 0, height: 0 }}
						animate={{ opacity: 1, height: 'auto' }}
						exit={{ opacity: 0, height: 0 }}
						className='overflow-hidden'>
						<VoiceSettings />
					</motion.div>
				)}
			</AnimatePresence>
		</div>
	);
};
```

## Styling Guidelines

### CSS Classes for Voice Components

```css
/* Voice button states */
.voice-button {
	@apply transition-all duration-200 ease-out;
}

.voice-button:hover {
	@apply transform scale-105;
}

.voice-button.listening {
	@apply bg-red-500 shadow-lg shadow-red-500/25;
	animation: pulse 2s cubic-bezier(0.4, 0, 0.6, 1) infinite;
}

.voice-button.speaking {
	@apply bg-green-500 shadow-lg shadow-green-500/25;
}

/* Voice indicator animations */
.voice-indicator {
	@apply flex items-center gap-2 justify-center;
}

.voice-indicator .listening-bars {
	@apply flex gap-1;
}

.voice-indicator .speaking-pulse {
	@apply w-4 h-4 bg-green-500 rounded-full opacity-30;
	animation: speaking-pulse 0.8s ease-in-out infinite;
}

/* Waveform visualizer */
.voice-waveform {
	@apply flex items-end justify-center gap-1;
}

.voice-waveform .bar {
	@apply w-1 rounded-full transition-all duration-100 ease-out;
	min-height: 2px;
}

/* Keyframes */
@keyframes pulse {
	0%,
	100% {
		opacity: 1;
	}
	50% {
		opacity: 0.8;
	}
}

@keyframes speaking-pulse {
	0%,
	100% {
		transform: scale(1);
	}
	50% {
		transform: scale(1.2);
	}
}
```

## Accessibility Considerations

### ARIA Labels and Screen Reader Support

```typescript
// Enhanced VoiceButton with accessibility
export const AccessibleVoiceButton: React.FC<VoiceButtonProps> = (props) => {
	const voice = useCedarStore((state) => state.voice);

	const getAriaLabel = () => {
		if (voice.isListening) return 'Stop listening';
		if (voice.voicePermissionStatus === 'denied')
			return 'Microphone access denied';
		if (voice.voicePermissionStatus === 'not-supported')
			return 'Voice not supported';
		return 'Start listening';
	};

	return (
		<motion.button
			{...props}
			aria-label={getAriaLabel()}
			aria-pressed={voice.isListening}
			role='button'
			tabIndex={0}
			onKeyDown={(e) => {
				if (e.key === 'Enter' || e.key === ' ') {
					e.preventDefault();
					handleClick();
				}
			}}>
			{/* Button content */}
		</motion.button>
	);
};
```

### Reduced Motion Support

```typescript
// Respect user's motion preferences
const shouldReduceMotion = window.matchMedia(
	'(prefers-reduced-motion: reduce)'
).matches;

const animationProps = shouldReduceMotion
	? {}
	: {
			animate: { scale: [1, 1.2, 1] },
			transition: { duration: 0.8, repeat: Infinity },
	  };

<motion.div {...animationProps}>{/* Content */}</motion.div>;
```

## Testing Voice Components

```typescript
import { render, screen, fireEvent, waitFor } from '@testing-library/react';
import { VoiceButton } from './VoiceButton';
import { useCedarStore } from '@cedar/core';

// Mock the store
jest.mock('@cedar/core', () => ({
	useCedarStore: jest.fn(),
}));

describe('VoiceButton', () => {
	const mockVoice = {
		isListening: false,
		voicePermissionStatus: 'granted',
		toggleVoice: jest.fn(),
		requestVoicePermission: jest.fn(),
	};

	beforeEach(() => {
		(useCedarStore as jest.Mock).mockReturnValue(mockVoice);
	});

	it('renders correctly', () => {
		render(<VoiceButton />);
		expect(screen.getByRole('button')).toBeInTheDocument();
	});

	it('calls toggleVoice when clicked', async () => {
		render(<VoiceButton />);

		fireEvent.click(screen.getByRole('button'));

		await waitFor(() => {
			expect(mockVoice.toggleVoice).toHaveBeenCalled();
		});
	});

	it('shows correct state when listening', () => {
		(useCedarStore as jest.Mock).mockReturnValue({
			...mockVoice,
			isListening: true,
		});

		render(<VoiceButton />);

		expect(screen.getByLabelText('Stop listening')).toBeInTheDocument();
	});
});
```

## Next Steps

<CardGroup cols={2}>
  <Card title="Voice Overview" icon="microphone" href="/voice/voice-integration">
    Return to the main voice documentation
  </Card>

  <Card title="Backend Integration" icon="server" href="/voice/agentic-backend">
    Learn about setting up voice backend processing
  </Card>
</CardGroup>

{' '}


# Voice Endpoint Format
Source: https://docs.cedarcopilot.com/voice/voice-endpoint-format

Learn how to implement voice endpoints for Mastra and custom backends

# Voice Endpoint Format

Cedar OS provides two approaches for handling voice, depending on your provider configuration:

1. **Mastra/Custom backends**: Direct voice endpoint handling
2. **AI SDK/OpenAI providers**: Automatic transcription and speech generation

## Provider-Specific Voice Handling

### Mastra and Custom Backends

When using Mastra or custom backends, Cedar OS sends voice data directly to your voice endpoint. You have full control over:

* Audio transcription
* Response generation
* Text-to-speech synthesis
* Response format

### AI SDK and OpenAI Providers

When using AI SDK or OpenAI providers, Cedar OS automatically:

1. Transcribes audio using OpenAI's Whisper model
2. Generates a text response using the configured LLM
3. Optionally generates speech using OpenAI's TTS model (when `useBrowserTTS` is false)

## Request Format (Mastra/Custom)

Cedar OS sends voice data to your endpoint as a multipart form data request:

```http
POST /voice
Content-Type: multipart/form-data
Authorization: Bearer YOUR_API_KEY (if configured)

FormData:
- audio: Blob (audio file, typically webm format)
- settings: JSON string containing voice settings
- context: String containing additional context
```

### Voice Settings Structure

The `settings` field contains a JSON object with the following structure:

```json
{
	"language": "en-US",
	"voiceId": "optional-voice-id",
	"pitch": 1.0,
	"rate": 1.0,
	"volume": 1.0,
	"useBrowserTTS": false,
	"autoAddToMessages": true
}
```

* `language`: Language code for speech recognition/synthesis
* `voiceId`: Voice identifier for TTS (provider-specific)
* `pitch`, `rate`, `volume`: Voice modulation parameters
* `useBrowserTTS`: Whether to use browser's built-in TTS
* `autoAddToMessages`: Whether to add voice interactions to chat history

### Context

The `context` field contains stringified additional context from the Cedar state, which may include:

* Current chat messages
* Application state
* User-defined context

## Response Format (All Providers)

Your endpoint can return different types of responses:

### 1. JSON Response (Recommended)

```json
{
	"text": "The assistant's response text",
	"transcription": "What the user said",
	"audioData": "base64-encoded-audio-data",
	"audioUrl": "https://example.com/audio.mp3",
	"audioFormat": "audio/mpeg",
	"usage": {
		"promptTokens": 100,
		"completionTokens": 50,
		"totalTokens": 150
	},
	"object": {
		"type": "action",
		"stateKey": "myState",
		"setterKey": "updateValue",
		"args": ["new value"]
	}
}
```

All fields are optional:

* `text`: The text response from the assistant
* `transcription`: The transcribed user input
* `audioData`: Base64-encoded audio response
* `audioUrl`: URL to an audio file
* `audioFormat`: MIME type of the audio
* `usage`: Token usage statistics
* `object`: Structured response for actions

### 2. Audio Response

Return raw audio data with appropriate content type:

```http
HTTP/1.1 200 OK
Content-Type: audio/mpeg

[Binary audio data]
```

### 3. Plain Text Response

```http
HTTP/1.1 200 OK
Content-Type: text/plain

This is the assistant's response.
```

## Implementation Example (Mastra)

Here's an example of implementing a voice endpoint in a Mastra backend:

```typescript
import { Agent } from '@mastra/core';

export async function POST(request: Request) {
	const formData = await request.formData();
	const audio = formData.get('audio') as Blob;
	const settings = JSON.parse(formData.get('settings') as string);
	const context = formData.get('context') as string;

	// Process audio (transcription)
	const transcription = await transcribeAudio(audio);

	// Generate response using your agent
	const agent = new Agent({
		// ... agent configuration
	});

	const response = await agent.generate({
		prompt: transcription,
		context: context,
	});

	// Optionally generate speech
	let audioData;
	if (!settings.useBrowserTTS) {
		audioData = await generateSpeech(response.text);
	}

	// Return JSON response
	return Response.json({
		text: response.text,
		transcription: transcription,
		audioData: audioData
			? Buffer.from(audioData).toString('base64')
			: undefined,
		usage: response.usage,
	});
}
```

## Voice Response Handling

Cedar OS provides a unified `handleLLMVoice` function that processes voice responses consistently across all providers:

1. **Audio Playback**: Handles base64 audio data, audio URLs, or browser TTS
2. **Message Integration**: Automatically adds transcriptions and responses to chat history
3. **Action Execution**: Processes structured responses to trigger state changes

## Structured Responses

Cedar OS supports structured responses that can trigger actions in your application:

### Action Response

To execute a state change:

```json
{
	"text": "I've updated the value for you.",
	"object": {
		"type": "action",
		"stateKey": "myCustomState",
		"setterKey": "setValue",
		"args": [42]
	}
}
```

This will call `myCustomState.setValue(42)` in your Cedar state.

## Error Handling

Return appropriate HTTP status codes:

* `200 OK`: Successful response
* `400 Bad Request`: Invalid request format
* `401 Unauthorized`: Missing or invalid API key
* `500 Internal Server Error`: Server-side error

## Voice Configuration

Configure voice settings when initializing Cedar:

```typescript
const store = createCedarStore({
	voiceSettings: {
		language: 'en-US',
		voiceId: 'alloy', // OpenAI voice options: alloy, echo, fable, onyx, nova, shimmer
		useBrowserTTS: false, // Use provider TTS instead of browser
		autoAddToMessages: true, // Add voice interactions to chat
	},
});
```

## Provider-Specific Notes

### OpenAI/AI SDK

* Transcription: Uses Whisper model (`whisper-1`)
* Speech: Uses TTS model (`tts-1`) with configurable voices
* Audio format: MP3 (audio/mpeg)

### Mastra/Custom

* Full control over transcription and TTS services
* Can integrate with any speech service (Google, Azure, AWS, etc.)
* Flexible audio format support


# Voice Integration (Beta)
Source: https://docs.cedarcopilot.com/voice/voice-integration

Add voice capabilities to your Cedar OS application

# Voice Integration

Cedar OS provides comprehensive voice support that integrates seamlessly with your agent backend. Voice processing is now handled through the agent connection system, providing a unified approach across different providers.

## Voice Processing Architecture

<Note>
  As of the latest update, voice processing now works out of the box for Mastra, AI SDK, and custom backends.

  * **Mastra/Custom backends**: Voice data is sent directly to your voice endpoint
  * **AI SDK/OpenAI providers**: Voice is transcribed using Whisper, then processed through the LLM.

  See the [Voice Endpoint Format](/voice/voice-endpoint-format) documentation for implementation details.

  This is still in beta. If you want more details please check out the repo, book a call at [https://calendly.com/jesse-cedarcopilot/30min](https://calendly.com/jesse-cedarcopilot/30min) or join our discord.
</Note>

## Quick Start

Initialize Cedar with voice settings (already configured with the built in Cedar chat input):

```tsx
import { CedarCopilot } from 'cedar-os';

function App() {
	return (
		<CedarCopilot
			llmProvider={{
				provider: 'openai',
				apiKey: process.env.OPENAI_API_KEY!,
			}}
			voiceSettings={{
				language: 'en-US',
				voiceId: 'alloy', // OpenAI voices: alloy, echo, fable, onyx, nova, shimmer
				useBrowserTTS: false, // Use OpenAI TTS instead of browser
				autoAddToMessages: true, // Add voice interactions to chat history
				pitch: 1.0,
				rate: 1.0,
				volume: 1.0,
				endpoint: '/api/voice', // Optional: Custom voice endpoint
			}}>
			{/* Your app content */}
		</CedarCopilot>
	);
}
```

### Voice Settings

The `voiceSettings` prop accepts a partial configuration object with the following TypeScript interface:

```typescript
interface VoiceSettings {
	language: string;                // Required - Language code for speech recognition/synthesis
	voiceId?: string;               // Optional - Voice identifier (provider-specific)
	pitch?: number;                 // Optional - Voice pitch modulation (0.5-2.0)
	rate?: number;                  // Optional - Speech rate (0.5-2.0)
	volume?: number;                // Optional - Audio volume (0.0-1.0)
	useBrowserTTS?: boolean;        // Optional - Use browser's built-in TTS
	autoAddToMessages?: boolean;    // Optional - Add voice interactions to chat
	endpoint?: string;              // Optional - Custom voice endpoint URL
}

// Usage: All properties except 'language' are optional
voiceSettings?: Partial<VoiceSettings>
```

#### Default Values

| Setting             | Type    | Default     | Description                                    |
| ------------------- | ------- | ----------- | ---------------------------------------------- |
| `language`          | string  | `'en-US'`   | Language code for speech recognition/synthesis |
| `voiceId`           | string  | `undefined` | Voice identifier (provider-specific)           |
| `pitch`             | number  | `1.0`       | Voice pitch modulation (0.5-2.0)               |
| `rate`              | number  | `1.0`       | Speech rate (0.5-2.0)                          |
| `volume`            | number  | `1.0`       | Audio volume (0.0-1.0)                         |
| `useBrowserTTS`     | boolean | `false`     | Use browser's built-in TTS                     |
| `autoAddToMessages` | boolean | `true`      | Add voice interactions to chat                 |
| `endpoint`          | string  | `undefined` | Custom voice endpoint URL                      |

### Provider-Specific Configuration

#### OpenAI / AI SDK

```tsx
<CedarCopilot
  llmProvider={{
    provider: 'ai-sdk',
    providers: {
      openai: { apiKey: process.env.OPENAI_API_KEY! },
    },
  }}
  voiceSettings={{
    voiceId: 'nova', // OpenAI voice selection
    useBrowserTTS: false, // Use OpenAI TTS
  }}
>
```

#### Mastra

```tsx
<CedarCopilot
  llmProvider={{
    provider: 'mastra',
    baseURL: 'https://your-mastra-api.com',
    apiKey: process.env.MASTRA_API_KEY,
    voiceRoute: '/voice', // Auto-configures voice endpoint
  }}
  voiceSettings={{
    language: 'en-US',
    autoAddToMessages: true,
    // endpoint is auto-configured from voiceRoute
  }}
>
```

#### Custom Backend

```tsx
<CedarCopilot
  llmProvider={{
    provider: 'custom',
    // ... custom provider config
  }}
  voiceSettings={{
    endpoint: 'https://your-api.com/voice',
    useBrowserTTS: true, // Use browser TTS if backend doesn't provide audio
  }}
>
```

## Overview

Cedar-OS provides a complete voice integration system that enables natural voice conversations with AI agents. The voice system handles audio capture, streaming to backend services, and automatic playback of responses, with seamless integration into the messaging system.

## Features

* 🎤 **Voice Capture**: Browser-based audio recording with permission management
* 🔊 **Audio Playback**: Automatic playback of audio responses from agents
* 🌐 **Streaming Support**: Real-time audio streaming to configurable endpoints
* 🔧 **Flexible Configuration**: Customizable voice settings (language, pitch, rate, volume)
* 🎯 **State Management**: Full integration with Cedar's Zustand-based store
* 💬 **Message Integration**: Automatic addition of voice interactions to chat history
* 🛡️ **Error Handling**: Comprehensive error states and recovery
* 🎨 **Visual Indicators**: Animated voice status indicators

## ChatInput Component with Built-in Voice

The `ChatInput` component from `cedar-os-components` comes with voice functionality built-in, providing a seamless voice experience out of the box. When you use this component, voice capabilities are automatically available without additional configuration.

### How It Works

The ChatInput component integrates voice through the following key features:

<CodeGroup>
  ```tsx useVoice Hook
  import { useVoice } from 'cedar-os';

  // The useVoice hook provides complete access to voice state and controls
  const voice = useVoice();

  // Available properties
  voice.isListening; // Is currently recording audio
  voice.isSpeaking; // Is playing audio response
  voice.voicePermissionStatus; // 'granted' | 'denied' | 'prompt' | 'not-supported'
  voice.voiceError; // Error message if any
  voice.voiceSettings; // Current voice configuration

  // Available methods
  voice.checkVoiceSupport(); // Check browser compatibility
  voice.requestVoicePermission(); // Request microphone access
  voice.toggleVoice(); // Start/stop recording
  voice.startListening(); // Start recording
  voice.stopListening(); // Stop recording
  voice.updateVoiceSettings(); // Update configuration
  voice.resetVoiceState(); // Clean up resources
  ```

  ```tsx VoiceIndicator Component
  import { VoiceIndicator } from 'cedar-os';

  // Visual feedback component for voice status
  <VoiceIndicator
  	voiceState={{
  		isListening: voice.isListening,
  		isSpeaking: voice.isSpeaking,
  		voiceError: voice.voiceError,
  		voicePermissionStatus: voice.voicePermissionStatus,
  	}}
  />;

  // The indicator shows:
  // - Animated bars when listening (red)
  // - Pulsing circle when speaking (green)
  // - Error messages with alert icon
  // - Automatically hides when inactive
  ```

  ```tsx Voice Toggle Implementation
  // Handle voice toggle with permission management
  const handleVoiceToggle = async () => {
  	// Check browser support
  	if (!voice.checkVoiceSupport()) {
  		console.error('Voice not supported');
  		return;
  	}

  	// Request permission if needed
  	if (voice.voicePermissionStatus === 'prompt') {
  		await voice.requestVoicePermission();
  	}

  	// Toggle voice if permitted
  	if (voice.voicePermissionStatus === 'granted') {
  		voice.toggleVoice();
  	}
  };
  ```
</CodeGroup>

### ChatInput Implementation Details

The ChatInput component automatically:

1. **Displays a microphone button** that changes appearance based on voice state
2. **Shows the VoiceIndicator** when voice is active (listening or speaking)
3. **Handles keyboard shortcuts** - Press 'M' to toggle voice (when not typing)
4. **Manages permissions** - Automatically requests microphone access when needed
5. **Provides visual feedback** - Button animations and color changes for different states

<CodeGroup>
  ```tsx Microphone Button States
  // The mic button automatically changes appearance
  getMicButtonClass() {
    if (voice.isListening) {
      // Red pulsing animation when recording
      return 'text-red-500 animate-pulse';
    }
    if (voice.isSpeaking) {
      // Green when playing response
      return 'text-green-500';
    }
    if (voice.voicePermissionStatus === 'denied') {
      // Grayed out if permission denied
      return 'text-gray-400 cursor-not-allowed';
    }
    // Default state
    return 'text-gray-600 hover:text-black';
  }
  ```

  ```tsx Keyboard Shortcut
  // Global keyboard shortcut for voice
  useEffect(() => {
  	const handleKeyDown = (e: KeyboardEvent) => {
  		// Press 'M' to toggle voice
  		if (e.key === 'm' || e.key === 'M') {
  			// Only when not typing in input fields
  			if (!isTypingInInput) {
  				e.preventDefault();
  				handleVoiceToggle();
  			}
  		}
  	};

  	window.addEventListener('keydown', handleKeyDown);
  	return () => window.removeEventListener('keydown', handleKeyDown);
  }, [handleVoiceToggle]);
  ```

  ```tsx Complete ChatInput Usage
  import { ChatInput } from 'cedar-os-components';

  // Simply use the ChatInput component - voice is included!
  <ChatInput
  	stream={true}
  	className='custom-styles'
  	handleFocus={() => console.log('Focused')}
  	handleBlur={() => console.log('Blurred')}
  />;

  // Voice features are automatically available:
  // - Mic button in the input
  // - Voice indicator when active
  // - Keyboard shortcut (M key)
  // - Permission handling
  // - Visual feedback
  ```
</CodeGroup>

### Exported Components and Hooks

All voice-related functionality is exported from the main `cedar-os` package:

```typescript
// Main exports from 'cedar-os'
export { useVoice } from 'cedar-os'; // Voice control hook
export { VoiceIndicator } from 'cedar-os'; // Voice status component
export { cn } from 'cedar-os'; // Utility for className merging

// The ChatInput component is in cedar-os-components
import { ChatInput } from 'cedar-os-components';
```

## Quick Start

### 1. Automatic Configuration with Mastra

The easiest way to set up voice integration is through the Mastra provider configuration:

```typescript
import { CedarCopilot } from 'cedar-os';

function App() {
	return (
		<CedarCopilot
			llmProvider={{
				provider: 'mastra',
				baseURL: 'http://localhost:3000/api',
				voiceRoute: '/chat/voice-execute', // Automatically configures voice endpoint
			}}>
			<YourVoiceApp />
		</CedarCopilot>
	);
}
```

When you specify a `voiceRoute` in your Mastra configuration, Cedar-OS automatically sets the voice endpoint to `baseURL + voiceRoute`.

### 2. Manual Configuration

For non-Mastra providers or custom setups, configure the voice endpoint manually:

```typescript
import { useCedarStore } from '@cedar/core';

function VoiceChat() {
	const voice = useCedarStore((state) => state.voice);

	useEffect(() => {
		// Configure the voice endpoint manually
		voice.setVoiceEndpoint('http://localhost:3456/api/chat/voice');

		// Cleanup on unmount
		return () => {
			voice.resetVoiceState();
		};
	}, []);

	return (
		<div>
			<button
				onClick={() => voice.toggleVoice()}
				disabled={voice.voicePermissionStatus !== 'granted'}>
				{voice.isListening ? 'Stop Listening' : 'Start Listening'}
			</button>

			{voice.voiceError && <div className='error'>{voice.voiceError}</div>}
		</div>
	);
}
```

### 3. Request Microphone Permission

```typescript
const handleEnableVoice = async () => {
	if (!voice.checkVoiceSupport()) {
		alert('Voice features are not supported in your browser');
		return;
	}

	await voice.requestVoicePermission();

	if (voice.voicePermissionStatus === 'granted') {
		console.log('Voice enabled!');
	}
};
```

### 4. Using the Voice Indicator

```typescript
import { VoiceIndicator } from '@cedar/voice';

function App() {
	const voice = useCedarStore((state) => state.voice);

	return (
		<div>
			<VoiceIndicator voiceState={voice} />
			{/* Rest of your app */}
		</div>
	);
}
```

## Provider-Specific Voice Configuration

### Mastra Provider

When using the Mastra provider, voice configuration is streamlined through the provider setup:

```typescript
<CedarCopilot
	llmProvider={{
		provider: 'mastra',
		baseURL: 'http://localhost:3000/api',
		chatPath: '/chat',
		voiceRoute: '/chat/voice-execute', // Automatically sets voice endpoint
	}}>
	<YourApp />
</CedarCopilot>
```

**Benefits of Mastra voice integration:**

* Automatic endpoint configuration
* Consistent routing with chat endpoints
* Built-in context passing
* Structured response handling

### Other Providers

For OpenAI, Anthropic, AI SDK, or custom providers, configure the voice endpoint manually:

```typescript
const voice = useCedarStore((state) => state.voice);

useEffect(() => {
	// Set your custom voice endpoint
	voice.setVoiceEndpoint('https://your-backend.com/api/voice');
}, []);
```

## Voice State

The voice slice manages comprehensive state for voice interactions:

```typescript
interface VoiceState {
	// Core state
	isVoiceEnabled: boolean;
	isListening: boolean;
	isSpeaking: boolean;
	voiceEndpoint: string;
	voicePermissionStatus: 'granted' | 'denied' | 'prompt' | 'not-supported';
	voiceError: string | null;

	// Audio resources
	audioStream: MediaStream | null;
	audioContext: AudioContext | null;
	mediaRecorder: MediaRecorder | null;

	// Voice settings
	voiceSettings: {
		language: string; // Required - Language code (e.g., 'en-US')
		voiceId?: string; // Optional - Voice ID for TTS (provider-specific)
		pitch?: number; // Optional - Voice pitch (0.5 to 2.0)
		rate?: number; // Optional - Speech rate (0.5 to 2.0)
		volume?: number; // Optional - Audio volume (0.0 to 1.0)
		useBrowserTTS?: boolean; // Optional - Use browser TTS instead of backend
		autoAddToMessages?: boolean; // Optional - Add voice interactions to messages
		endpoint?: string; // Optional - Custom voice endpoint URL
	};
}
```

## Available Actions

### Permission Management

* `checkVoiceSupport()` - Check if browser supports voice features
* `requestVoicePermission()` - Request microphone access

### Voice Control

* `startListening()` - Start recording audio
* `stopListening()` - Stop recording and send to endpoint
* `toggleVoice()` - Toggle between listening and idle states

### Audio Processing

* `streamAudioToEndpoint(audioData)` - Send audio to backend
* `playAudioResponse(audioUrl)` - Play audio response

### Configuration

* `setVoiceEndpoint(endpoint)` - Set the backend endpoint URL
* `updateVoiceSettings(settings)` - Update voice configuration
* `setVoiceError(error)` - Set error message
* `resetVoiceState()` - Clean up and reset all voice state

## Message Integration

By default, voice interactions are automatically added to the Cedar messages store, creating a seamless conversation history:

```typescript
// User speech is transcribed and added as a user message
{
  type: 'text',
  role: 'user',
  content: 'Show me the latest reports',
  metadata: {
    source: 'voice',
    timestamp: '2024-01-01T12:00:00Z'
  }
}

// Agent response is added as an assistant message
{
  type: 'text',
  role: 'assistant',
  content: 'Here are your latest reports...',
  metadata: {
    source: 'voice',
    usage: { /* token usage data */ },
    timestamp: '2024-01-01T12:00:01Z'
  }
}
```

### Disabling Message Integration

```typescript
voice.updateVoiceSettings({
	autoAddToMessages: false,
});
```

## Browser Compatibility

The voice system requires modern browser APIs:

* `navigator.mediaDevices.getUserMedia` - Audio capture
* `MediaRecorder` API - Audio recording
* `AudioContext` API - Audio processing

**Supported browsers:**

* Chrome/Edge 47+
* Firefox 25+
* Safari 11+
* Opera 34+

<Warning>
  HTTPS is required for microphone access in production environments (localhost
  is exempt).
</Warning>

## Next Steps

<CardGroup cols={2}>
  <Card title="Backend Integration" icon="server" href="/voice/agentic-backend">
    Learn how to set up your backend to handle voice requests
  </Card>

  <Card title="Streaming Implementation" icon="wave-square" href="/voice/streams">
    Implement real-time audio streaming
  </Card>
</CardGroup>

## Examples

### Complete Voice Chat Component

```typescript
import { useCedarStore } from '@cedar/core';
import { VoiceIndicator } from '@cedar/voice';
import { useEffect } from 'react';

export function VoiceChat() {
	const voice = useCedarStore((state) => state.voice);

	useEffect(() => {
		voice.setVoiceEndpoint('http://localhost:3456/api/chat/voice');
		return () => voice.resetVoiceState();
	}, []);

	const handleVoiceToggle = async () => {
		if (voice.voicePermissionStatus === 'prompt') {
			await voice.requestVoicePermission();
		}

		if (voice.voicePermissionStatus === 'granted') {
			voice.toggleVoice();
		}
	};

	return (
		<div className='voice-chat'>
			<VoiceIndicator voiceState={voice} />

			<button
				onClick={handleVoiceToggle}
				className={`voice-button ${voice.isListening ? 'listening' : ''}`}
				disabled={voice.voicePermissionStatus === 'denied'}>
				{voice.isListening ? 'Stop' : 'Talk'}
			</button>

			{voice.voiceError && (
				<div className='error-message'>{voice.voiceError}</div>
			)}

			<div className='voice-settings'>
				<select
					value={voice.voiceSettings.language}
					onChange={(e) =>
						voice.updateVoiceSettings({ language: e.target.value })
					}>
					<option value='en-US'>English (US)</option>
					<option value='en-GB'>English (UK)</option>
					<option value='es-ES'>Spanish</option>
					<option value='fr-FR'>French</option>
				</select>
			</div>
		</div>
	);
}
```
